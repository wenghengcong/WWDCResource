1
00:00:16,750 --> 00:00:23,757 line:0
（研究和护理框架进阶）


2
00:00:30,998 --> 00:00:32,131 line:-1
感谢大家的到来


3
00:00:32,299 --> 00:00:36,970 line:-1
欢迎来听研究和护理框架进阶会议


4
00:00:37,538 --> 00:00:39,873 line:-2
我叫Srinath
我是一名软件工程师


5
00:00:40,541 --> 00:00:43,310 line:-2
如果你熟悉我们过去的
Dub Dub Talks


6
00:00:43,610 --> 00:00:47,381 line:-1
你就会记得我们的会议主要聚焦于


7
00:00:47,447 --> 00:00:49,683 line:-1
两个开源健康框架


8
00:00:50,717 --> 00:00:52,753 line:-2
ResearchKit
和CareKit


9
00:00:53,453 --> 00:00:57,124 line:-2
尽管今年我们仍然会
谈论很多这些框架的内容


10
00:00:57,491 --> 00:01:02,229 line:-2
我们还会谈到一些
我们一直在研究的新API和功能


11
00:01:02,462 --> 00:01:03,797 line:-1
在更大的健康领域中


12
00:01:04,631 --> 00:01:07,534 line:-1
对于你们中那些对框架感到陌生的人


13
00:01:07,868 --> 00:01:11,905 line:-1
我强烈建议你们看看去年的两个演讲


14
00:01:13,407 --> 00:01:17,077 line:-2
CareKit和ResearchKit的新内容
是由Sam Mravca主讲的


15
00:01:17,611 --> 00:01:22,015 line:-2
她对ResearchKit和
CareKit做了很好的介绍


16
00:01:22,649 --> 00:01:25,452 line:-2
另一个演讲
将CareKit连接到云


17
00:01:25,719 --> 00:01:27,421 line:-2
由Kelsey Dedoshka
主讲


18
00:01:27,721 --> 00:01:30,691 line:-2
在那个演讲中 我们引入了一个
新的CareKit桥API


19
00:01:30,958 --> 00:01:35,896 line:-2
它允许程序员在病人和
供应商间同步护理计划


20
00:01:35,963 --> 00:01:38,232 line:-1
其中使用了HIPAA适用后台


21
00:01:38,899 --> 00:01:42,169 line:-2
在那张幻灯片上
我非常激动地向你们展示


22
00:01:42,402 --> 00:01:46,240 line:-2
Penn Medicine取得的
一些很棒的成果


23
00:01:47,241 --> 00:01:52,312 line:-2
Penn Life Gained app利用了
CareKit和CareKit桥API


24
00:01:52,646 --> 00:01:56,884 line:-1
在肥胖症手术的术前和术后帮助患者


25
00:01:57,084 --> 00:01:58,085 line:-1
在Penn Medicine


26
00:01:59,219 --> 00:02:01,288 line:-1
这个app权衡了护理计划


27
00:02:01,488 --> 00:02:04,691 line:-1
治疗数据和其他交互式组件


28
00:02:05,125 --> 00:02:08,495 line:-1
来帮助病人度过他们的减肥之旅


29
00:02:09,229 --> 00:02:14,801 line:-2
数据一直在被同步
在患者使用的iOS app


30
00:02:15,068 --> 00:02:19,673 line:-2
与治疗提供商所用的iPad app间
以此来密切监测


31
00:02:19,840 --> 00:02:22,576 line:-1
并实时与患者进行互动


32
00:02:23,143 --> 00:02:26,046 line:-1
我们的团队已收到了医患双方


33
00:02:26,246 --> 00:02:28,849 line:-1
非常积极的反馈


34
00:02:29,049 --> 00:02:33,921 line:-2
鉴于这些app对于他们
肥胖症项目的影响


35
00:02:35,189 --> 00:02:38,258 line:-1
既然我们已谈到了从去年以来的进展


36
00:02:38,959 --> 00:02:41,128 line:-1
我想要快速地


37
00:02:41,395 --> 00:02:43,597 line:-1
整体介绍下我们的健康框架


38
00:02:44,431 --> 00:02:48,235 line:-1
这些框架的创造来源于我们的初衷


39
00:02:48,302 --> 00:02:51,205 line:-1
就是通过科技来改善世界健康水平


40
00:02:51,839 --> 00:02:55,042 line:-2
在此过程中 我们真的想让
我们的用户获得更多


41
00:02:55,309 --> 00:02:58,545 line:-2
并且为开发者
以及研究者提供更多工具


42
00:02:58,745 --> 00:03:01,815 line:-1
最终你和我们


43
00:03:01,982 --> 00:03:05,752 line:-1
可以帮助改进两个核心领域


44
00:03:06,053 --> 00:03:07,721 line:-1
研究和治疗


45
00:03:08,922 --> 00:03:11,625 line:-1
今年我很激动地向你们介绍


46
00:03:11,892 --> 00:03:14,795 line:-1
一些我们非常关注的主题


47
00:03:15,028 --> 00:03:16,496 line:-1
从很多方面来说


48
00:03:16,864 --> 00:03:18,899 line:-1
最开始我们聚焦于框架


49
00:03:19,132 --> 00:03:22,970 line:-2
我会给你们介绍我们对于
ResearchKit框架所做的一些更新


50
00:03:23,504 --> 00:03:27,074 line:-2
然后我们会介绍
一个更关注于状况的方法


51
00:03:27,441 --> 00:03:31,044 line:-2
特别是像帕金森症这样的
运动失调疾病


52
00:03:31,411 --> 00:03:35,716 line:-2
这里会由Gabriel上台
来给你们介绍下这个新的API


53
00:03:36,450 --> 00:03:40,787 line:-2
最后我们会将所有内容
集合起来做一个演示


54
00:03:41,021 --> 00:03:45,492 line:-2
看看你们可以如何在代码中
直接使用这些新API和功能


55
00:03:47,160 --> 00:03:49,196 line:-1
我们从ResearchKit开始


56
00:03:50,163 --> 00:03:53,300 line:-1
从去年开始 我们花了很多精力


57
00:03:53,700 --> 00:03:57,137 line:-1
来改善我们社区的开放性和参与性


58
00:03:58,472 --> 00:04:01,508 line:-2
一些对于ResearchKit
用户界面和组件的更新


59
00:04:02,142 --> 00:04:07,047 line:-1
以及当前任务现有库的一些新内容


60
00:04:08,348 --> 00:04:10,751 line:-1
现在让我们从社区的更新开始说吧


61
00:04:11,084 --> 00:04:12,920 line:-1
我想谈两个主要话题


62
00:04:13,187 --> 00:04:15,989 line:-1
仓库特权和计划更新


63
00:04:17,156 --> 00:04:20,961 line:-2
过去的几个月
我们一直在扩展我们的访问权限


64
00:04:21,028 --> 00:04:24,164 line:-1
并且给我们社区的成员提供写入权限


65
00:04:24,932 --> 00:04:28,101 line:-1
事实上 我们选了五个明星贡献者


66
00:04:28,168 --> 00:04:31,305 line:-2
给了他们ResearchKit
仓库的直接访问权限


67
00:04:31,572 --> 00:04:34,308 line:-1
这能允许他们合并MPR


68
00:04:34,942 --> 00:04:37,544 line:-1
非常感谢 并且恭喜你们


69
00:04:37,778 --> 00:04:41,882 line:-2
Erin、Fernando、Nino
Ricardo 还有Shannon


70
00:04:43,083 --> 00:04:46,420 line:-1
接着我想讲一下我们的计划更新


71
00:04:46,887 --> 00:04:51,592 line:-2
之前我们一直在同时推送到
master和stable分支


72
00:04:52,025 --> 00:04:56,263 line:-2
我们意识到这样会制约
你们开发者的能力


73
00:04:56,797 --> 00:05:02,536 line:-2
为了权衡我们的一些内部功能
像是定位、可用性以及质量保证


74
00:05:03,136 --> 00:05:06,073 line:-2
这也是我们为什么
今年会推送到stable


75
00:05:06,373 --> 00:05:09,009 line:-2
比推送到master
要晚两到三个月


76
00:05:09,843 --> 00:05:11,612 line:-1
我们希望你们可以利用这段时间


77
00:05:12,145 --> 00:05:16,984 line:-2
来用下我们的最新更新
提供反馈并提交评论


78
00:05:17,651 --> 00:05:18,986 line:-1
而你们所做的变动


79
00:05:19,353 --> 00:05:23,657 line:-2
会使其很快出现在我们的稳定
发布分支


80
00:05:23,924 --> 00:05:27,594 line:-1
而不是让你等整个发布周期


81
00:05:29,162 --> 00:05:31,465 line:-1
现在我们已经介绍了社区的更新


82
00:05:31,532 --> 00:05:35,802 line:-1
我想更深入介绍下我们对于


83
00:05:36,069 --> 00:05:40,440 line:-2
ResearchKit框架本身所做的更新
从用户界面更新开始


84
00:05:41,775 --> 00:05:46,113 line:-2
作为比较
这是我们的迷你表格步骤


85
00:05:46,380 --> 00:05:50,784 line:-2
在ResearchKit 1.5中的样子
它是从ORK测试app中得到的


86
00:05:51,718 --> 00:05:53,787 line:-1
而这是相同的迷你表格步骤


87
00:05:54,354 --> 00:05:57,391 line:-2
在ResearchKit
2.0中的样子


88
00:05:58,592 --> 00:06:04,264 line:-2
如你所见
我们非常努力地在更新整个视图


89
00:06:04,331 --> 00:06:09,069 line:-2
让ResearchKit的界面
更接近最新的iOS样式指引


90
00:06:09,703 --> 00:06:11,104 line:-1
让我们仔细看一下


91
00:06:13,574 --> 00:06:16,610 line:-1
我们将进度标签从导航条的中心


92
00:06:16,677 --> 00:06:18,612 line:-1
移到了右边 并且做了一些修饰


93
00:06:19,313 --> 00:06:23,517 line:-1
这让我们补充了导航条的大标题功能


94
00:06:23,784 --> 00:06:26,186 line:-1
并将其app到我们的所有步骤标题中


95
00:06:26,987 --> 00:06:31,525 line:-2
为了保持一致的样式
我们还添加了一个新的卡片视图


96
00:06:31,992 --> 00:06:34,761 line:-1
用以改善整体的用户体验


97
00:06:34,828 --> 00:06:38,365 line:-1
对那些回答多个问题和调查问卷的人


98
00:06:38,565 --> 00:06:42,736 line:-2
因为它提供了一个你想要实现的
清晰的间断


99
00:06:44,505 --> 00:06:49,409 line:-2
这个卡片视图被默认app到
我们所有的步骤和表格中


100
00:06:49,476 --> 00:06:51,912 line:-1
我们还有一个新的布林属性


101
00:06:52,179 --> 00:06:54,882 line:-1
你可以将其设为假来保证向下兼容性


102
00:06:55,616 --> 00:06:59,119 line:-1
最后我们还加入了一个页脚容器视图


103
00:06:59,486 --> 00:07:01,355 line:-1
来改进导航流


104
00:07:01,822 --> 00:07:04,391 line:-1
取消按钮现在是页脚视图的一部分了


105
00:07:04,591 --> 00:07:06,426 line:-1
它会一直固定在底部


106
00:07:06,927 --> 00:07:09,029 line:-1
这就意味着你的用户


107
00:07:09,229 --> 00:07:12,533 line:-1
再也不用一直拉动到步骤的底部


108
00:07:13,066 --> 00:07:15,636 line:-1
来进入后面的导航选项了


109
00:07:15,936 --> 00:07:20,607 line:-2
把所有的控制功能放到一起
使其更为直观了


110
00:07:22,042 --> 00:07:23,143 line:-1
而接下来


111
00:07:23,577 --> 00:07:28,482 line:-2
在ResearchKit最常用到
的组件之一就是“知情同意”


112
00:07:29,149 --> 00:07:33,253 line:-1
它来生成PDF文件并附上用户签名


113
00:07:33,887 --> 00:07:37,558 line:-1
我们意识到在你的app中


114
00:07:37,624 --> 00:07:40,460 line:-1
向你的用户调查一些机密文件


115
00:07:40,827 --> 00:07:42,496 line:-1
是多么重要


116
00:07:42,763 --> 00:07:46,934 line:-2
这也是我们为什么加入了
一个新的ORK PDF浏览器步骤


117
00:07:47,467 --> 00:07:52,105 line:-2
它构建于去年推出的
PDFKit框架之上


118
00:07:52,873 --> 00:07:55,876 line:-1
让我们仔细看一些功能吧


119
00:07:55,943 --> 00:07:57,211 line:-1
这个步骤所提供的


120
00:07:59,546 --> 00:08:02,482 line:-1
用于轻松在页面间切换的快速导航


121
00:08:03,617 --> 00:08:07,187 line:-1
在需要时来标记文档的实时备注功能


122
00:08:08,055 --> 00:08:09,857 line:-1
搜索功能 让你的用户


123
00:08:09,923 --> 00:08:12,960 line:-1
通过关键词或短语来检索整个文档


124
00:08:13,427 --> 00:08:16,797 line:-1
以及分享或保存PDF


125
00:08:16,964 --> 00:08:19,132 line:-1
利用标准iOS共享表单


126
00:08:19,700 --> 00:08:24,271 line:-1
更棒的是将其放入你的app非常简单


127
00:08:25,372 --> 00:08:28,275 line:-2
你需要创建一个
ORKPDFViewerStep实例


128
00:08:28,342 --> 00:08:29,910 line:-1
一个唯一的标识符


129
00:08:30,344 --> 00:08:32,246 line:-1
然后提供给我们


130
00:08:32,312 --> 00:08:34,548 line:-1
你想要显示的PDF文档的文件路径


131
00:08:36,250 --> 00:08:39,019 line:-1
现在我想要换个话题


132
00:08:39,385 --> 00:08:42,856 line:-2
介绍下ResearchKit的
一个核心组件


133
00:08:44,291 --> 00:08:45,325 line:-1
当前任务


134
00:08:46,159 --> 00:08:51,231 line:-2
你们当中有人可能不熟悉
当前任务是个预封装的模块


135
00:08:51,431 --> 00:08:56,170 line:-1
能让用户执行特定任务或特定测试


136
00:08:56,436 --> 00:08:57,938 line:-1
在给定的时间内


137
00:08:58,505 --> 00:09:01,842 line:-2
当用户完成这个步骤时
开发者会收到


138
00:09:01,909 --> 00:09:04,945 line:-2
一个包含了ORKResult对象
的回调函数


139
00:09:05,646 --> 00:09:07,915 line:-1
这个对象由一些数据点组成


140
00:09:07,981 --> 00:09:12,019 line:-1
包括像是用户响应、定时信息


141
00:09:12,085 --> 00:09:16,623 line:-2
以及从不同源录制的数据
像是加速计、陀螺仪


142
00:09:17,057 --> 00:09:19,193 line:-1
治疗数据 甚至是你的麦克风


143
00:09:20,194 --> 00:09:24,031 line:-2
今年我们还加入了
对于健康记录的支持


144
00:09:24,898 --> 00:09:28,268 line:-2
现在让我们看下
在你的app中应该如何使用它


145
00:09:28,802 --> 00:09:30,871 line:-1
你的目标是要创建


146
00:09:30,938 --> 00:09:35,309 line:-2
一个记录配置
来查询健康诊断数据类型


147
00:09:35,709 --> 00:09:38,445 line:-1
你需要向我们提供两个重要参数


148
00:09:38,745 --> 00:09:40,948 line:-2
第一个是
HKClinicalType


149
00:09:41,481 --> 00:09:45,519 line:-2
第二个是可选的
HKFHIRResourceType


150
00:09:46,386 --> 00:09:48,956 line:-1
在你创建了这个记录配置后


151
00:09:49,156 --> 00:09:50,624 line:-1
你可以把它加到步骤中


152
00:09:51,158 --> 00:09:54,461 line:-1
现在当用户要执行任务的时候


153
00:09:54,761 --> 00:09:58,532 line:-2
他们就会看到HealthKit的
新授权用户界面


154
00:09:59,066 --> 00:10:02,703 line:-1
他们只有允许访问才能执行查询


155
00:10:03,237 --> 00:10:05,038 line:-1
而在用户完成任务后


156
00:10:05,272 --> 00:10:08,275 line:-2
作为你ORKResult对象中
代理回调的一部分


157
00:10:08,575 --> 00:10:12,813 line:-2
你还会从健康记录中
得到你请求的信息


158
00:10:13,614 --> 00:10:17,885 line:-2
为了更好理解和学习
这些不同的健康记录类型


159
00:10:18,252 --> 00:10:22,823 line:0
我强烈建议你们去参加这个演讲


160
00:10:23,123 --> 00:10:27,394 line:0
“通过HealthKit访问 健康数据”
该演讲下午三点开始


161
00:10:27,661 --> 00:10:31,632 line:0
它会详细介绍健康记录的所有内容


162
00:10:31,698 --> 00:10:34,301 line:-1
包括了一些很重要的app


163
00:10:35,802 --> 00:10:37,104 line:-1
我们现在已经介绍了


164
00:10:37,471 --> 00:10:40,207 line:-1
当前任务模块的整体更新


165
00:10:41,875 --> 00:10:44,478 line:-1
让我来介绍下当前任务本身吧


166
00:10:45,012 --> 00:10:50,017 line:-2
我们今年添加了聚焦于
健康三个主要领域的新模块


167
00:10:50,717 --> 00:10:53,320 line:-1
听力、语言表达以及视力


168
00:10:54,321 --> 00:10:55,789 line:-1
让我们从听力开始说


169
00:10:57,624 --> 00:11:01,828 line:-2
我们添加了一个新的
dBHL声调测听步骤


170
00:11:02,563 --> 00:11:05,265 line:-1
其实现了降十升五方法


171
00:11:05,666 --> 00:11:07,134 line:-1
并且让你决定


172
00:11:07,201 --> 00:11:11,205 line:-1
用户在dBHL量程内的听力阈值


173
00:11:12,606 --> 00:11:16,877 line:-2
为了实现这一点
并且确保你得到最精确的结果


174
00:11:17,211 --> 00:11:20,080 line:-1
我非常激动地说


175
00:11:20,547 --> 00:11:24,017 line:-2
我们首次开源了
AirPods的校准数据


176
00:11:24,618 --> 00:11:26,887 line:-1
有三个表格


177
00:11:27,254 --> 00:11:32,426 line:-2
第一个是AirPods
在所有iOS设备上的音量曲线


178
00:11:32,893 --> 00:11:36,129 line:-1
第二个是每频率的敏感度


179
00:11:36,196 --> 00:11:39,633 line:-2
这里的敏感度是以
分贝声压级别来测量的


180
00:11:40,000 --> 00:11:42,035 line:-1
最后我们还提供了


181
00:11:42,269 --> 00:11:47,040 line:-2
参考同声压级表单
或者叫RETSPL


182
00:11:48,642 --> 00:11:53,614 line:-2
需要注意的是
RETSPL表仍在测试中


183
00:11:54,348 --> 00:11:58,385 line:-2
这意味着我们在积极地运行
内部校验和测试


184
00:11:58,652 --> 00:11:59,853 line:-1
在接下来的数周内


185
00:11:59,920 --> 00:12:04,491 line:-2
随着我们集中精确的数据
我们会更新这些表


186
00:12:05,926 --> 00:12:09,596 line:-2
现在让我们看下
当前任务是怎么运作的


187
00:12:10,764 --> 00:12:13,567 line:-1
用户需要听声调


188
00:12:14,067 --> 00:12:18,572 line:-1
在特定频率不同的分贝值


189
00:12:19,306 --> 00:12:20,674 line:-1
当用户听到一个声调时


190
00:12:20,741 --> 00:12:23,911 line:-2
他们要按下按钮
来表示他们听到了声音


191
00:12:24,278 --> 00:12:27,648 line:-1
这时候我们会开始降低分贝值


192
00:12:27,915 --> 00:12:30,284 line:-1
就像是这里的绿点所表示的那样


193
00:12:30,784 --> 00:12:35,222 line:-1
当用户没能在给定时间内点击按钮时


194
00:12:35,289 --> 00:12:39,293 line:-2
我们会如红点所示那样
开始提高分贝值


195
00:12:39,593 --> 00:12:41,929 line:-1
我们会将这些数据点提供给


196
00:12:41,995 --> 00:12:44,965 line:-1
降十升五方法来决定


197
00:12:45,032 --> 00:12:48,535 line:-1
用户以分贝计的听力阈值


198
00:12:52,005 --> 00:12:53,774 line:-1
从开发者的角度


199
00:12:54,041 --> 00:12:57,845 line:-1
整个声调的产生发生在三个阶段


200
00:12:58,445 --> 00:13:00,981 line:-1
首先第一个是预刺激延迟


201
00:13:01,381 --> 00:13:04,685 line:-2
这是个以秒计的开发者定义的
最大数值


202
00:13:04,952 --> 00:13:09,356 line:-2
我们用它来生成一个
从1到该值间的随机延迟


203
00:13:09,423 --> 00:13:11,592 line:-1
在我们将该声调播放给用户之前


204
00:13:12,292 --> 00:13:14,895 line:-1
这是为了确保用户不会欺骗测试


205
00:13:15,162 --> 00:13:17,030 line:-1
通过随机点击按钮


206
00:13:18,599 --> 00:13:22,102 line:-1
我们还提供了一个声调持续的属性


207
00:13:22,169 --> 00:13:25,405 line:-1
其监控了播放声调的实际持续时间


208
00:13:26,173 --> 00:13:30,244 line:-1
最后就是后刺激延迟 也就是


209
00:13:30,310 --> 00:13:34,081 line:-1
用户响应特定声调的时间


210
00:13:35,082 --> 00:13:36,817 line:-1
为了在你的app中实现


211
00:13:37,684 --> 00:13:41,922 line:-2
你需要创建一个
ORKdBHLToneAudiometryStep实例


212
00:13:41,989 --> 00:13:46,560 line:-2
一个唯一的标识符
并提供我们一些参数值


213
00:13:46,627 --> 00:13:50,297 line:-1
包括频率表 也就是一个频率数组


214
00:13:50,731 --> 00:13:52,766 line:-1
你想要将其回播给你的用户


215
00:13:53,367 --> 00:13:55,169 line:-1
我们还有更多的属性


216
00:13:55,235 --> 00:13:58,605 line:-1
你可以定制你特有的用例


217
00:14:00,541 --> 00:14:04,011 line:-2
当用户完成这项任务时
作为代理回调的一部分


218
00:14:04,511 --> 00:14:06,947 line:-2
ORKResult对象
会被返回给你


219
00:14:07,414 --> 00:14:11,251 line:-2
让我们看看
这个任务中的对象是什么样的


220
00:14:12,085 --> 00:14:13,253 line:-1
在最上层 你会得到


221
00:14:13,320 --> 00:14:16,490 line:-2
很多信息
包括像是输出音量


222
00:14:16,924 --> 00:14:19,893 line:-1
还有示例对象的数组


223
00:14:20,527 --> 00:14:24,731 line:-1
这些对象会封装像是声道这类东西


224
00:14:24,798 --> 00:14:28,769 line:-2
也就是声音是从左还是右声道
播放给用户的


225
00:14:29,436 --> 00:14:30,871 line:-1
还有阈值


226
00:14:30,938 --> 00:14:33,106 line:-1
其是由降十升五方法决定的


227
00:14:33,807 --> 00:14:36,643 line:-1
它也是由单元对象数组组成的


228
00:14:37,344 --> 00:14:41,348 line:-2
而单元对象会提供
像是分贝值这样的细节


229
00:14:41,648 --> 00:14:44,051 line:-1
在该分贝下 特定的声调被播放


230
00:14:44,117 --> 00:14:45,752 line:-1
还有很多的时间戳


231
00:14:45,819 --> 00:14:49,823 line:-1
它包含了用户是何时点击按钮的


232
00:14:51,091 --> 00:14:55,696 line:-2
接下来让我们介绍听力类的
下一个任务


233
00:14:56,363 --> 00:14:59,199 line:-1
我们加入了一个环境SPL测量计


234
00:15:00,334 --> 00:15:02,903 line:-1
它实现了一个A级过滤器


235
00:15:03,136 --> 00:15:07,174 line:-1
用来以分贝测量环境的声压级别


236
00:15:07,808 --> 00:15:10,410 line:-1
换句话说 它告诉你有多吵


237
00:15:11,011 --> 00:15:14,314 line:-1
这个步骤现在还接受了阈值


238
00:15:14,848 --> 00:15:16,483 line:-1
这让事情变得有趣了


239
00:15:16,550 --> 00:15:20,187 line:-2
因为你现在可以将该步骤
作为门户步骤了


240
00:15:20,754 --> 00:15:24,858 line:-2
例如 如果你想让你的用户
执行声调测听任务


241
00:15:25,225 --> 00:15:29,696 line:-2
你可以在此之前加入这个任务
以确保你的用户不会


242
00:15:29,763 --> 00:15:33,600 line:-2
处于一个太吵的环境
以至于影响任务执行的精确性


243
00:15:36,403 --> 00:15:40,407 line:-2
要加入这个任务 你需要创建一个
ORKEnvironmentSPLMeterStep实例


244
00:15:40,474 --> 00:15:43,944 line:-2
一个唯一的标识符
并向我们提供阈值


245
00:15:44,845 --> 00:15:48,482 line:-2
我们还有一些
你可以后面定制用的属性


246
00:15:50,317 --> 00:15:55,889 line:-2
现在让我们换个话题
转到下一个分类 讲话


247
00:15:57,191 --> 00:15:59,960 line:-1
我们加入了一个语音识别模块


248
00:16:00,327 --> 00:16:03,797 line:-1
其利用了iOS上的语音识别框架


249
00:16:04,064 --> 00:16:08,368 line:-1
它让我们可直接访问实时语音识别器


250
00:16:08,435 --> 00:16:11,438 line:-1
它支持超过50种不同的语言


251
00:16:12,206 --> 00:16:14,474 line:-1
作为这个任务的一部分


252
00:16:14,541 --> 00:16:18,445 line:-1
用户要重复一个句子或描述一个图像


253
00:16:19,213 --> 00:16:20,914 line:-1
在他们说完之后


254
00:16:21,315 --> 00:16:24,818 line:-1
用户会自动进入下一步


255
00:16:25,352 --> 00:16:28,222 line:-1
他们可以编辑生成的脚本


256
00:16:28,822 --> 00:16:31,425 line:-1
例如在本例中 quick和fox


257
00:16:31,491 --> 00:16:34,027 line:-1
被错误理解成quiet和box


258
00:16:34,494 --> 00:16:38,866 line:-2
你的用户可以点击这些词
如果需要的话还可以编辑


259
00:16:40,234 --> 00:16:42,669 line:-2
需要注意的是
作为此项任务的一部分


260
00:16:42,736 --> 00:16:44,104 line:-1
我们会返回给你


261
00:16:44,438 --> 00:16:48,075 line:-2
一个非常丰富的数据集
包括三个主要内容


262
00:16:48,976 --> 00:16:51,645 line:-1
一个用户所说内容的直接录音


263
00:16:52,412 --> 00:16:55,182 line:-1
由语音识别引擎生成的脚本


264
00:16:55,916 --> 00:16:58,585 line:-1
以及由用户编辑的脚本


265
00:17:00,387 --> 00:17:01,455 line:-1
要将其加入你的app


266
00:17:01,522 --> 00:17:04,758 line:-2
你需要创建一个
ORKSpeechRecognitionStep实例


267
00:17:04,825 --> 00:17:07,861 line:-2
并提供给我们一个用户界面图像
或一个字符串


268
00:17:08,829 --> 00:17:12,266 line:-1
你也能为此识别定制本地化


269
00:17:12,900 --> 00:17:15,903 line:-1
并且你还可以得到实时脚本


270
00:17:15,969 --> 00:17:17,237 line:-1
随着用户讲话


271
00:17:18,605 --> 00:17:20,406 line:-1
让我们详细看下


272
00:17:20,473 --> 00:17:23,343 line:-1
我们结果对象的一个子集吧


273
00:17:24,545 --> 00:17:26,780 line:-1
这是SFTranscription类型


274
00:17:27,047 --> 00:17:29,416 line:-1
由语音识别框架所展现的


275
00:17:29,883 --> 00:17:32,219 line:-1
该格式化字符串提供了脚本


276
00:17:32,553 --> 00:17:34,321 line:-1
以及分段对象数组


277
00:17:34,555 --> 00:17:37,658 line:-1
将脚本分成了子字符串


278
00:17:37,925 --> 00:17:41,028 line:-2
还为每个子字符串
提供了一个保密等级


279
00:17:41,595 --> 00:17:45,299 line:-2
在这之上
它们还提供了一个替代字符串的数组


280
00:17:45,632 --> 00:17:48,068 line:-1
在这个解释例子中 你们可以看到


281
00:17:49,603 --> 00:17:53,907 line:-1
这些结果可以用来获取句法、文法的


282
00:17:54,174 --> 00:17:56,944 line:-1
语言学功能 还有讲话速率


283
00:17:57,444 --> 00:18:02,082 line:-1
以评估在不同医疗情况下的说话模式


284
00:18:02,149 --> 00:18:04,117 line:-1
包括认知和情绪


285
00:18:07,120 --> 00:18:12,526 line:-2
有趣的是 我们的下一个任务
是说话和听力的结合


286
00:18:13,861 --> 00:18:15,829 line:-1
讲话与噪音当前任务


287
00:18:16,597 --> 00:18:20,400 line:-1
这能让你实现完全自动化的语音测听


288
00:18:21,668 --> 00:18:25,639 line:-1
传统的声音测听使用纯声音


289
00:18:25,706 --> 00:18:27,541 line:-1
也就是记号波形


290
00:18:28,141 --> 00:18:29,843 line:-1
对于录好的实例


291
00:18:29,910 --> 00:18:32,946 line:-1
用户可以清晰地分辨纯语音


292
00:18:33,013 --> 00:18:35,849 line:-1
但是要分辨词语非常困难


293
00:18:36,049 --> 00:18:37,885 line:-1
当这些词语跟噪音混在一起的时候


294
00:18:38,919 --> 00:18:40,621 line:-1
这很接近于


295
00:18:40,687 --> 00:18:44,525 line:-1
现实世界中早期听力下降的例子


296
00:18:44,825 --> 00:18:47,928 line:-1
例如 在一个吵闹的餐厅里


297
00:18:47,995 --> 00:18:51,698 line:-2
当你不能理解
坐在你前面的人在说什么时


298
00:18:52,966 --> 00:18:54,902 line:-1
在我详细介绍之前


299
00:18:54,968 --> 00:18:58,238 line:-2
让我们来看看
这个任务是如何运作的


300
00:19:03,877 --> 00:19:05,746 line:-1
行动器尝试了三张绿色的图片


301
00:19:08,215 --> 00:19:11,151 line:-2
正如你所看到的
一个音频文件被播放给用户


302
00:19:11,318 --> 00:19:12,352 line:-1
而在完成了之后


303
00:19:12,419 --> 00:19:15,756 line:-2
他们会被要求立即
重复他们刚刚听到的内容


304
00:19:17,858 --> 00:19:23,664 line:-1
语音和噪音当前任务使用音频文件


305
00:19:23,730 --> 00:19:28,068 line:-2
这些音频文件是由一个闭集矩阵的
五个词语组成 它们被内部校验了


306
00:19:28,135 --> 00:19:32,539 line:-1
像是词语相似度、一致性、困难度


307
00:19:32,806 --> 00:19:36,777 line:-1
还有确保由同步器生成的句子


308
00:19:37,244 --> 00:19:40,681 line:-2
发音更平衡
更重要的是 是一致的


309
00:19:41,982 --> 00:19:44,518 line:-1
这些文件然后被用程序混合


310
00:19:44,585 --> 00:19:49,256 line:-2
与背景噪音一起
在不同的信噪比或SNR之下


311
00:19:50,157 --> 00:19:51,491 line:-1
开发者也可以


312
00:19:51,558 --> 00:19:56,063 line:-1
为所有的噪音信号设置增益的值


313
00:19:58,098 --> 00:20:03,270 line:-2
语音接收阈值
被定义为SNR中的最小值


314
00:20:03,470 --> 00:20:07,641 line:-1
用户只能理解50%的口述词语


315
00:20:10,077 --> 00:20:13,981 line:-2
我们这个测试的愿景是
在接下来的几周内


316
00:20:14,047 --> 00:20:17,784 line:-1
我们会上传超过175个不同的文件


317
00:20:18,785 --> 00:20:22,222 line:-2
对应了25个列表
每个都有7个句子


318
00:20:22,623 --> 00:20:23,724 line:-1
而从长远来看


319
00:20:24,057 --> 00:20:27,661 line:-1
我们希望这个测试能够支持多种语言


320
00:20:27,961 --> 00:20:31,865 line:-2
特别是目前那些言语
和噪音是不能实现的


321
00:20:32,165 --> 00:20:35,936 line:-1
由于缺乏语音数据库或其他测试资源


322
00:20:36,436 --> 00:20:39,306 line:-1
如果你是这个特定领域的研究者


323
00:20:39,540 --> 00:20:41,708 line:-1
并且你对特定的区域设置有需求


324
00:20:42,309 --> 00:20:44,478 line:-1
我强烈建议你与我们联系


325
00:20:44,678 --> 00:20:46,914 line:-1
我们会尽力达成您的要求


326
00:20:49,550 --> 00:20:50,651 line:-1
要将这个功能加入你的app


327
00:20:50,717 --> 00:20:53,353 line:-2
你需要创建一个
ORKSpeechInNoiseStep实例


328
00:20:53,921 --> 00:20:56,290 line:-1
并将我们指向你想播放的音频文件


329
00:20:56,723 --> 00:20:59,960 line:-2
你也可以指定
app于噪声信号的增益


330
00:21:02,496 --> 00:21:05,098 line:-1
最后 让我们来介绍下视觉


331
00:21:06,600 --> 00:21:09,970 line:-1
Amsler网格是一种工具


332
00:21:10,270 --> 00:21:13,740 line:-1
用于检测在用户视野中的问题


333
00:21:13,807 --> 00:21:19,379 line:-2
可能是由于像黄斑变性
这种问题造成的


334
00:21:20,547 --> 00:21:25,586 line:-1
这个测试通常在医生的办公室进行


335
00:21:25,819 --> 00:21:27,387 line:-1
在一张传统的纸上


336
00:21:27,454 --> 00:21:30,057 line:-2
显示一个图形
就像你在这里看到的那样


337
00:21:30,891 --> 00:21:34,127 line:-2
视力好的用户
会看清楚这个图形


338
00:21:34,361 --> 00:21:36,730 line:-1
而受到某些疾病困扰的用户


339
00:21:36,930 --> 00:21:39,433 line:-1
会开始在这张图上看到扭曲


340
00:21:40,167 --> 00:21:43,837 line:-2
如果你现在看到扭曲
请不要惊慌


341
00:21:44,271 --> 00:21:47,140 line:-2
这是故意的
为了添加戏剧效果


342
00:21:48,075 --> 00:21:51,812 line:-2
用户只需指向
他们看到的网格上的扭曲


343
00:21:52,212 --> 00:21:55,549 line:-2
通过复制这个网格
我们能够将


344
00:21:55,616 --> 00:21:59,620 line:-2
这个任务的功能带到
用户家中的设备上


345
00:21:59,987 --> 00:22:03,891 line:-2
用户只需在他们看到扭曲的网格上
标注区域


346
00:22:05,025 --> 00:22:10,330 line:-2
我们相信开发者可以
利用一些令人兴奋的iOS功能


347
00:22:10,631 --> 00:22:12,866 line:-2
像是压力按压
或是深度感测相机


348
00:22:13,133 --> 00:22:18,005 line:-1
来增加正在进行这一任务的用户体验


349
00:22:19,306 --> 00:22:20,908 line:-1
所有这些当前任务


350
00:22:20,974 --> 00:22:24,611 line:-1
用于数据收集和分析真的很棒


351
00:22:25,345 --> 00:22:30,117 line:-2
但它们被设计成要在给定时间段
的特定时间执行


352
00:22:31,251 --> 00:22:35,322 line:-2
当我们研究具体问题时
我们意识到


353
00:22:35,389 --> 00:22:39,860 line:-2
一些更复杂的健康问题
需要不断监测


354
00:22:40,127 --> 00:22:45,299 line:-2
因此我们引入了
被动、非侵入性数据收集


355
00:22:45,999 --> 00:22:47,301 line:-1
为了介绍更多的相关信息


356
00:22:47,534 --> 00:22:49,837 line:-1
我想请Gabriel上台来


357
00:22:56,577 --> 00:22:57,411 line:-1
大家好


358
00:22:57,911 --> 00:23:01,315 line:-2
我叫Gabriel
我代表Core Motion团队


359
00:23:01,582 --> 00:23:04,051 line:-1
来介绍一个新的研究API


360
00:23:04,585 --> 00:23:05,919 line:-1
运动障碍API


361
00:23:09,089 --> 00:23:13,193 line:-2
正如Srinath所提及的
这是个被动的 全天监测的API


362
00:23:13,760 --> 00:23:15,329 line:-1
在Apple Watch上可用


363
00:23:15,596 --> 00:23:18,765 line:-2
这将允许你监测
运动障碍的症状


364
00:23:19,066 --> 00:23:21,602 line:-1
特别是两种运动障碍


365
00:23:21,802 --> 00:23:24,738 line:-1
跟帕金森病的研究有关


366
00:23:25,672 --> 00:23:29,710 line:-2
因为现在有针对性的用例
作为研究API


367
00:23:30,244 --> 00:23:32,579 line:-2
你需要申请一个
特殊的代码签名权利


368
00:23:32,880 --> 00:23:34,314 line:-1
来使用这个API


369
00:23:35,115 --> 00:23:38,352 line:-2
这个app将会
在Apple开发者门户网站完成


370
00:23:38,652 --> 00:23:39,920 line:-1
从第二波开始


371
00:23:40,687 --> 00:23:43,323 line:-2
如果你等不及了
我也不怪你


372
00:23:44,224 --> 00:23:47,694 line:-2
将会有样本数据集
以及可用的演示代码


373
00:23:47,761 --> 00:23:50,864 line:-2
在ResearchKit
这个演讲的GitHub仓库里


374
00:23:51,965 --> 00:23:55,936 line:-2
让我们来谈论下
这两个运动障碍症状


375
00:23:58,939 --> 00:23:59,840 line:-1
正如你们有些人可能知道的那样


376
00:24:00,474 --> 00:24:03,410 line:-1
帕金森病是一种退行性神经疾病


377
00:24:03,911 --> 00:24:06,813 line:-1
这会影响受疾病困扰患者的运动机能


378
00:24:07,781 --> 00:24:11,752 line:-2
帕金森病的其中一种可识别症状
是震颤


379
00:24:12,219 --> 00:24:17,391 line:-2
此API监测静止时的震颤
其特征在于


380
00:24:17,691 --> 00:24:23,497 line:-2
当某人不想移动
而身体产生抖动


381
00:24:24,665 --> 00:24:28,302 line:-2
现在 有相应的治疗措施
包括药物治疗


382
00:24:28,635 --> 00:24:32,206 line:-2
可以帮助抑制和控制
帕金森症状


383
00:24:32,673 --> 00:24:36,910 line:-2
然而 这些非常相似的治疗
往往会产生不良的副作用


384
00:24:37,644 --> 00:24:40,047 line:-1
例如运动障碍


385
00:24:41,682 --> 00:24:45,452 line:-2
这个API可以监控的
一种运动障碍症状


386
00:24:45,886 --> 00:24:49,690 line:-2
就是表现出烦躁以及身体摇摆
被称作舞蹈病样综合征


387
00:24:50,924 --> 00:24:55,629 line:-2
那么让我们回顾一下 你有震颤
这种疾病的表现


388
00:24:55,963 --> 00:24:59,333 line:-2
以及运动障碍
治疗后产生的副作用


389
00:25:00,334 --> 00:25:04,571 line:-2
让我们看下
研究人员和临床医生目前用到的工具


390
00:25:04,638 --> 00:25:05,873 line:-1
来评估这些症状


391
00:25:08,842 --> 00:25:11,845 line:-2
通常这些类型的评估
是在诊所完成的


392
00:25:12,479 --> 00:25:15,749 line:-2
临床医生会要求
帕金森病患者


393
00:25:16,049 --> 00:25:18,986 line:-1
进行物理诊断测试


394
00:25:19,186 --> 00:25:22,389 line:-2
以评估
他们状况的严重程度


395
00:25:23,457 --> 00:25:28,195 line:-2
这些评级提供了量化数据
尽管受到评估者自身


396
00:25:28,262 --> 00:25:31,431 line:-2
以及在诊所
测量时的身体状况的影响


397
00:25:32,566 --> 00:25:34,735 line:-2
为了得到更广泛的
和更完整的描述


398
00:25:35,135 --> 00:25:37,371 line:-1
我们会鼓励患者坚持写日记


399
00:25:37,738 --> 00:25:39,540 line:-1
手动记录他们的症状


400
00:25:40,807 --> 00:25:42,876 line:-1
但是 这可能对于患者来说很麻烦


401
00:25:43,110 --> 00:25:45,279 line:-1
有些人可能会忘记 这很正常


402
00:25:45,345 --> 00:25:50,984 line:-1
或无法完整描述他们每天的症状


403
00:25:52,486 --> 00:25:53,320 line:-1
这不是很好吗


404
00:25:54,454 --> 00:25:59,960 line:-2
若有一种被动的、不引人注目的方式
来监测这些症状


405
00:26:01,128 --> 00:26:04,097 line:-1
通过使用运动障碍API


406
00:26:04,631 --> 00:26:07,301 line:-1
像你们一样的研究人员和开发人员


407
00:26:07,534 --> 00:26:11,004 line:-2
将能够构建app
来持续收集这些类型的数据


408
00:26:12,272 --> 00:26:16,310 line:-2
病人戴着Apple Watch时
不断收集


409
00:26:17,344 --> 00:26:20,614 line:-2
这不仅会给你一个量化的方式
测量这些症状


410
00:26:21,081 --> 00:26:24,151 line:-2
在这里显示
作为观察时间的百分比


411
00:26:24,685 --> 00:26:27,721 line:-1
它也会给你一个纵向的分析


412
00:26:27,921 --> 00:26:30,724 line:-1
你可以持续跟踪这些症状的变化


413
00:26:31,658 --> 00:26:35,362 line:-1
这些算法的设计和试用


414
00:26:35,629 --> 00:26:39,366 line:-2
来自于内部临床研究中
收集的帕金森患者的数据


415
00:26:40,367 --> 00:26:44,171 line:-2
我们希望你可以使用
这些工具和这些知识


416
00:26:44,438 --> 00:26:49,476 line:-2
来建立新的护理体验
用以提高


417
00:26:49,543 --> 00:26:50,844 line:-1
帕金森患者的生活质量


418
00:26:52,513 --> 00:26:54,248 line:-1
但是在你这么做之前


419
00:26:54,815 --> 00:26:56,984 line:-2
你需要知道
如何使用这个API 对吧？


420
00:26:57,684 --> 00:26:59,820 line:-1
那么 让我们看下代码


421
00:27:01,989 --> 00:27:05,526 line:-1
你首先需要做的是请求


422
00:27:05,592 --> 00:27:08,729 line:-2
来自用户的运动授权
以便使用他们的运动障碍数据


423
00:27:09,930 --> 00:27:14,334 line:-2
一旦你完成这些 你需要调用
monitorKinesias函数


424
00:27:14,635 --> 00:27:16,770 line:-1
以启用症状监测


425
00:27:17,738 --> 00:27:22,009 line:-2
现在这个症状监视器会打开
Apple Watch另外传感器


426
00:27:22,242 --> 00:27:25,479 line:-2
所以这会对你用户的电池寿命
产生影响


427
00:27:25,779 --> 00:27:28,549 line:-1
不过他们仍然可以收集一天的数据


428
00:27:28,615 --> 00:27:29,550 line:-1
仅充一次电


429
00:27:31,518 --> 00:27:34,955 line:-1
正如你所见 最长的录音时间为七天


430
00:27:35,589 --> 00:27:37,157 line:-1
我知道你们中的很多人将要进行


431
00:27:37,224 --> 00:27:39,259 line:-1
超过7天的研究


432
00:27:39,526 --> 00:27:43,530 line:-2
如果是这样 只要再次调用
monitorKinesias函数


433
00:27:43,730 --> 00:27:45,799 line:-1
来扩展你的数据收集间隔


434
00:27:47,234 --> 00:27:50,871 line:-2
这将开始在你的授意下
在用户的设备上


435
00:27:51,038 --> 00:27:53,106 line:-1
存储震颤和运动障碍结果


436
00:27:54,408 --> 00:27:56,076 line:-1
之后的某个时候 你将要


437
00:27:56,143 --> 00:27:59,146 line:-2
返回到该app
你也可以查询这些记录


438
00:27:59,379 --> 00:28:01,515 line:-2
让我们看下
查询函数是什么样的


439
00:28:04,218 --> 00:28:08,021 line:-2
如你所见 在这行
我们查询自我们上次查询起


440
00:28:08,088 --> 00:28:11,391 line:-1
新的存储在设备上的震颤记录


441
00:28:13,060 --> 00:28:16,563 line:-1
这些记录通过API存储在设备上


442
00:28:17,197 --> 00:28:19,566 line:-1
但在7天后失效


443
00:28:20,000 --> 00:28:21,702 line:-1
所以 在它失效之前


444
00:28:21,902 --> 00:28:24,605 line:-1
你需要获得这些数据的所有权


445
00:28:25,138 --> 00:28:29,276 line:-2
通过序列化它们
并将它们存储在这个设备上


446
00:28:29,610 --> 00:28:33,146 line:-2
或传输到不同的平台
这样你就可以将其可视化并进行分析


447
00:28:34,414 --> 00:28:38,852 line:-2
数据将返回给你
作为分钟长的对象的数组


448
00:28:39,286 --> 00:28:43,223 line:-2
所以一小时的数据
有60个结果对象


449
00:28:43,690 --> 00:28:46,660 line:-2
我们来看一看
其中一个结果对象是什么样的


450
00:28:52,165 --> 00:28:56,203 line:-2
正如你所看到的那样 结果对象
返回了时间的百分比


451
00:28:56,270 --> 00:28:59,806 line:-2
一分钟的百分比
该算法能够观察到


452
00:28:59,873 --> 00:29:02,943 line:-1
存在或不存在的症状


453
00:29:03,777 --> 00:29:07,347 line:-2
对于运动障碍 在右侧
你可以看到这很简单


454
00:29:07,981 --> 00:29:09,583 line:-1
可能或者不太可能


455
00:29:10,817 --> 00:29:12,419 line:-1
震颤给你更多的选择


456
00:29:12,853 --> 00:29:13,687 line:-1
让我们来看看


457
00:29:16,690 --> 00:29:20,494 line:-2
由于这个是在静止时的震颤
任何活动或混乱的动作


458
00:29:20,561 --> 00:29:23,263 line:-1
都会简单地返回未知百分比


459
00:29:23,730 --> 00:29:27,134 line:-2
这和我们使用微弱信号
是同一类别


460
00:29:27,201 --> 00:29:28,602 line:-1
我们无法做出决定


461
00:29:29,636 --> 00:29:32,573 line:-1
然而如果这个算法可以做出决定


462
00:29:32,639 --> 00:29:34,541 line:-1
它可以返回震颤的严重程度


463
00:29:34,842 --> 00:29:37,377 line:-1
范围从轻微到强烈


464
00:29:38,846 --> 00:29:44,117 line:-2
现在 为了给你展示
被动监视下的运动障碍API


465
00:29:44,418 --> 00:29:45,252 line:-1
能够与ResearchKit


466
00:29:45,319 --> 00:29:49,223 line:-2
的当前任务中的
主动监测结合使用


467
00:29:49,623 --> 00:29:52,025 line:-1
我想把Akshay请到舞台上


468
00:29:52,359 --> 00:29:56,163 line:-2
他将两者整合
成为一个一流的研究程序


469
00:30:03,470 --> 00:30:04,371 line:-1
大家好


470
00:30:04,438 --> 00:30:06,740 line:-1
欢迎来到护理研究进阶的演示


471
00:30:07,307 --> 00:30:09,676 line:-2
在这个演示中 我们会看到一些
ResearchKit更新


472
00:30:10,043 --> 00:30:12,246 line:-1
并创建运动障碍API


473
00:30:13,046 --> 00:30:14,948 line:-2
作为我们在GitHub仓库上
ResearchKit的一部分


474
00:30:15,282 --> 00:30:17,818 line:-2
我们已经添加
ORK帕金森的研究app


475
00:30:18,552 --> 00:30:20,854 line:-2
这个app实现了
运动障碍API


476
00:30:21,455 --> 00:30:24,791 line:-2
并可视化了
震颤和运动障碍


477
00:30:24,858 --> 00:30:26,226 line:-1
以及我们刚刚看到的症状数据点


478
00:30:26,827 --> 00:30:28,495 line:-2
我们现在看看
这个app长什么样


479
00:30:36,904 --> 00:30:37,938 line:-2
我们有一个
Apple Watch app


480
00:30:38,605 --> 00:30:40,674 line:-1
我们实现了运动障碍API


481
00:30:41,041 --> 00:30:43,277 line:-2
收集了震颤和运动障碍
症状数据点


482
00:30:43,510 --> 00:30:44,611 line:-1
并将它们发送给电话app


483
00:30:45,312 --> 00:30:48,515 line:-2
在iPhone app中
我们有一个调查问卷 一些主动任务


484
00:30:49,016 --> 00:30:51,919 line:-2
并且可视化了这些震颤
和运动障碍症状数据点


485
00:30:52,719 --> 00:30:56,023 line:-2
让我们看看代码是什么样的
对于这个演示 我们将


486
00:30:56,089 --> 00:30:58,859 line:-2
从一个基础乏味的平面app开始
并试着重建这个app


487
00:31:02,896 --> 00:31:05,232 line:-2
这是我的Xcode工作空间
如你们所见


488
00:31:05,299 --> 00:31:08,335 line:-2
在我的ResearchKit中
我有个帕金森研究app


489
00:31:08,969 --> 00:31:10,404 line:-2
我们有一个任务列表
viewController


490
00:31:10,904 --> 00:31:13,340 line:-2
我们将在那里添加
所有当前任务和问卷


491
00:31:13,807 --> 00:31:16,310 line:-2
一个graphviewController
我们将在那里进行可视化


492
00:31:16,376 --> 00:31:18,212 line:-2
这些震颤和运动障碍
症状数据点


493
00:31:18,679 --> 00:31:19,813 line:0
还有一个评估管理器


494
00:31:19,880 --> 00:31:22,349 line:0
我们将在那里实现
运动障碍API


495
00:31:23,884 --> 00:31:27,154 line:0
当帕金森病患者
或者说PD患者去看医生


496
00:31:27,454 --> 00:31:29,189 line:-1
他们会被问及一些特定的问题


497
00:31:29,590 --> 00:31:32,526 line:-1
这些问题包括他们日常生活的活动


498
00:31:32,593 --> 00:31:34,962 line:-1
例如从0到10分


499
00:31:35,028 --> 00:31:36,230 line:-1
今天你的痛苦程度如何


500
00:31:36,864 --> 00:31:38,932 line:-2
或者你有什么非运动症状
的感觉吗


501
00:31:39,666 --> 00:31:42,302 line:-2
我们已经在我们的app中
添加了这些问题的一个子集


502
00:31:42,836 --> 00:31:46,206 line:-1
这些问卷通常伴随着七次物理测试


503
00:31:46,273 --> 00:31:48,976 line:-2
其中一个物理测试
是评估讲话的清晰度


504
00:31:49,376 --> 00:31:52,246 line:-2
让我们继续并添加
语音识别当前任务


505
00:31:53,480 --> 00:31:54,882 line:-1
在我的任务列表视图控制器中


506
00:31:55,616 --> 00:31:58,085 line:-2
我会继续并添加
语音识别当前任务


507
00:31:58,652 --> 00:31:59,486 line:-1
如你所见


508
00:31:59,553 --> 00:32:02,422 line:-2
我们只加入了一个
ORKOrderedTask类型的语音识别


509
00:32:03,023 --> 00:32:07,027 line:-2
如果注意的话 其中一个参数
是语音识别器语言环境


510
00:32:07,394 --> 00:32:09,630 line:-2
这是由ResearchKit
提供的项目


511
00:32:09,897 --> 00:32:12,833 line:-1
表示所有支持的语言环境


512
00:32:12,900 --> 00:32:14,034 line:-1
被语音识别API所支持


513
00:32:14,401 --> 00:32:17,538 line:-1
让你作为开发者不必担心


514
00:32:17,604 --> 00:32:20,307 line:-2
你的语言环境是否
被这些语音识别API支持


515
00:32:21,408 --> 00:32:23,577 line:-2
现在 让我们接着介绍
评估管理器


516
00:32:24,945 --> 00:32:27,848 line:-2
正如Gabriel提到的那样
我们有一个叫做管理器的变量


517
00:32:27,915 --> 00:32:30,517 line:-2
CMMovementDisorderManager
类型的


518
00:32:31,218 --> 00:32:33,387 line:-2
如果你注意的话
我们有一个函数调用


519
00:32:33,453 --> 00:32:36,757 line:-2
monitorKinesias
方法来计算七天的最大持续时间


520
00:32:37,357 --> 00:32:39,226 line:-1
让我们在初始化程序中调用它


521
00:32:40,694 --> 00:32:45,399 line:-2
不管谁创建了一个
AssessmentManager类型的对象


522
00:32:45,666 --> 00:32:48,035 line:-2
都能开始震颤
和运动障碍症状查询


523
00:32:49,069 --> 00:32:50,470 line:-1
一旦我们收集了这些数据


524
00:32:50,537 --> 00:32:52,472 line:-2
我们也需要一个方法来查询
这些数据点


525
00:32:52,673 --> 00:32:55,509 line:-2
让我们继续添加一个方法
来查询这些数据


526
00:32:56,476 --> 00:33:01,215 line:-2
我添加了一个新的queryNewAssessments方法
我在那里调用queryTremor方法


527
00:33:01,281 --> 00:33:05,052 line:-2
对于给定的开始日期和结束日期
和queryDyskineticSymptom方法


528
00:33:05,485 --> 00:33:07,087 line:-1
为相同的开始日期和结束日期


529
00:33:09,456 --> 00:33:12,693 line:-2
为了这个演示 我们已经运行了
这个查询并收集震颤


530
00:33:12,759 --> 00:33:15,596 line:-2
以及运动障碍症状数据点
并将它们保存为JSON文件的一部分


531
00:33:16,029 --> 00:33:19,733 line:-2
让我们接着使用这些JSON文件
来创建ResearchKit图表


532
00:33:20,868 --> 00:33:22,903 line:-2
我会转到
我的图形视图控制器


533
00:33:23,904 --> 00:33:26,507 line:-2
在这里 如你所见
我有一个createGraph方法


534
00:33:26,773 --> 00:33:29,710 line:-2
它可以读取JSON文件
并从它们创建ResearchKit图表


535
00:33:30,277 --> 00:33:32,045 line:-2
让我们在viewDidLoad中
调用这些方法


536
00:33:34,381 --> 00:33:36,950 line:-1
很好 现在让我们运行它


537
00:33:38,685 --> 00:33:41,355 line:-2
如你所见 我们添加了
语音识别当前任务


538
00:33:41,855 --> 00:33:44,157 line:-1
并添加了运动障碍API


539
00:33:44,925 --> 00:33:46,727 line:-1
这就是我们的帕金森研究app


540
00:33:47,027 --> 00:33:48,295 line:-1
我们在顶部有调查问卷


541
00:33:48,629 --> 00:33:50,397 line:-1
让我们运行一个问卷吧


542
00:33:51,765 --> 00:33:55,469 line:-2
如Srinath提到的 我们拥有
一包含所有调查项目的卡片视图


543
00:33:56,036 --> 00:34:00,641 line:-2
所有这些ResearchKit中
的步骤都坚持了iOS规范


544
00:34:01,842 --> 00:34:04,411 line:-2
让我们快速完成这份问卷
这样就完成了


545
00:34:05,112 --> 00:34:07,614 line:-2
现在让我们接着看下
语音识别当前任务


546
00:34:08,382 --> 00:34:11,385 line:-2
讲述如何使用
语音识别步骤的前两个步骤


547
00:34:11,985 --> 00:34:13,754 line:-2
只要我按下
开始录制按钮


548
00:34:14,054 --> 00:34:15,589 line:-1
我会重复我看到的文字


549
00:34:17,791 --> 00:34:20,060 line:-1
敏捷的棕色狐狸跳过了懒狗


550
00:34:23,030 --> 00:34:25,799 line:-2
我跳转到了下一步
这是另一个转录步骤


551
00:34:26,099 --> 00:34:29,303 line:-2
如Srinath所说 这一步
是可选的并且可以被替换的


552
00:34:29,369 --> 00:34:32,773 line:-2
从任务中通过设置
允许编辑脚本属性为no


553
00:34:35,175 --> 00:34:38,679 line:-1
很好 让我们看看创建的这些图表


554
00:34:38,745 --> 00:34:40,714 line:-1
消除震颤和运动障碍症状数据点


555
00:34:40,981 --> 00:34:43,449 line:-2
由于ResearchKit图表
在水平模式下看起来非常棒


556
00:34:43,650 --> 00:34:46,687 line:-2
我会快速横置我的手机
让我们看看图表


557
00:34:47,454 --> 00:34:51,458 line:-2
在这里可以看到 我们有所有的震颤
和运动障碍症状数据点


558
00:34:51,525 --> 00:34:54,194 line:-2
在特定的一天
从早上7点到下午6点


559
00:34:54,995 --> 00:34:58,699 line:-2
我们可以看到轻微
轻度、中度和强烈震颤


560
00:34:58,966 --> 00:35:00,234 line:-1
也可能是运动障碍


561
00:35:01,335 --> 00:35:04,404 line:-2
很好 现在
我想把Srinath叫上台来


562
00:35:04,805 --> 00:35:05,873 line:-1
接着进行这个演讲


563
00:35:06,340 --> 00:35:07,174 line:-1
谢谢大家


564
00:35:08,976 --> 00:35:09,810 line:-1
谢谢 Akshay


565
00:35:12,946 --> 00:35:14,314 line:-1
感谢你精彩的演示


566
00:35:15,249 --> 00:35:19,319 line:-2
现在让我们回顾下
我们今天都谈了哪些内容


567
00:35:21,922 --> 00:35:24,992 line:-2
我们开始谈论了
我们向社区做出的更新


568
00:35:25,626 --> 00:35:29,096 line:-2
通过扩展特权
并更新我们的发布时间表


569
00:35:30,030 --> 00:35:33,000 line:-2
我们展示了ResearchKit
的新用户界面


570
00:35:33,567 --> 00:35:37,504 line:-2
并且我们还添加了一些新的当前任务
重点关注三个主要健康领域


571
00:35:37,838 --> 00:35:39,306 line:-1
听觉、言语和视觉


572
00:35:40,007 --> 00:35:42,910 line:-2
Gabriel和你们谈论了
新的运动障碍API


573
00:35:42,976 --> 00:35:46,380 line:-2
这在Apple Watch的
watchOS 5上可用


574
00:35:47,781 --> 00:35:52,953 line:-2
现在 我们期望
社区的现有成员和新成员


575
00:35:53,220 --> 00:35:55,923 line:-1
继续与我们互动并提供反馈


576
00:35:56,690 --> 00:36:00,360 line:-2
我们也鼓励你充分利用
我们新的发布时间表


577
00:36:00,961 --> 00:36:04,631 line:-2
这样你就可以利用
我们的一些内部功能


578
00:36:04,898 --> 00:36:09,770 line:-1
比如辅助功能、定位和质量保证


579
00:36:10,838 --> 00:36:15,843 line:-2
这样的话 当我们继续
扩展我们的进展中的任务库


580
00:36:16,109 --> 00:36:20,180 line:-2
我们期待所有的开发者
研究人员和健康专家


581
00:36:20,480 --> 00:36:22,349 line:-1
帮助我们改进这些内容


582
00:36:22,816 --> 00:36:26,987 line:-2
这些任务就像是搭建积木
我们希望你可以利用它们


583
00:36:27,221 --> 00:36:31,992 line:-2
来创造更大的研究成果
护理计划和治疗机制


584
00:36:32,292 --> 00:36:36,663 line:-2
像你做的那样 我们鼓励你
回馈ResearchKit


585
00:36:36,997 --> 00:36:39,666 line:-2
这样我们可以继续
改善我们的基础


586
00:36:40,067 --> 00:36:44,972 line:-2
并扩展进展项目的广泛性
让每个人都可以去用它


587
00:36:46,540 --> 00:36:51,678 line:-2
如果你想获知更多ResearchKit的信息
请前往我们的网站researchkit.org


588
00:36:52,479 --> 00:36:54,681 line:0
如果你要想了解
这个演讲的其他信息


589
00:36:54,748 --> 00:36:56,550 line:0
也可以访问以下的链接


590
00:36:57,184 --> 00:37:00,354 line:0
我也鼓励你前往
我们今天举办的实验室演讲


591
00:37:00,888 --> 00:37:02,322 line:0
我们团队会在那里


592
00:37:02,523 --> 00:37:05,659 line:0
会很高兴地回答
你提出的任何问题


593
00:37:05,859 --> 00:37:08,595 line:0
我们也会讨论
这次系统升级的一些资讯


594
00:37:09,496 --> 00:37:13,233 line:0
最后 我们真的很期待
在接下来的日子里看到你们


595
00:37:13,300 --> 00:37:16,036 line:0
在接下来的日子能用到这些更新


596
00:37:16,103 --> 00:37:16,937 line:-1
谢谢

