1
00:00:16,750 --> 00:00:21,588 line:0
（在Vision中进行对象跟踪
演讲716）


2
00:00:27,628 --> 00:00:31,665 line:-2
大家下午好 欢迎来到演讲
“在Vision中进行对象跟踪”


3
00:00:32,732 --> 00:00:35,802 line:-2
你是否需要面对并解决
各种计算机视觉问题


4
00:00:36,537 --> 00:00:40,140 line:-2
如果是的话
无论你是Mac还是iOS


5
00:00:40,207 --> 00:00:42,809 line:-2
或者是tvOS开发者
你都来对地方了


6
00:00:43,210 --> 00:00:44,811 line:-1
我叫Sergey Kamensky


7
00:00:44,878 --> 00:00:47,648 line:-2
我很高兴与你分享
Vision框架可以如何帮助你


8
00:00:51,585 --> 00:00:53,687 line:-1
我们今天的议程包括四个部分


9
00:00:53,754 --> 00:00:55,522 line:-2
首先 我们将讨论
为何选择Vision


10
00:00:56,190 --> 00:00:58,625 line:-1
其次 我们将讨论今年


11
00:00:58,692 --> 00:01:00,060 line:-1
我们引入了什么新变化


12
00:01:00,861 --> 00:01:02,696 line:-1
第三 我们将深入探讨


13
00:01:02,763 --> 00:01:04,665 line:-2
如何与Vision API
进行交互


14
00:01:05,498 --> 00:01:06,333 line:-1
最后


15
00:01:06,400 --> 00:01:08,335 line:-1
我们将开始这次演讲的主题


16
00:01:08,402 --> 00:01:10,404 line:-1
即在Vision中进行跟踪


17
00:01:14,007 --> 00:01:15,008 line:-1
为什么选择Vision


18
00:01:17,911 --> 00:01:21,281 line:-2
当我们设计该框架时
我们希望它成为一个中心站点


19
00:01:21,348 --> 00:01:23,450 line:-1
能够解决你所有的计算机视觉问题


20
00:01:24,117 --> 00:01:26,687 line:-1
首先是简单而一致的界面


21
00:01:27,454 --> 00:01:33,026 line:-2
支持多平台 我们的框架在iOS
macOS和tvOS上均可运行


22
00:01:34,127 --> 00:01:35,429 line:-1
我们以隐私为导向


23
00:01:35,863 --> 00:01:39,066 line:-1
这意味着你的数据永远不会离开设备


24
00:01:39,499 --> 00:01:41,068 line:-1
所有的处理都在本地进行


25
00:01:42,169 --> 00:01:43,570 line:-1
而且我们持续演进


26
00:01:44,738 --> 00:01:49,009 line:-2
我们既增强现有算法
也会开发新算法


27
00:01:51,545 --> 00:01:52,880 line:-1
来看看Vision基础


28
00:01:54,548 --> 00:01:57,084 line:-2
当你考虑如何
与Vision API交互时


29
00:01:57,150 --> 00:01:59,419 line:-1
我希望你从这些方面来思考它


30
00:02:00,287 --> 00:02:03,390 line:-2
要处理什么 如何处理
以及结果在哪里


31
00:02:04,691 --> 00:02:07,060 line:-1
要处理什么与请求族相关


32
00:02:07,761 --> 00:02:09,463 line:-2
这就是你如何告诉我们
你想做什么


33
00:02:10,364 --> 00:02:14,201 line:0
如何处理与我们的请求处理程序
或引擎相关


34
00:02:14,268 --> 00:02:18,172 line:0
我们的请求处理程序
用于处理我们的请求


35
00:02:19,106 --> 00:02:20,240 line:0
最后是结果


36
00:02:20,774 --> 00:02:23,510 line:0
Vision中的结果
以观察的形式呈现


37
00:02:24,511 --> 00:02:25,712 line:0
请看一下这张幻灯片


38
00:02:26,246 --> 00:02:28,715 line:0
如果你要记住此演讲中的任何内容


39
00:02:28,782 --> 00:02:30,984 line:0
这张幻灯片可能是
最重要的幻灯片之一


40
00:02:31,285 --> 00:02:33,020 line:0
这张幻灯片展示的是一种思想


41
00:02:33,086 --> 00:02:37,591 line:0
即如何与Vision、请求
请求处理程序和观察进行交互


42
00:02:42,296 --> 00:02:43,664 line:0
让我们先看一下请求


43
00:02:44,164 --> 00:02:46,867 line:0
这些是我们现在支持的一些请求


44
00:02:47,568 --> 00:02:49,937 line:0
如你所见 我们有各种检测器


45
00:02:50,437 --> 00:02:52,105 line:0
也有ImageRegistrationRequest


46
00:02:52,706 --> 00:02:55,843 line:0
我们有两个跟踪器
还有一个CoreMLRequest


47
00:02:56,310 --> 00:03:00,047 line:0
如果你有兴趣了解有关
集成Vision和CoreML更多信息


48
00:03:00,113 --> 00:03:02,349 line:0
我邀请你参加
这个会议室的下一个演讲


49
00:03:02,416 --> 00:03:05,919 line:0
其中我的同事Frank
将介绍这种集成的细节


50
00:03:08,488 --> 00:03:10,090 line:0
让我们来看看请求处理程序


51
00:03:11,191 --> 00:03:12,359 line:-1
在Vision中 我们有两个


52
00:03:12,426 --> 00:03:16,196 line:-2
我们有图像请求处理程序
也有序列请求处理程序


53
00:03:16,263 --> 00:03:18,832 line:-1
让我们使用这些标准来比较它们


54
00:03:20,200 --> 00:03:22,069 line:0
我们首先看一下图像请求处理程序


55
00:03:24,037 --> 00:03:28,675 line:0
图像请求处理程序用于处理
同一图像上的一个或多个请求


56
00:03:30,077 --> 00:03:32,546 line:0
它没有说的是
它会缓存某些信息


57
00:03:32,613 --> 00:03:35,649 line:0
比如图像衍生物和上一个请求的结果


58
00:03:35,949 --> 00:03:39,019 line:0
从而使管道中的其它请求
可以使用这些信息


59
00:03:39,887 --> 00:03:41,221 line:0
我来举个例子


60
00:03:41,288 --> 00:03:44,024 line:0
如果请求依赖于运行神经网络


61
00:03:44,091 --> 00:03:47,027 line:-2
如你所知
神经网络需要具有特定尺寸


62
00:03:47,094 --> 00:03:48,495 line:-1
和特定色彩方案的图像


63
00:03:48,562 --> 00:03:53,066 line:-2
假设你的神经网络预期处理
500x500的黑白图像


64
00:03:53,834 --> 00:03:57,137 line:-2
你获得的用户输入很少会
直接匹配该格式


65
00:03:57,204 --> 00:03:59,706 line:-2
所以我们在内部做的是
我们将转换图像


66
00:03:59,773 --> 00:04:03,076 line:-2
我们会将其输入神经网络
以获取当前请求的结果


67
00:04:03,143 --> 00:04:06,280 line:-2
但我们也会在请求处理程序对象上
缓存该信息


68
00:04:06,613 --> 00:04:08,215 line:-1
所以当下一个请求到来时


69
00:04:08,282 --> 00:04:10,884 line:-2
如果它需要使用相同的格式
它已经存在于那里了


70
00:04:10,951 --> 00:04:12,519 line:-1
因此不需要重新计算


71
00:04:13,253 --> 00:04:15,889 line:0
我们也会缓存从请求中获得的结果


72
00:04:15,956 --> 00:04:17,958 line:0
从而使其它请求可以在管道中使用它


73
00:04:18,024 --> 00:04:21,161 line:0
我们将稍后再讨论管道


74
00:04:22,930 --> 00:04:24,765 line:0
让我们来看看序列请求处理程序


75
00:04:25,566 --> 00:04:28,769 line:0
序列请求处理程序用于处理特定操作


76
00:04:28,836 --> 00:04:31,672 line:0
例如在一系列帧中进行跟踪


77
00:04:32,339 --> 00:04:35,576 line:0
它在内部所做的是
它缓存了该操作的状态


78
00:04:35,642 --> 00:04:38,378 line:0
从一帧到另一帧
再到另一帧的整个序列


79
00:04:39,580 --> 00:04:43,483 line:0
在Vision中 它用于处理跟踪
和图像配准请求


80
00:04:43,917 --> 00:04:46,720 line:0
所有其它请求都使用我们的
图像请求处理程序来处理


81
00:04:51,124 --> 00:04:52,259 line:-1
让我们来看看结果


82
00:04:52,693 --> 00:04:54,895 line:-2
Vision的结果
以观察的形式呈现


83
00:04:55,362 --> 00:05:01,034 line:-2
Observation是从VNObservation类
派生的一系列类


84
00:05:01,468 --> 00:05:02,903 line:-1
我们如何获得一个观察


85
00:05:03,604 --> 00:05:07,074 line:-1
首先 最自然的方式是在处理请求时


86
00:05:07,140 --> 00:05:09,443 line:-2
你可以查看
该请求的results属性


87
00:05:09,510 --> 00:05:11,912 line:-2
而该results属性
就是观察的集合


88
00:05:11,979 --> 00:05:14,481 line:-1
这就是我们告诉你处理结果的方式


89
00:05:16,483 --> 00:05:19,219 line:-1
第二种方法是手动创建它


90
00:05:19,887 --> 00:05:22,789 line:-2
我们将在演示过程中
看到如何使用这两种方法的例子


91
00:05:27,361 --> 00:05:29,730 line:-1
现在让我们来看看今年有什么新东西


92
00:05:30,397 --> 00:05:32,900 line:-1
首先 我们有了新的面部检测器


93
00:05:34,801 --> 00:05:39,306 line:-2
现在我们可以检测到更多的面孔
而且与方向无关


94
00:05:39,806 --> 00:05:40,807 line:-1
让我们看看这个例子


95
00:05:41,175 --> 00:05:43,911 line:0
在左侧 你可以看到
包含七个面孔的图像


96
00:05:44,444 --> 00:05:47,314 line:0
如果我们使用去年的检测器
处理该图像


97
00:05:47,381 --> 00:05:48,782 line:0
我们只能检测到三张面孔


98
00:05:49,216 --> 00:05:52,786 line:0
这些面孔是接近直立位置的面孔


99
00:05:53,921 --> 00:05:55,222 line:0
如果你使用今年的


100
00:05:55,289 --> 00:05:57,925 line:0
面部检测器
来处理相同的图像


101
00:05:57,991 --> 00:05:59,927 line:0
如你所见 所有面孔都可以检测到


102
00:05:59,993 --> 00:06:01,695 line:0
方向现在不再是问题


103
00:06:04,097 --> 00:06:06,300 line:0
让我们再看一下细节
谢谢


104
00:06:15,175 --> 00:06:17,978 line:-1
首先 我们新的面部检测器


105
00:06:18,312 --> 00:06:20,447 line:-1
使用与去年相同的API


106
00:06:20,781 --> 00:06:23,217 line:-1
唯一的区别是如果你想指定修正版本


107
00:06:23,283 --> 00:06:25,719 line:-2
你需要重写该请求的
revision属性


108
00:06:25,786 --> 00:06:28,055 line:-1
并将其明确设置为使用修正版本2


109
00:06:28,422 --> 00:06:30,591 line:-1
我们将在下一张幻灯片中谈及原因


110
00:06:31,558 --> 00:06:34,127 line:-1
我们还引入了两个新属性


111
00:06:34,428 --> 00:06:37,798 line:-2
一个是roll
这是当头像这样旋转时


112
00:06:37,865 --> 00:06:41,034 line:-2
另一个是yaw
即当头围绕颈部旋转时


113
00:06:45,072 --> 00:06:45,906 line:-1
修正版本


114
00:06:46,473 --> 00:06:49,576 line:-2
当我们需要引入新算法时
Vision会发生什么呢


115
00:06:49,643 --> 00:06:51,512 line:-1
我们不会立即弃用旧版本


116
00:06:52,045 --> 00:06:54,781 line:-2
相反 我们将两种修正版本
都保留一段时间


117
00:06:54,848 --> 00:06:57,684 line:-1
或者甚至可能同时发展它们


118
00:06:58,352 --> 00:07:00,454 line:-2
你通过指定请求的
revision属性


119
00:07:00,521 --> 00:07:02,689 line:-1
告诉我们你要使用哪一个


120
00:07:03,991 --> 00:07:05,392 line:-1
这就是显式行为


121
00:07:05,792 --> 00:07:07,361 line:-1
但我们也有默认行为


122
00:07:07,761 --> 00:07:11,164 line:-2
如果你创建了一个请求对象
并且你根本没有告诉我们任何东西


123
00:07:11,231 --> 00:07:14,201 line:-2
然后你开始处理该请求
以下是将要发生的事情


124
00:07:14,801 --> 00:07:18,972 line:-1
默认情况下 你将得到你的app


125
00:07:19,306 --> 00:07:20,974 line:-1
所连接的SDK的


126
00:07:21,308 --> 00:07:23,610 line:-1
最新版本


127
00:07:24,044 --> 00:07:26,380 line:-2
理解这一点很重要
我将举一个例子


128
00:07:27,014 --> 00:07:30,050 line:-1
假设你的app连接到去年的SDK


129
00:07:30,651 --> 00:07:32,753 line:-1
去年 我们只有一个检测器


130
00:07:33,253 --> 00:07:34,955 line:-1
所以这就是你将使用的检测器


131
00:07:35,455 --> 00:07:38,058 line:-2
即使你将该app运行在
当前操作系统上


132
00:07:38,125 --> 00:07:39,359 line:-1
而没有进行编译


133
00:07:40,527 --> 00:07:43,397 line:-1
另一方面 如果你重新编译app


134
00:07:43,463 --> 00:07:47,100 line:-2
使用当前的SDK
而不更改一行代码或坐标


135
00:07:47,167 --> 00:07:49,937 line:-2
并且你在当前操作系统上运行它
默认情况下你将得到


136
00:07:50,003 --> 00:07:53,040 line:-2
第二个修正版本
因为这是当前SDK中的方式


137
00:07:54,708 --> 00:07:57,277 line:-2
我们强烈建议你的app
应该具有前瞻性


138
00:07:57,344 --> 00:07:58,912 line:-1
并引用明确的修正版本


139
00:07:59,513 --> 00:08:02,416 line:-2
你将得到的
首先是确定性行为


140
00:08:02,916 --> 00:08:06,987 line:-1
你知道要引用的算法的性能


141
00:08:07,254 --> 00:08:08,355 line:-1
你知道会发生什么


142
00:08:08,755 --> 00:08:12,025 line:-1
你可以通过让你的app更具前瞻性


143
00:08:12,326 --> 00:08:13,927 line:-1
从而避免像这样的错误


144
00:08:13,994 --> 00:08:16,296 line:-1
即若干年后


145
00:08:16,363 --> 00:08:17,631 line:-1
如果我们弃用某些版本


146
00:08:18,031 --> 00:08:20,667 line:-1
那么你今天就可以引用它了


147
00:08:25,506 --> 00:08:29,109 line:-2
让我们更深入了解一下
如何与Vision API交互


148
00:08:32,078 --> 00:08:34,214 line:-2
我们首先看一个
图像请求处理程序的例子


149
00:08:35,549 --> 00:08:37,183 line:-1
你应该还记得图像请求处理程序


150
00:08:37,251 --> 00:08:40,687 line:-2
用于处理同一图像上的
一个或多个请求


151
00:08:42,523 --> 00:08:44,958 line:-2
它通过缓存一些信息
如图像衍生物和请求结果


152
00:08:45,025 --> 00:08:46,960 line:-1
来进行优化


153
00:08:47,227 --> 00:08:50,631 line:-1
以便即将处理的连续请求


154
00:08:51,064 --> 00:08:52,232 line:-1
可以使用此信息


155
00:08:53,333 --> 00:08:54,501 line:0
让我们看一下代码示例


156
00:08:56,904 --> 00:08:58,438 line:0
在我们深入研究代码示例之前


157
00:08:58,505 --> 00:08:59,907 line:0
我想强调几点


158
00:08:59,973 --> 00:09:02,509 line:0
关于此次演讲中的代码示例的问题


159
00:09:03,377 --> 00:09:06,380 line:0
其中错误处理的代码
不是处理错误的一个很好的例子


160
00:09:06,446 --> 00:09:09,016 line:0
我使用了try的简短版本
还使用了强制解包


161
00:09:09,650 --> 00:09:11,685 line:0
这只是为了简化示例


162
00:09:11,919 --> 00:09:14,154 line:0
当你编写app代码时
你应该使用guard


163
00:09:14,221 --> 00:09:16,056 line:0
以防出现预期外的行为


164
00:09:17,124 --> 00:09:22,729 line:0
我还在创建图像请求处理程序对象时
使用了imageURL


165
00:09:23,530 --> 00:09:27,401 line:0
它只是SSD中文件所在的位置


166
00:09:28,435 --> 00:09:29,570 line:0
现在我们来看看这个例子


167
00:09:30,737 --> 00:09:34,341 line:0
首先 我要创建一个
detectFacesRequest对象


168
00:09:35,042 --> 00:09:37,578 line:0
然后我将创建我的
图像请求处理程序


169
00:09:37,644 --> 00:09:40,581 line:0
并传入imageURL


170
00:09:40,647 --> 00:09:42,382 line:0
即保存面部图像数据的文件


171
00:09:43,250 --> 00:09:46,186 line:0
接下来 我要求requestHandler
处理我的请求


172
00:09:46,720 --> 00:09:48,488 line:0
最后 我将查看结果


173
00:09:49,823 --> 00:09:52,826 line:0
非常简单
如果我有一张只包含一张脸的图像


174
00:09:53,193 --> 00:09:54,962 line:0
我的结果会是这样的


175
00:09:59,967 --> 00:10:01,134 line:0
我得到的是什么呢？


176
00:10:01,201 --> 00:10:02,970 line:0
我得到了一个
FaceObservation对象


177
00:10:03,237 --> 00:10:05,372 line:0
该对象中最重要的字段之一


178
00:10:05,439 --> 00:10:07,407 line:0
是面部所在的边界框


179
00:10:09,409 --> 00:10:11,078 line:0
让我们再看看这张幻灯片


180
00:10:11,144 --> 00:10:13,714 line:-2
前三行代码几乎是你为了
找到图片中所有面孔


181
00:10:13,780 --> 00:10:16,283 line:-2
所要做的全部
那不是很酷吗？


182
00:10:19,486 --> 00:10:20,320 line:-1
谢谢


183
00:10:22,723 --> 00:10:24,925 line:-2
现在让我们看看
序列请求处理程序


184
00:10:27,895 --> 00:10:29,496 line:-2
如果你还记得
序列请求处理程序


185
00:10:29,563 --> 00:10:34,001 line:-2
用于处理特定操作
例如跟踪帧序列


186
00:10:35,402 --> 00:10:37,304 line:0
让我们看看代码示例
此代码示例


187
00:10:37,371 --> 00:10:40,507 line:0
几乎是我们用Vision API
可以实现的


188
00:10:40,574 --> 00:10:42,376 line:0
最简单的跟踪序列


189
00:10:43,810 --> 00:10:46,146 line:0
首先 我将创建我的
SequenceRequestHandler


190
00:10:47,414 --> 00:10:50,117 line:0
然后 我需要指定要跟踪的对象


191
00:10:50,184 --> 00:10:51,885 line:0
我通过创建一个


192
00:10:51,952 --> 00:10:53,887 line:0
DetectedObjectObservation
对象来做到这点


193
00:10:53,954 --> 00:10:57,491 line:0
它为我们提供了一个位置参数
一个边界框


194
00:10:58,792 --> 00:11:01,061 line:0
接下来我将开始跟踪信号


195
00:11:01,695 --> 00:11:04,164 line:0
在这个例子中
我将在连续的5个帧中


196
00:11:04,231 --> 00:11:05,632 line:0
跟踪我的对象


197
00:11:06,800 --> 00:11:08,235 line:0
让我们看看序列如何工作


198
00:11:08,735 --> 00:11:10,938 line:0
首先 我有个
frameFeeder对象


199
00:11:11,004 --> 00:11:13,674 line:0
你使用的可能是相机输入等


200
00:11:14,241 --> 00:11:15,542 line:0
这是我获取帧的地方


201
00:11:16,510 --> 00:11:19,112 line:0
我得到了帧 创建了请求对象


202
00:11:19,179 --> 00:11:23,884 line:0
并将DetectedObjectObservation
作为参数传递给其初始化程序


203
00:11:24,218 --> 00:11:26,887 line:0
这就是我在循环开始之前创建的东西


204
00:11:28,689 --> 00:11:31,959 line:0
然后 我将要求我的
requestHandler处理请求


205
00:11:33,660 --> 00:11:35,562 line:0
我现在要查看结果


206
00:11:35,629 --> 00:11:38,632 line:0
这就是我分析结果
并继续处理它们的地方


207
00:11:39,233 --> 00:11:43,470 line:0
最后一步非常重要
我在这里做的是


208
00:11:43,737 --> 00:11:45,772 line:0
我从当前迭代中获取结果


209
00:11:45,839 --> 00:11:48,041 line:0
并将其传递给下一次迭代


210
00:11:48,242 --> 00:11:50,277 line:0
因此当下一个迭代请求被创建时


211
00:11:50,344 --> 00:11:51,845 line:0
我想在其中看到这些结果


212
00:11:53,580 --> 00:11:55,916 line:0
如果我在一个包含5帧的序列中
运行它


213
00:11:55,983 --> 00:11:57,818 line:0
我的结果会是这样的


214
00:12:06,393 --> 00:12:07,861 line:0
如何创建一个请求对象


215
00:12:09,496 --> 00:12:11,798 line:-1
首先 了解请求有两种类型的属性


216
00:12:11,865 --> 00:12:13,300 line:-1
非常重要


217
00:12:13,367 --> 00:12:16,336 line:-1
即强制属性和可选属性


218
00:12:17,070 --> 00:12:19,206 line:-1
强制属性 顾名思义


219
00:12:19,473 --> 00:12:21,408 line:-1
它们需要通过初始化程序提供


220
00:12:21,608 --> 00:12:24,444 line:-1
以便能够创建请求对象


221
00:12:25,879 --> 00:12:27,014 line:-1
我们看看这个例子


222
00:12:28,448 --> 00:12:31,084 line:-2
这是我们刚才在上一张幻灯片中
看到的一些东西


223
00:12:31,518 --> 00:12:33,687 line:-1
一个detectedObjectObservation对象


224
00:12:33,754 --> 00:12:36,456 line:-2
被传入TrackObjectRequest的
初始化程序


225
00:12:36,790 --> 00:12:38,625 line:-1
这是一个强制属性的示例


226
00:12:40,627 --> 00:12:42,162 line:-1
我们也有可选属性


227
00:12:43,363 --> 00:12:45,566 line:-2
顺便说一下
两种类型的属性都被声明了


228
00:12:45,766 --> 00:12:49,369 line:-2
你可以在声明请求对象的地方
找到它们


229
00:12:51,004 --> 00:12:53,607 line:-1
可选属性是这样一组属性


230
00:12:53,674 --> 00:12:55,509 line:-1
即我们为它们预设了有意义的默认值


231
00:12:55,876 --> 00:12:57,711 line:-1
所以我们会为你初始化它们


232
00:12:57,778 --> 00:12:59,847 line:-2
但是如果有需要
你可以稍后覆盖它们


233
00:13:00,480 --> 00:13:01,582 line:-1
让我们看看这个例子


234
00:13:05,786 --> 00:13:08,789 line:0
我在这里所做的是我正在创建
DetectBarcodesRequest对象


235
00:13:09,489 --> 00:13:12,526 line:0
如果我什么都不做
只是将我的请求对象


236
00:13:12,593 --> 00:13:14,928 line:0
直接传递给请求处理程序


237
00:13:14,995 --> 00:13:18,565 line:0
我会处理整个图像来寻找条形码


238
00:13:19,032 --> 00:13:20,534 line:0
然而我在这里做的是


239
00:13:20,601 --> 00:13:24,505 line:0
我将指定一小部分
比如图像的中心裁切


240
00:13:24,872 --> 00:13:27,508 line:0
这就是我想要重点寻找条形码的地方


241
00:13:28,141 --> 00:13:30,711 line:0
并且我将覆盖
regionOfInterest属性


242
00:13:31,378 --> 00:13:34,448 line:0
如果我现在将该请求对象
传递给请求处理程序


243
00:13:34,515 --> 00:13:37,117 line:0
我将只专注于处理图像的一小部分


244
00:13:38,151 --> 00:13:40,053 line:-1
这里的regionOfInterest属性


245
00:13:40,120 --> 00:13:42,656 line:-1
就是我们的可选属性的示例


246
00:13:43,690 --> 00:13:45,459 line:-1
这里需要重点理解的是


247
00:13:45,526 --> 00:13:49,229 line:-1
一旦你手中拿到一个请求对象


248
00:13:49,296 --> 00:13:51,064 line:-1
它就已经是一个完全构造的对象


249
00:13:51,498 --> 00:13:53,500 line:-1
这是你可以立即开始使用的对象


250
00:13:53,567 --> 00:13:55,636 line:-1
我们永远不会给你不完整的对象


251
00:13:55,702 --> 00:13:58,639 line:-1
如果你稍后决定覆盖某些属性


252
00:13:58,705 --> 00:13:59,940 line:-1
欢迎你这样做


253
00:14:00,007 --> 00:14:03,210 line:-2
但是只要你获得这个对象
那你就可以开始使用它


254
00:14:07,481 --> 00:14:09,283 line:0
这张幻灯片上没有说到的一件事


255
00:14:09,349 --> 00:14:11,685 line:0
其将在下一个演讲中详细介绍


256
00:14:12,586 --> 00:14:15,422 line:0
即关于边界框


257
00:14:16,156 --> 00:14:20,294 line:0
如你所见
我们收到的坐标是经过标准化的


258
00:14:20,794 --> 00:14:24,431 line:0
它们在0到1之间
并且始终以左下角作为参照点


259
00:14:24,998 --> 00:14:27,601 line:0
下一个演讲“CoreML
与Vision集成”


260
00:14:27,668 --> 00:14:29,670 line:0
将更详细地介绍这一方面


261
00:14:33,674 --> 00:14:35,442 line:-1
让我们来看看如何理解结果


262
00:14:37,077 --> 00:14:40,280 line:-2
如前所述 Vision的结果
以观察的形式展现


263
00:14:41,715 --> 00:14:43,617 line:-1
观察由请求对象的


264
00:14:43,684 --> 00:14:45,686 line:-1
results属性填充


265
00:14:46,420 --> 00:14:47,888 line:-1
你能得到多少个观察呢


266
00:14:49,790 --> 00:14:51,859 line:-1
其集合可以从0到N


267
00:14:52,492 --> 00:14:53,861 line:-1
这里还有另一个方面


268
00:14:53,927 --> 00:14:55,996 line:-2
若得到的results
被设置为nil


269
00:14:56,063 --> 00:14:58,298 line:-1
这表示请求的泛同步失败


270
00:14:58,799 --> 00:15:01,268 line:-1
这与得到0次观察不同


271
00:15:01,902 --> 00:15:03,604 line:-1
得到0次观察意味着


272
00:15:03,670 --> 00:15:06,073 line:-2
无论你在寻找什么
它都不在那里


273
00:15:06,773 --> 00:15:09,510 line:-2
举个例子
假设你运行一个面部检测器


274
00:15:10,744 --> 00:15:12,579 line:-1
如果你提供的图片中没有面孔


275
00:15:12,646 --> 00:15:14,481 line:-1
很自然地 你会得到0次观察


276
00:15:15,315 --> 00:15:19,019 line:-2
另一方面 如果你输入的图像中
包含一个或多个面孔


277
00:15:19,453 --> 00:15:22,089 line:-1
你将得到适当数量的观察结果


278
00:15:24,258 --> 00:15:28,629 line:-2
观察的另一个重要特性是
它们是不可变的


279
00:15:28,962 --> 00:15:31,698 line:-1
我们稍后会看一下使用它的示例


280
00:15:33,233 --> 00:15:35,802 line:0
还有两个我希望你注意的属性


281
00:15:35,869 --> 00:15:38,839 line:0
它们都在所有观察的基类中声明


282
00:15:39,239 --> 00:15:40,407 line:0
一个是唯一ID


283
00:15:41,041 --> 00:15:43,810 line:0
该唯一ID标识了


284
00:15:43,877 --> 00:15:46,647 line:0
创建此特定结果的处理步骤


285
00:15:47,381 --> 00:15:49,149 line:0
另一个是置信度


286
00:15:50,284 --> 00:15:51,485 line:0
置信度告诉你


287
00:15:51,552 --> 00:15:54,388 line:0
算法对该结果有多大信心


288
00:15:55,489 --> 00:15:58,525 line:0
置信度介于0到1之间


289
00:15:59,026 --> 00:16:02,696 line:0
同样 这个主题也将在下一个演讲中
更详细地介绍


290
00:16:07,100 --> 00:16:08,902 line:-1
让我们看一下请求管道


291
00:16:09,670 --> 00:16:10,804 line:-1
什么是管道呢？


292
00:16:11,738 --> 00:16:15,375 line:-2
假设我有三个请求
而恰好请求1


293
00:16:15,442 --> 00:16:17,377 line:-1
取决于请求2的执行


294
00:16:17,444 --> 00:16:20,480 line:-1
而请求2又取决于请求3的执行


295
00:16:22,049 --> 00:16:23,650 line:-1
当处理顺序与请求顺序相反时


296
00:16:23,717 --> 00:16:26,253 line:0
我们该如何处理序列呢？


297
00:16:26,720 --> 00:16:30,324 line:0
我在这里做的是
我将首先处理请求3


298
00:16:30,390 --> 00:16:32,192 line:0
我将从该请求中得到结果


299
00:16:32,259 --> 00:16:33,861 line:0
并将其提供给请求2


300
00:16:34,294 --> 00:16:36,630 line:0
我将对请求2做同样的事情


301
00:16:36,697 --> 00:16:39,132 line:0
最后 我将处理我的请求1


302
00:16:41,568 --> 00:16:45,672 line:0
让我们看看如何以隐式和显式顺序


303
00:16:45,739 --> 00:16:47,641 line:0
运行请求管道的示例


304
00:16:48,208 --> 00:16:51,044 line:0
我们将在下一张幻灯片中
看下这两个用例


305
00:16:51,111 --> 00:16:53,747 line:-1
并且将运行我们的人脸特征点检测器


306
00:16:54,381 --> 00:16:55,883 line:-1
你可能知道


307
00:16:55,949 --> 00:16:58,452 line:-1
人脸特征点是脸部的特征


308
00:16:58,519 --> 00:17:02,022 line:-2
它是你的眼睛 眉毛
鼻子和嘴巴的位置


309
00:17:02,956 --> 00:17:05,526 line:-2
让我们首先看看如何隐式地
做到这一点


310
00:17:14,067 --> 00:17:16,737 line:-2
我有一个跟我们刚才看过的
类似的简单代码片段


311
00:17:17,104 --> 00:17:19,473 line:-1
首先 我要创建一个人脸特征点请求


312
00:17:20,507 --> 00:17:22,843 line:-1
然后我将创建图像请求处理程序


313
00:17:23,810 --> 00:17:25,412 line:-1
接下来我将处理该请求


314
00:17:26,113 --> 00:17:27,848 line:-1
最后 我将查看结果


315
00:17:28,949 --> 00:17:30,751 line:-1
如果我的图像中有一张脸


316
00:17:31,218 --> 00:17:33,020 line:-1
我的结果会是这样的


317
00:17:35,422 --> 00:17:37,491 line:-1
抱歉


318
00:17:39,226 --> 00:17:40,060 line:-1
这就是结果


319
00:17:40,727 --> 00:17:42,529 line:-1
我得到了脸部的边界框


320
00:17:42,963 --> 00:17:47,167 line:-2
这是脸部所在的位置
并且我得到了脸部的特征点


321
00:17:48,535 --> 00:17:49,970 line:-1
这里需要重点理解的是


322
00:17:50,037 --> 00:17:53,073 line:-1
当开始处理人脸特征点请求时


323
00:17:53,574 --> 00:17:57,811 line:-2
人脸特征点请求发现
尚未检测到面部


324
00:17:58,612 --> 00:18:01,548 line:-1
它将会替我们执行面部检测器


325
00:18:02,049 --> 00:18:04,351 line:-1
它从面部检测器获得结果


326
00:18:04,418 --> 00:18:06,720 line:-1
而这就是搜索人脸特征点的地方


327
00:18:09,223 --> 00:18:10,858 line:-1
在右侧 你可以看到


328
00:18:12,960 --> 00:18:15,662 line:-1
观察对象的样子


329
00:18:15,729 --> 00:18:17,631 line:-1
这些只是该对象的几个字段


330
00:18:17,698 --> 00:18:20,968 line:-2
一个是我们讨论过的uuid
它被设置为某个唯一编号


331
00:18:21,668 --> 00:18:24,471 line:-2
然后是boundingBox
即面部所在的位置


332
00:18:24,972 --> 00:18:26,940 line:-1
最后是landmarks字段


333
00:18:27,007 --> 00:18:29,843 line:-1
它指向描述特征点的某个对象


334
00:18:33,780 --> 00:18:38,352 line:-2
现在让我们看一下相同的案例
但现在使用的是显式实现


335
00:18:40,621 --> 00:18:42,189 line:-1
我首先要做的是


336
00:18:42,256 --> 00:18:44,491 line:-1
我将显式地运行我的面部检测器


337
00:18:46,460 --> 00:18:48,328 line:-1
你已经在演示文稿中


338
00:18:48,395 --> 00:18:49,730 line:-1
多次看过这四行代码


339
00:18:49,796 --> 00:18:51,798 line:-1
当我运行它时 我将得到边界框


340
00:18:52,499 --> 00:18:56,603 line:-2
如你所见 结果以相同的类型返回
即FaceObservation


341
00:18:57,204 --> 00:19:00,240 line:-2
我们在上一张幻灯片中看到的字段
可能看起来像这样


342
00:19:00,307 --> 00:19:03,544 line:-2
你有一个唯一的数字来标识
这个特定的处理步骤


343
00:19:04,144 --> 00:19:05,679 line:-1
然后是边界框的位置


344
00:19:05,746 --> 00:19:08,715 line:-1
这是处理此请求的主要结果


345
00:19:09,183 --> 00:19:11,952 line:-2
而landmarks字段被设置为nil
因为面部检测器


346
00:19:12,019 --> 00:19:13,554 line:-1
对人脸特征点一无所知


347
00:19:14,755 --> 00:19:18,058 line:-2
接下来我要做的是
创建我的人脸特征点请求


348
00:19:18,492 --> 00:19:20,961 line:-1
然后我将使用上一步的结果


349
00:19:21,028 --> 00:19:24,565 line:-2
并将其传入该请求的
inputObjectObservation属性


350
00:19:25,933 --> 00:19:28,402 line:-1
然后 我将要求请求处理程序处理它


351
00:19:29,236 --> 00:19:31,004 line:-1
最后 我将查看结果


352
00:19:31,605 --> 00:19:33,173 line:-1
如果我在同一张图片上运行它


353
00:19:33,240 --> 00:19:37,778 line:-2
我得到的结果
与上一张幻灯片完全相同


354
00:19:38,679 --> 00:19:40,480 line:-1
但让我们看看观察有何不同


355
00:19:40,881 --> 00:19:43,050 line:-1
记住 我们说过观察是不可变的


356
00:19:43,550 --> 00:19:47,621 line:-1
尽管面部检测器和人脸特征点检测器


357
00:19:47,688 --> 00:19:50,691 line:-2
返回的是相同的类型
但我们不会覆盖


358
00:19:50,757 --> 00:19:52,659 line:-1
被传入的观察


359
00:19:53,193 --> 00:19:55,963 line:-1
我们所做的是将前两个字段


360
00:19:56,029 --> 00:19:57,631 line:-1
复制到一个新对象中


361
00:19:58,265 --> 00:20:00,801 line:-2
然后计算特征点
并填充landmarks字段


362
00:20:02,202 --> 00:20:06,340 line:-2
现在如果你看一下两者的uuid
你会注意到它们是相同的


363
00:20:07,140 --> 00:20:09,977 line:-2
为什么会这样？
因为这是同一张脸


364
00:20:10,043 --> 00:20:12,446 line:-1
它具有相同的处理步骤


365
00:20:14,147 --> 00:20:16,517 line:-1
你分别应该在何时使用隐式与显式呢


366
00:20:17,251 --> 00:20:19,453 line:-1
如果你的app非常简单


367
00:20:19,520 --> 00:20:22,256 line:-1
你可能希望选择隐式方式


368
00:20:22,322 --> 00:20:24,091 line:-1
它很简单 你创建一个请求


369
00:20:24,157 --> 00:20:25,926 line:-1
其它一切都自动替你完成


370
00:20:28,595 --> 00:20:31,398 line:-1
另一方面 如果你的app比较复杂


371
00:20:31,465 --> 00:20:34,501 line:-2
例如 你想先处理面部
检测它们


372
00:20:34,902 --> 00:20:35,903 line:-1
然后做一些过滤


373
00:20:35,969 --> 00:20:38,205 line:-1
假设你不关心周边的面孔


374
00:20:38,272 --> 00:20:41,074 line:-1
或者你只想专注于中间的那些


375
00:20:41,875 --> 00:20:43,010 line:-1
你可以做那一步


376
00:20:43,076 --> 00:20:46,446 line:-2
然后你可以在剩下的一组面孔上
做特征点检测


377
00:20:47,147 --> 00:20:49,550 line:-2
在这种情况下
你可能希望使用显式版本


378
00:20:50,551 --> 00:20:51,518 line:-1
因为在这种情况下


379
00:20:51,952 --> 00:20:55,322 line:-2
特征点检测器不会重新运行
面部检测器


380
00:21:02,529 --> 00:21:04,665 line:-1
我们希望你的app


381
00:21:04,731 --> 00:21:07,534 line:-2
在内存使用和执行速度方面
具有最佳性能


382
00:21:07,601 --> 00:21:09,570 line:-1
因此接下来的两张幻灯片很重要


383
00:21:11,905 --> 00:21:13,740 line:-1
你应该将对象保存在内存中多久


384
00:21:16,944 --> 00:21:18,478 line:-1
对于图像请求处理程序


385
00:21:18,545 --> 00:21:21,481 line:-2
只要图像还需要处理
你就应该保留它


386
00:21:22,282 --> 00:21:24,952 line:-2
这听起来像是一个
非常天真和简单的陈述


387
00:21:25,219 --> 00:21:27,254 line:-1
但你做到这一点非常重要


388
00:21:28,188 --> 00:21:29,656 line:-1
如果你过早释放该对象


389
00:21:29,723 --> 00:21:32,159 line:-1
而你仍有待处理的未完成请求


390
00:21:32,226 --> 00:21:34,394 line:-1
你将必须重新创建图像请求处理程序


391
00:21:34,661 --> 00:21:36,597 line:-1
但现在你已经丢失了所有


392
00:21:36,663 --> 00:21:38,465 line:-1
与前一个对象关联的缓存


393
00:21:38,665 --> 00:21:42,035 line:-2
因此你必须花费这些开销
才能重新计算这些衍生物


394
00:21:43,837 --> 00:21:46,039 line:-1
另一方面 如果你释放得太晚了


395
00:21:46,473 --> 00:21:49,276 line:-1
首先这会导致内存碎片


396
00:21:49,743 --> 00:21:52,946 line:-1
并且你的app将无法回收内存


397
00:21:53,013 --> 00:21:54,848 line:-1
来做你想要做的其它有意义的事情


398
00:21:56,416 --> 00:21:57,751 line:-1
所以释放它很重要


399
00:21:57,818 --> 00:22:00,153 line:-2
只要你需要就使用它
否则就立即释放它


400
00:22:00,687 --> 00:22:04,224 line:-2
请记住 它会缓存图像
和多种图像衍生物


401
00:22:06,360 --> 00:22:08,595 line:-1
序列请求处理程序的情况


402
00:22:08,662 --> 00:22:10,764 line:-2
与其非常相似
唯一的区别是


403
00:22:10,831 --> 00:22:12,299 line:-1
如果你过早释放


404
00:22:12,366 --> 00:22:14,067 line:-1
你几乎作废了整个序列


405
00:22:14,134 --> 00:22:16,170 line:-1
因为整个缓存现在已经消失了


406
00:22:18,539 --> 00:22:20,807 line:-1
请求和观察又会怎么样呢？


407
00:22:21,675 --> 00:22:25,412 line:-1
请求和观察是非常轻量级的对象


408
00:22:25,479 --> 00:22:27,347 line:-1
你可以根据需要创建和释放它们


409
00:22:27,414 --> 00:22:28,382 line:-1
无需缓存它们


410
00:22:35,822 --> 00:22:37,324 line:-1
我们应该在何处处理你的请求


411
00:22:39,860 --> 00:22:43,430 line:-2
Vision中的许多请求都依赖于
在设备上运行神经网络


412
00:22:44,464 --> 00:22:49,203 line:-2
众所周知 在GPU上运行
神经网络通常比CPU更快


413
00:22:51,805 --> 00:22:53,907 line:-2
所以很自然的问题是
我们该在哪运行它？


414
00:22:55,742 --> 00:22:59,746 line:-2
我们在Vision中是这样做的
如果请求可在GPU中运行


415
00:22:59,813 --> 00:23:01,248 line:-1
我们会先尝试这样做


416
00:23:01,949 --> 00:23:05,052 line:-2
如果GPU在那个时间点
由于某种原因不可用


417
00:23:05,118 --> 00:23:06,386 line:-1
我们将切换到CPU


418
00:23:07,955 --> 00:23:09,389 line:-1
因为这是我们的默认行为


419
00:23:10,991 --> 00:23:13,927 line:0
但假设你的app需要


420
00:23:13,994 --> 00:23:16,063 line:0
在屏幕上显示大量图片


421
00:23:16,129 --> 00:23:19,433 line:0
因此你可能希望为该特定作业
保留GPU的使用


422
00:23:20,133 --> 00:23:23,003 line:0
在这种情况下 你可以将请求对象的


423
00:23:23,070 --> 00:23:25,105 line:0
usesCPUOnly属性
设置为true


424
00:23:25,439 --> 00:23:28,275 line:0
这将告诉我们直接在CPU上
处理你的请求


425
00:23:35,282 --> 00:23:38,886 line:0
我们已经介绍了如何与Vision
特别是Vision API


426
00:23:38,952 --> 00:23:42,055 line:0
交互的基础知识
我们也已经看过几个例子


427
00:23:42,623 --> 00:23:44,591 line:0
让我们切换到这次演讲的主题


428
00:23:44,658 --> 00:23:46,426 line:0
即在Vision中进行跟踪


429
00:23:48,862 --> 00:23:52,466 line:-2
什么是跟踪呢？
跟踪被定义为


430
00:23:52,533 --> 00:23:55,702 line:-1
在一系列帧中找到感兴趣的对象


431
00:23:56,069 --> 00:23:58,172 line:-1
通常 你在第一帧中找到该对象


432
00:23:58,238 --> 00:24:01,208 line:-1
并在接下来尝试在帧序列中查找它


433
00:24:02,142 --> 00:24:03,944 line:-1
此类app的例子有什么呢？


434
00:24:04,511 --> 00:24:06,213 line:-1
你可能已经看过很多了


435
00:24:06,813 --> 00:24:10,284 line:-1
比如实时评注体育赛事


436
00:24:10,350 --> 00:24:12,786 line:-2
相机的跟踪对焦
以及很多其它的app


437
00:24:15,422 --> 00:24:17,291 line:-1
你可能会说如果我能对序列中的


438
00:24:17,357 --> 00:24:19,993 line:-2
每一帧进行检测
那为什么还要使用跟踪呢？


439
00:24:20,827 --> 00:24:22,529 line:-1
原因有多种


440
00:24:23,063 --> 00:24:25,966 line:-1
首先 你可能没有一个特定的跟踪器


441
00:24:26,033 --> 00:24:28,235 line:-1
来跟踪你想跟踪的每种类型的对象


442
00:24:28,869 --> 00:24:31,305 line:-2
假设你想跟踪面孔
你很幸运


443
00:24:31,772 --> 00:24:33,440 line:-1
因为你可以使用面部检测器


444
00:24:34,041 --> 00:24:37,110 line:-1
但如果你需要跟踪特定品种的鸟


445
00:24:37,811 --> 00:24:39,646 line:-1
你可能没有那个检测器


446
00:24:39,713 --> 00:24:42,382 line:-1
现在你必须创建那个特定的检测器


447
00:24:42,449 --> 00:24:46,386 line:-1
你可能不想这样做


448
00:24:46,453 --> 00:24:48,589 line:-1
因为你的app还有别的事情要处理


449
00:24:50,390 --> 00:24:52,759 line:-2
但我们假设你很幸运
你正在跟踪面孔


450
00:24:52,993 --> 00:24:54,394 line:-1
那你应该使用检测器吗


451
00:24:55,128 --> 00:24:57,364 line:-1
也许在这种情况下也不应该


452
00:24:58,065 --> 00:24:59,333 line:-1
让我们看看这个例子


453
00:25:00,501 --> 00:25:01,802 line:-1
你开始跟踪序列


454
00:25:01,869 --> 00:25:03,737 line:-1
并在第一帧上运行面部检测器


455
00:25:04,137 --> 00:25:05,372 line:-1
你得到了五张脸


456
00:25:06,073 --> 00:25:09,042 line:-2
然后在第二帧中再运行它
你又得到了五张脸


457
00:25:09,576 --> 00:25:12,279 line:-1
你怎么知道第二帧中的面孔


458
00:25:12,346 --> 00:25:14,748 line:-1
就是第一帧中的那些面孔呢？


459
00:25:15,182 --> 00:25:17,651 line:-2
有可能一个人退出了画面
而另一个人又出现了


460
00:25:19,253 --> 00:25:22,689 line:-1
所以现在你必须匹配你找到的对象


461
00:25:23,023 --> 00:25:26,026 line:-2
这是一个你可能不想处理的
完全不同的任务


462
00:25:27,060 --> 00:25:28,462 line:-1
另一方面


463
00:25:30,364 --> 00:25:32,399 line:-1
跟踪器使用重要信息来匹配对象


464
00:25:32,466 --> 00:25:34,601 line:-2
它们知道轨迹
即物体如何移动


465
00:25:34,668 --> 00:25:37,738 line:-2
并且它们可以稍微预测
下一帧中它们的移动位置


466
00:25:39,573 --> 00:25:40,841 line:-1
但假设你非常幸运


467
00:25:41,708 --> 00:25:46,013 line:-2
你正在跟踪面孔 并且你的用例
仅限于帧中只存在一张脸的情况


468
00:25:46,079 --> 00:25:47,581 line:-1
那你应该使用检测器吗


469
00:25:48,215 --> 00:25:50,217 line:-1
即便在这种情况下也未必


470
00:25:56,490 --> 00:25:57,658 line:-1
现在速度是一个问题


471
00:25:58,058 --> 00:26:00,561 line:-1
跟踪器通常是轻量级算法


472
00:26:00,627 --> 00:26:03,630 line:-2
而探测器通常会
在设备上运行神经网络


473
00:26:03,697 --> 00:26:04,698 line:-1
这会花费更长时间


474
00:26:05,999 --> 00:26:09,436 line:-1
此外 如果你需要在图形用户界面上


475
00:26:09,503 --> 00:26:11,438 line:-2
显示跟踪信息
你可能会发现


476
00:26:11,505 --> 00:26:13,774 line:-1
跟踪器更平滑而且不那么抖动


477
00:26:17,244 --> 00:26:20,113 line:-2
还记得吗 在第一张幻灯片中
我请你记住


478
00:26:23,217 --> 00:26:25,018 line:-2
这三个词
什么 如何和结果


479
00:26:25,853 --> 00:26:29,223 line:-2
让我们看看它们如何
映射到跟踪用例中


480
00:26:30,958 --> 00:26:31,792 line:-1
首先


481
00:26:32,993 --> 00:26:33,827 line:-1
是请求


482
00:26:34,361 --> 00:26:37,731 line:-2
在Vision中
我们有两种类型的跟踪请求


483
00:26:38,165 --> 00:26:40,133 line:-1
一个是通用的对象跟踪器


484
00:26:40,200 --> 00:26:42,035 line:-1
另一个是矩形对象跟踪器


485
00:26:42,936 --> 00:26:43,770 line:-1
如何？


486
00:26:44,505 --> 00:26:45,906 line:-1
正如你现在应该猜到的那样


487
00:26:45,973 --> 00:26:48,075 line:-1
我们将使用序列请求处理程序


488
00:26:49,743 --> 00:26:50,577 line:-1
结果


489
00:26:51,578 --> 00:26:53,247 line:-1
这里有两种重要的类型


490
00:26:53,313 --> 00:26:56,950 line:-2
一个是DetectedObjectObservation
其中有一个重要属性


491
00:26:57,017 --> 00:26:59,453 line:-2
即boundingBox
它告诉你对象的位置


492
00:27:00,287 --> 00:27:02,456 line:-1
还有一个RectangleObservation


493
00:27:02,990 --> 00:27:04,958 line:-1
其有四个额外的属性


494
00:27:05,025 --> 00:27:07,127 line:-1
来表示矩形的顶点


495
00:27:07,861 --> 00:27:09,396 line:-2
现在你可能会说
既然有了边界框


496
00:27:09,463 --> 00:27:11,331 line:-1
为什么还需要矩形的顶点呢？


497
00:27:12,332 --> 00:27:14,234 line:-1
当你跟踪矩形时


498
00:27:14,735 --> 00:27:16,937 line:-1
它们是现实生活中的矩形物体


499
00:27:17,004 --> 00:27:19,640 line:-2
当它们投射到帧中时
可能看起来有所不同


500
00:27:20,073 --> 00:27:22,075 line:-1
例如 它们可能看起来像梯形


501
00:27:23,510 --> 00:27:26,747 line:-2
所以这种情况下的边界框
不是矩形本身


502
00:27:26,813 --> 00:27:28,182 line:-1
而是包含矩形所有顶点的


503
00:27:28,248 --> 00:27:30,184 line:-1
最小的框


504
00:27:33,720 --> 00:27:34,855 line:-1
现在让我们看看演示


505
00:27:36,290 --> 00:27:37,958 line:-2
（演示
在Vision中进行跟踪）


506
00:27:47,401 --> 00:27:50,037 line:-2
我这里有一个示例app
顺便说一下


507
00:27:50,103 --> 00:27:52,706 line:-1
你可以从WWDC网站下载它


508
00:27:52,773 --> 00:27:54,741 line:-1
链接就在此演讲旁边


509
00:27:55,442 --> 00:27:58,011 line:-1
该app可以处理一个视频


510
00:27:59,479 --> 00:28:01,315 line:-1
它将该视频解析为帧


511
00:28:01,982 --> 00:28:03,617 line:-1
你在第一帧中选择一个对象


512
00:28:03,684 --> 00:28:06,954 line:-2
你想跟踪多个对象
然后它会进行跟踪


513
00:28:08,555 --> 00:28:09,923 line:-1
首先 让我们使用这个视频


514
00:28:11,758 --> 00:28:12,993 line:-1
用户界面很简单


515
00:28:13,393 --> 00:28:15,529 line:-1
首先 你可以选择对象或矩形


516
00:28:15,729 --> 00:28:19,499 line:-2
其次 你可以选择要使用的算法
快速还是准确


517
00:28:19,833 --> 00:28:22,169 line:-2
Vision中会发生什么？
我们支持两种类型


518
00:28:23,103 --> 00:28:27,007 line:-2
快速和准确
这是速度与准确性之间的权衡


519
00:28:28,876 --> 00:28:32,012 line:-2
在这个例子中我将显示对象
并且使用快速算法


520
00:28:32,779 --> 00:28:33,847 line:-1
让我们选择对象


521
00:28:35,082 --> 00:28:38,685 line:-1
我要跟踪红伞下的这个人


522
00:28:38,752 --> 00:28:40,988 line:-1
我还将尝试追踪这群人


523
00:28:45,893 --> 00:28:46,827 line:-1
让我们运行它


524
00:28:54,601 --> 00:28:59,640 line:-2
如你所见
我们可以成功跟踪我们选择的对象


525
00:29:06,947 --> 00:29:09,449 line:-1
让我们看一个更复杂的例子


526
00:29:10,684 --> 00:29:13,453 line:-2
我想在这里做的是
跟踪这个水上滑板者


527
00:29:14,688 --> 00:29:16,723 line:-2
在这个例子中
我将使用准确算法


528
00:29:17,891 --> 00:29:19,226 line:-1
我先选择该对象


529
00:29:23,297 --> 00:29:24,298 line:-1
然后运行它


530
00:29:30,337 --> 00:29:34,675 line:-2
正如你所看到的
这个对象几乎完全改变了


531
00:29:34,741 --> 00:29:37,244 line:-2
它的形状 位置 颜色
所有东西


532
00:29:37,311 --> 00:29:40,013 line:-2
我们仍然可以跟踪它
我认为这很酷


533
00:29:47,821 --> 00:29:49,289 line:-1
现在我们将切换到我的演示机


534
00:29:49,356 --> 00:29:52,626 line:-2
并查看实际跟踪序列是如何
在此app中实现的


535
00:30:01,168 --> 00:30:04,705 line:-2
我启动了Xcode
并将我的iPhone连接上它


536
00:30:04,771 --> 00:30:07,941 line:-2
它上面运行了
我们刚看到的同一个app


537
00:30:09,343 --> 00:30:10,978 line:-1
我将在调试器中运行它


538
00:30:16,583 --> 00:30:17,885 line:-1
我要选择对象


539
00:30:19,219 --> 00:30:20,821 line:-1
我选择什么并不重要


540
00:30:20,888 --> 00:30:22,623 line:-1
因为我们只想查看序列


541
00:30:23,323 --> 00:30:24,358 line:-1
然后运行它


542
00:30:26,727 --> 00:30:29,329 line:-1
我在这里有一个断点


543
00:30:29,396 --> 00:30:31,965 line:-1
它在performTracking函数中中断


544
00:30:32,032 --> 00:30:34,301 line:-1
这是此app中最重要的函数


545
00:30:34,601 --> 00:30:36,837 line:-1
这是实现实际序列的函数


546
00:30:38,438 --> 00:30:39,706 line:-1
让我们看看这里做了什么


547
00:30:40,340 --> 00:30:43,544 line:-2
首先 我们创建了一个
videoReader


548
00:30:44,011 --> 00:30:46,780 line:-2
然后我们读取第一帧
并丢弃该帧


549
00:30:46,847 --> 00:30:48,949 line:-1
因为该帧用于选择对象


550
00:30:50,284 --> 00:30:51,818 line:-1
这里是取消标志


551
00:30:52,753 --> 00:30:56,390 line:-2
然后我将初始化
inputObservations集合


552
00:30:56,456 --> 00:30:58,425 line:-1
记住我们在幻灯片中看到的例子


553
00:31:01,328 --> 00:31:04,231 line:-2
接下来我设置了簿记
以便能够在图形用户界面中


554
00:31:04,298 --> 00:31:05,632 line:-1
显示结果


555
00:31:06,466 --> 00:31:09,670 line:-1
它被保存在trackedPolyRect类型中


556
00:31:11,038 --> 00:31:13,574 line:-2
然后我在type上
运行switch语句


557
00:31:13,941 --> 00:31:16,443 line:-1
type来自用户界面


558
00:31:16,510 --> 00:31:18,412 line:-2
这个例子中
我们正使用objects类型


559
00:31:20,080 --> 00:31:22,115 line:-1
我们选择了两个对象


560
00:31:23,283 --> 00:31:26,119 line:-1
这是来自用户界面的信息


561
00:31:26,453 --> 00:31:28,322 line:-1
我们应该能在这里看到这两个对象


562
00:31:31,859 --> 00:31:36,096 line:-2
好的 的确有两个
所以这个循环将运行两次


563
00:31:36,330 --> 00:31:38,532 line:-2
它将初始化
inputObservations


564
00:31:38,599 --> 00:31:40,434 line:-2
它会根据传入的
boundingBox


565
00:31:40,501 --> 00:31:42,202 line:-1
创建DetectedObjectObservation


566
00:31:43,437 --> 00:31:45,072 line:-1
正如幻灯片中显示的那样


567
00:31:46,206 --> 00:31:49,743 line:-1
并且我们初始化我们的簿记结构


568
00:31:50,043 --> 00:31:50,878 line:-1
让我们运行它


569
00:31:58,452 --> 00:32:00,087 line:-1
让我们看一下观察对象


570
00:32:08,028 --> 00:32:09,997 line:0
这里有几个重要的字段


571
00:32:10,063 --> 00:32:12,032 line:0
这是我们讨论过的唯一ID


572
00:32:12,799 --> 00:32:15,602 line:0
这是我们在标准化坐标中的边界框


573
00:32:17,971 --> 00:32:20,841 line:-2
如果我一直运行下来
我将命中这个断点


574
00:32:21,108 --> 00:32:23,911 line:-2
因为这个例子中我们没有使用它
它是用于矩形对象的


575
00:32:26,346 --> 00:32:28,916 line:-1
这是我创建序列请求处理程序的地方


576
00:32:30,884 --> 00:32:32,286 line:-1
这是我的帧计数器


577
00:32:33,854 --> 00:32:35,622 line:-1
这是如果出现问题时的标志


578
00:32:36,623 --> 00:32:38,926 line:-1
现在我终于要开始跟踪序列了


579
00:32:39,493 --> 00:32:41,328 line:-2
正如你所看到的
这是一个无限循环


580
00:32:41,728 --> 00:32:43,463 line:-1
退出该循环的条件是


581
00:32:43,530 --> 00:32:47,000 line:-1
用户请求取消或者视频已经结束


582
00:32:51,271 --> 00:32:53,574 line:-1
我要初始化


583
00:32:54,875 --> 00:32:56,009 line:-1
rects结构


584
00:32:56,076 --> 00:32:58,946 line:-1
来保存稍后要显示的


585
00:32:59,012 --> 00:33:02,850 line:-2
图形用户界面的信息
并且我将开始遍历


586
00:33:03,250 --> 00:33:05,619 line:-2
inputObservations
我们必须这样做


587
00:33:06,386 --> 00:33:08,755 line:-2
对于每一个
我都会创建一个TrackObjectRequest


588
00:33:15,095 --> 00:33:18,198 line:-2
我将该request追加到
所有请求的集合中


589
00:33:18,265 --> 00:33:19,900 line:-1
在这个例子中我们必须这样做


590
00:33:22,536 --> 00:33:26,707 line:-2
现在我跳出了循环
我终于准备开始处理请求了


591
00:33:27,574 --> 00:33:29,209 line:-1
如你看下perform函数


592
00:33:29,276 --> 00:33:32,446 line:-1
它接收一个请求集合


593
00:33:33,113 --> 00:33:36,016 line:-1
在幻灯片中 我们只将一个请求


594
00:33:36,083 --> 00:33:37,885 line:-1
传递给该集合


595
00:33:37,951 --> 00:33:40,621 line:-1
但在这里我们将同时跟踪两个请求


596
00:33:41,321 --> 00:33:42,589 line:-1
我将执行它


597
00:33:44,591 --> 00:33:49,229 line:-2
现在既然已经执行了请求
我将开始查看结果


598
00:33:49,296 --> 00:33:52,065 line:-2
为此我将查看
每个请求的results属性


599
00:33:54,968 --> 00:33:56,570 line:-1
我将获取results属性


600
00:33:56,637 --> 00:33:59,773 line:-2
并获取该属性的第一个对象
因为我们预期


601
00:33:59,840 --> 00:34:02,776 line:-1
它是其中唯一的观察


602
00:34:03,710 --> 00:34:04,711 line:-1
我在这里做的是


603
00:34:04,778 --> 00:34:08,248 line:-2
我要查看observation的
confidence属性


604
00:34:08,748 --> 00:34:11,284 line:-1
并且我将阈值随意设置为0.5


605
00:34:12,018 --> 00:34:13,453 line:-1
所以如果它高于阈值


606
00:34:13,520 --> 00:34:16,822 line:-1
我将用实线绘制边框


607
00:34:17,157 --> 00:34:19,860 line:-2
如果它低于阈值
我将用虚线绘制它


608
00:34:19,927 --> 00:34:23,330 line:-1
因此我可以看出是否出了问题


609
00:34:26,366 --> 00:34:27,634 line:-1
剩下的就是简单簿记过程


610
00:34:27,701 --> 00:34:31,004 line:-1
我只是填充该rects结构


611
00:34:32,072 --> 00:34:34,308 line:-1
这是最后一步 它非常重要


612
00:34:34,373 --> 00:34:36,710 line:-1
我从当前迭代中获取观察


613
00:34:36,777 --> 00:34:39,913 line:-1
并将其传递给下一次迭代


614
00:34:43,784 --> 00:34:45,385 line:-1
我再执行一次


615
00:34:45,985 --> 00:34:47,754 line:-1
我运行到这个断点


616
00:34:49,156 --> 00:34:50,389 line:-1
这里会显示我的帧


617
00:34:51,859 --> 00:34:54,194 line:-2
并睡眠frameRateInSeconds
这么长的时间


618
00:34:54,261 --> 00:34:55,762 line:-1
来模拟实际的视频


619
00:34:56,730 --> 00:34:58,365 line:-1
然后很快


620
00:34:58,432 --> 00:35:01,068 line:-1
你就开始了跟踪序列的第二次迭代


621
00:35:03,971 --> 00:35:05,339 line:-1
现在让我们回到幻灯片


622
00:35:11,512 --> 00:35:12,546 line:-1
谢谢


623
00:35:18,886 --> 00:35:21,788 line:-2
我们看一下刚看到的东西中
有哪些需要记住的重要事项


624
00:35:23,056 --> 00:35:26,960 line:-1
首先是如何初始化跟踪的初始对象


625
00:35:27,961 --> 00:35:28,896 line:-1
我们看到了两种方式


626
00:35:28,962 --> 00:35:30,764 line:-1
有自动方式


627
00:35:30,831 --> 00:35:33,934 line:-2
它通常通过运行某些探测器
来获取边界框


628
00:35:34,635 --> 00:35:37,571 line:-2
第二种是手动方式
它通常来自用户输入


629
00:35:53,153 --> 00:35:56,823 line:-1
我们还看到我们为每个跟踪对象


630
00:35:57,324 --> 00:35:58,525 line:-1
都使用了一个跟踪请求


631
00:35:58,792 --> 00:36:00,460 line:-1
这里的关系是一对一的


632
00:36:03,630 --> 00:36:05,499 line:-1
我们还看到两种类型的跟踪器


633
00:36:05,732 --> 00:36:07,301 line:-1
一种是通用跟踪器


634
00:36:07,568 --> 00:36:09,570 line:-1
另一种是矩形对象跟踪器


635
00:36:12,639 --> 00:36:15,909 line:-2
我们还了解到每种跟踪器类型
都有两种算法


636
00:36:16,343 --> 00:36:17,845 line:-1
即快速算法或准确算法


637
00:36:17,911 --> 00:36:21,682 line:-1
它代表了速度和准确性之间的权衡


638
00:36:23,784 --> 00:36:27,187 line:0
最后但同样重要的是
我们看到了如何使用置信度属性


639
00:36:27,921 --> 00:36:30,924 line:0
来判断我们是否应该相信我们的结果


640
00:36:34,461 --> 00:36:37,664 line:-2
在Vision中实施跟踪序列
有哪些限制呢？


641
00:36:41,201 --> 00:36:43,303 line:-1
首先我们来谈谈跟踪器的数量


642
00:36:45,372 --> 00:36:47,708 line:-1
你可以同时跟踪多少个对象


643
00:36:48,408 --> 00:36:53,814 line:-2
在Vision中我们有一个限制
即每种类型最多有16个跟踪器


644
00:36:53,881 --> 00:36:56,583 line:-1
所以你可拥有16个通用对象跟踪器


645
00:36:56,650 --> 00:36:59,453 line:-1
和16个矩形对象跟踪器


646
00:37:00,554 --> 00:37:02,956 line:-2
如果你尝试分配更多
则会收到错误消息


647
00:37:03,891 --> 00:37:07,060 line:-2
如果发生这种情况
你可能需要释放


648
00:37:07,127 --> 00:37:08,996 line:-1
你正在使用的一些跟踪器


649
00:37:09,396 --> 00:37:10,330 line:-1
这该怎么做呢？


650
00:37:12,466 --> 00:37:15,769 line:-2
第一种方法是你可以
设置请求的lastFrame属性


651
00:37:16,236 --> 00:37:19,706 line:-2
并将该请求提供给请求处理程序
以进行处理


652
00:37:20,240 --> 00:37:21,975 line:-1
这样 请求处理程序就会知道


653
00:37:22,042 --> 00:37:24,545 line:-1
与此请求对象关联的跟踪器


654
00:37:24,611 --> 00:37:25,612 line:-1
应该被释放


655
00:37:26,280 --> 00:37:29,316 line:-2
另一种方法是释放整个
SequenceRequestHandler


656
00:37:29,383 --> 00:37:32,886 line:-2
在这种情况下
与该请求处理程序关联的所有跟踪器


657
00:37:32,953 --> 00:37:33,954 line:-1
都将被释放


658
00:37:38,892 --> 00:37:41,195 line:-1
现在假设你已经实现了跟踪序列


659
00:37:41,261 --> 00:37:43,564 line:-1
你可能面临的潜在挑战是什么？


660
00:37:44,364 --> 00:37:47,301 line:-2
正如你所见
跟踪序列中的对象


661
00:37:47,367 --> 00:37:49,937 line:-1
几乎可以改变自己的一切


662
00:37:50,003 --> 00:37:52,873 line:-2
它们可以改变自己的
形状 外观 颜色 位置


663
00:37:53,340 --> 00:37:55,776 line:-1
这对算法来说是一个很大的挑战


664
00:37:56,510 --> 00:37:57,678 line:-1
那么你能做什么呢？


665
00:37:58,545 --> 00:38:01,048 line:-2
一个不幸的答案是
没有一种方法


666
00:38:01,114 --> 00:38:03,851 line:-2
适合所有的解决方案
但你可以尝试一些事情


667
00:38:04,151 --> 00:38:05,919 line:-1
首先 分别尝试快速和准确算法


668
00:38:05,986 --> 00:38:09,189 line:-1
从而弄清楚你的特定用例


669
00:38:09,256 --> 00:38:11,325 line:-1
更适合使用哪种算法


670
00:38:15,162 --> 00:38:17,264 line:-1
如果由你来手动选择边界框


671
00:38:17,798 --> 00:38:21,068 line:-1
请尝试在场景中找到一个明显的对象


672
00:38:22,870 --> 00:38:24,338 line:-1
该使用多大的置信度阈值


673
00:38:25,239 --> 00:38:26,940 line:-1
同样 这里也没有唯一的标准


674
00:38:27,174 --> 00:38:29,643 line:-1
你会发现某些用例可以使用某些阈值


675
00:38:29,710 --> 00:38:32,746 line:-1
而其它用例只能使用其它阈值


676
00:38:34,948 --> 00:38:36,984 line:0
还有一种我推荐的技术


677
00:38:37,050 --> 00:38:38,719 line:0
假设你有一个很长的跟踪序列


678
00:38:38,785 --> 00:38:40,921 line:0
举个例子 它有1000帧


679
00:38:42,155 --> 00:38:44,224 line:0
如果你开始跟踪序列


680
00:38:44,525 --> 00:38:47,828 line:0
你在第一帧中选择的对象将开始偏离


681
00:38:47,895 --> 00:38:53,567 line:0
你离开那个初始帧越远
它的改变就越大


682
00:38:54,234 --> 00:38:57,838 line:0
你可以做的是把那个序列
分成更小的子序列


683
00:38:57,905 --> 00:38:59,139 line:0
比如每个50帧


684
00:38:59,640 --> 00:39:02,943 line:0
你运行检测器
并在接下来的50帧跟踪该对象


685
00:39:03,343 --> 00:39:06,246 line:-2
你重新运行检测器
然后再次运行50帧


686
00:39:06,313 --> 00:39:07,614 line:-1
不断重复这个过程


687
00:39:08,182 --> 00:39:10,317 line:-1
从终端用户的角度来看


688
00:39:10,384 --> 00:39:12,786 line:-1
它看起来就像是在跟踪一个对象


689
00:39:13,687 --> 00:39:16,557 line:-1
然而你在内部所做的是


690
00:39:16,623 --> 00:39:18,158 line:-1
你正在跟踪较小的序列


691
00:39:18,225 --> 00:39:21,495 line:-2
这是一种更智能的
运行和跟踪序列的方式


692
00:39:27,234 --> 00:39:28,936 line:-1
让我们总结一下今天的内容


693
00:39:30,237 --> 00:39:32,439 line:-2
首先我们讨论了
为什么要使用Vision


694
00:39:32,739 --> 00:39:34,875 line:-1
我们谈到它是一个多平台框架


695
00:39:35,209 --> 00:39:38,745 line:-2
并且是面向隐私的
而且提供简单一致的界面


696
00:39:41,381 --> 00:39:43,250 line:-1
其次 我们谈到了新特性


697
00:39:43,517 --> 00:39:47,154 line:-2
我们推出了一种新的
取向无关的面部检测器


698
00:39:48,021 --> 00:39:49,556 line:-1
我们还谈到了修正版本


699
00:39:51,225 --> 00:39:54,161 line:-2
接下来我们讨论了
如何与Vision API交互


700
00:39:54,461 --> 00:39:58,565 line:-2
并讨论了请求
请求处理程序和观察


701
00:39:59,933 --> 00:40:03,804 line:0
最后 如何在Vision中
实现跟踪序列


702
00:40:05,672 --> 00:40:10,043 line:0
若想了解更多信息
我建议你参考幻灯片上的此链接


703
00:40:10,477 --> 00:40:12,846 line:0
我也建议你留下来参加下一场演讲


704
00:40:12,913 --> 00:40:14,548 line:0
下午三点钟 在这个会议室


705
00:40:14,615 --> 00:40:17,551 line:0
Frank将介绍Vision
和CoreML集成的细节


706
00:40:17,784 --> 00:40:20,554 line:-2
如果你想部署自己的模型
这一点尤为重要


707
00:40:21,154 --> 00:40:24,892 line:-2
该演讲还将介绍
有关Vision框架的一些


708
00:40:24,958 --> 00:40:26,693 line:-1
这次演讲中没有谈到的细节


709
00:40:27,561 --> 00:40:30,497 line:0
明天下午三到五点
我们还会有Vison实验室


710
00:40:31,131 --> 00:40:33,934 line:0
谢谢大家 愿你们在WWDC
剩余演讲中度过美好时光

