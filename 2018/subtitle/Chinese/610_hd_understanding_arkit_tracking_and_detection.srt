1
00:00:07,516 --> 00:00:16,500
[ 音乐 ]


2
00:00:19,516 --> 00:00:25,500
[ 掌声 ]


3
00:00:26,456 --> 00:00:27,606
>> 大家好


4
00:00:27,986 --> 00:00:29,526
很高兴今天能在这里


5
00:00:29,526 --> 00:00:31,446
与大家探讨


6
00:00:31,446 --> 00:00:34,326
ARKit 的跟踪与检测功能


7
00:00:34,326 --> 00:00:36,496
将如何创造出更好


8
00:00:36,496 --> 00:00:39,576
的 AR（增强现实）体验


9
00:00:40,596 --> 00:00:42,316
我是 Marion


10
00:00:42,346 --> 00:00:43,266
我来自 ARKit 团队


11
00:00:43,266 --> 00:00:44,756
你呢


12
00:00:46,126 --> 00:00:49,206
你是一位对 ARKit 的发展有着


13
00:00:49,206 --> 00:00:50,736
浓厚兴趣的


14
00:00:50,736 --> 00:00:51,276
资深 ARKit 开发者吗


15
00:00:51,896 --> 00:00:53,316
那么这个演讲就是为你准备的


16
00:00:53,316 --> 00:00:56,596
刚刚接触 ARKit


17
00:00:57,546 --> 00:00:59,606
那从这次演讲中 你将会学到


18
00:00:59,606 --> 00:01:01,046
不同的跟踪技巧以及


19
00:01:01,206 --> 00:01:02,876
一些 AR 常用的


20
00:01:02,966 --> 00:01:04,676
基本知识与术语


21
00:01:04,676 --> 00:01:06,756
这些基本知识与术语


22
00:01:06,756 --> 00:01:08,176
能帮助你创造属于你


23
00:01:08,176 --> 00:01:10,016
自己的第一份 AR 的体验


24
00:01:10,726 --> 00:01:14,586
让我们开始吧


25
00:01:15,106 --> 00:01:17,926
跟踪指的是什么


26
00:01:18,066 --> 00:01:19,536
跟踪将提供你的相机


27
00:01:19,836 --> 00:01:22,906
在真实世界中


28
00:01:23,156 --> 00:01:24,956
观察位置与方向


29
00:01:25,376 --> 00:01:26,806
有了这些位置与方向


30
00:01:26,806 --> 00:01:29,036
你就可以在你的相机中


31
00:01:29,036 --> 00:01:30,096
加入增强视觉的因素


32
00:01:31,146 --> 00:01:33,656
比如在这个视频中


33
00:01:33,696 --> 00:01:35,836
在真实的物理平台上的


34
00:01:35,836 --> 00:01:39,206
桌子和椅子


35
00:01:39,926 --> 00:01:41,416
是增强视觉的虚拟内容


36
00:01:42,756 --> 00:01:44,006
顺便说一下 这个是宜家


37
00:01:45,296 --> 00:01:47,186
需要注意的是 视觉内容


38
00:01:47,186 --> 00:01:48,926
从视觉上看都是正确的


39
00:01:49,746 --> 00:01:52,086
正确的放置位置 正确的大小


40
00:01:52,686 --> 00:01:54,516
以及正确的透视角度


41
00:01:55,066 --> 00:01:56,836
因此不同的跟踪


42
00:01:56,836 --> 00:01:58,906
技术能为相机


43
00:01:59,076 --> 00:02:01,586
提供不同的参考系


44
00:02:01,816 --> 00:02:03,556
这意味着相机相对于


45
00:02:03,556 --> 00:02:05,246
你的世界参考系


46
00:02:05,246 --> 00:02:07,916
相机相对于图像或者 3D 物品的参考


47
00:02:09,086 --> 00:02:11,186
在接下来这一个小时里


48
00:02:11,296 --> 00:02:12,396
我们将会讨论


49
00:02:12,396 --> 00:02:14,116
不同类型的跟踪技术


50
00:02:14,936 --> 00:02:16,476
这样子你就可以


51
00:02:16,476 --> 00:02:17,626
针对你的特定用例


52
00:02:17,626 --> 00:02:18,796
做出正确的选择


53
00:02:19,216 --> 00:02:22,296
我们将会讨论


54
00:02:22,296 --> 00:02:24,196
现在已有的 AR 技术


55
00:02:24,486 --> 00:02:26,726
这包括 方向跟踪


56
00:02:26,866 --> 00:02:28,876
世界跟踪 以及 平面检测


57
00:02:29,616 --> 00:02:31,376
之后 我们才会进一步


58
00:02:31,496 --> 00:02:33,466
分析 ARKit 2 上


59
00:02:33,466 --> 00:02:36,536
自带的新的


60
00:02:36,536 --> 00:02:38,076
跟踪和检测技术


61
00:02:38,696 --> 00:02:40,316
这些技术包括 保存和加载地图


62
00:02:40,316 --> 00:02:43,776
图像跟踪以及物品检测


63
00:02:45,266 --> 00:02:46,926
在我们对这些


64
00:02:46,926 --> 00:02:48,886
技术做进一步的探讨之前


65
00:02:48,946 --> 00:02:51,416
不妨先来对 ARKit 进行一个


66
00:02:51,416 --> 00:02:52,646
简单的概括性的回顾


67
00:02:53,176 --> 00:02:55,936
这对刚刚接触 ARKit 的人来说会


68
00:02:55,936 --> 00:02:56,276
很有趣


69
00:02:56,786 --> 00:03:00,556
首先 你需要创造


70
00:03:00,556 --> 00:03:01,826
一个 ARSession


71
00:03:02,496 --> 00:03:04,576
ARSession 是处理


72
00:03:04,576 --> 00:03:06,946
从配置到运行 AR 技术


73
00:03:06,946 --> 00:03:11,226
所有事务的对象


74
00:03:11,476 --> 00:03:15,696
并且返回 AR 技术的结果


75
00:03:16,196 --> 00:03:19,406
然后你需要做的是


76
00:03:19,496 --> 00:03:21,866
描述你想要运行的技术类型


77
00:03:22,266 --> 00:03:23,256
比如说 你想要使用


78
00:03:23,256 --> 00:03:25,136
哪种跟踪技术或者


79
00:03:25,136 --> 00:03:26,376
想要启用哪种功能


80
00:03:26,436 --> 00:03:28,096
比如说平面检测功能


81
00:03:28,966 --> 00:03:32,506
接下来 你需要使用这一


82
00:03:32,506 --> 00:03:35,406
特定的 ARConfiguration


83
00:03:36,256 --> 00:03:39,476
并运行你自己的


84
00:03:39,476 --> 00:03:40,116
ARSession 实例


85
00:03:41,576 --> 00:03:43,206
然后 ARsession 会开始内部


86
00:03:44,306 --> 00:03:47,336
配置 AVCaptureSession


87
00:03:47,336 --> 00:03:49,596
并开始接受图像


88
00:03:50,096 --> 00:03:55,096
以及启动动作管理器


89
00:03:55,096 --> 00:03:57,616
接收运动传感数据


90
00:03:57,616 --> 00:03:58,626
这就是你设备


91
00:03:58,626 --> 00:04:02,666
上内置的 ARKit 输入系统


92
00:04:04,066 --> 00:04:07,066
数据经过处理后


93
00:04:07,066 --> 00:04:09,306
会返回为每秒 60 帧


94
00:04:09,306 --> 00:04:10,926
的 ARFrames


95
00:04:12,126 --> 00:04:14,226
ARFrame 是一种及时快照


96
00:04:14,226 --> 00:04:15,396
它能为你的


97
00:04:15,396 --> 00:04:17,055
AR 景象提供


98
00:04:17,055 --> 00:04:17,875
所需要的一切


99
00:04:18,366 --> 00:04:20,776
就好像 你拍摄的图像


100
00:04:20,776 --> 00:04:25,176
会成为你的 AR 场景的背景


101
00:04:25,826 --> 00:04:27,136
跟踪相机运动


102
00:04:27,136 --> 00:04:31,256
适用于调整虚拟相机


103
00:04:31,706 --> 00:04:34,406
使之能从


104
00:04:34,446 --> 00:04:36,746
物理相机的角度


105
00:04:36,746 --> 00:04:37,586
调整虚拟物品的角度


106
00:04:38,766 --> 00:04:40,426
它还能提供


107
00:04:40,876 --> 00:04:42,086
环境相关信息


108
00:04:42,086 --> 00:04:43,536
举个例子 它可以检测到盘子


109
00:04:43,536 --> 00:04:46,806
现在 让我们


110
00:04:46,806 --> 00:04:48,436
从第一个跟踪技术开始


111
00:04:48,436 --> 00:04:49,216
说起和构建


112
00:04:51,586 --> 00:04:53,176
方向跟踪


113
00:04:54,386 --> 00:04:56,396
你们觉得方向跟踪所跟踪


114
00:04:56,536 --> 00:04:56,976
的是什么呢


115
00:04:57,176 --> 00:04:58,156
方向


116
00:04:58,676 --> 00:05:01,226
说明它只跟踪事物的旋转


117
00:05:02,136 --> 00:05:03,206
你可以这样子想象


118
00:05:03,206 --> 00:05:05,456
你只能用你的帽子来观察


119
00:05:05,456 --> 00:05:06,916
虚拟物品 而帽子本身只能


120
00:05:07,156 --> 00:05:09,046
通过旋转来观察事物


121
00:05:09,966 --> 00:05:11,716
也就是说你可以从


122
00:05:11,866 --> 00:05:13,406
固定的位置的不同角度


123
00:05:13,406 --> 00:05:15,656
来体验虚拟物体


124
00:05:15,656 --> 00:05:17,276
但任何位置上的改变都


125
00:05:17,276 --> 00:05:17,856
无法被跟踪到


126
00:05:19,596 --> 00:05:21,546
旋转数据是分三个


127
00:05:21,546 --> 00:05:22,836
轴来跟踪


128
00:05:22,946 --> 00:05:24,276
这也是为什么有的人


129
00:05:24,316 --> 00:05:26,416
称它为三个自由度跟踪


130
00:05:26,416 --> 00:05:28,926
你可以把这个技术应用到一个


131
00:05:28,926 --> 00:05:30,846
球形的虚拟环境中


132
00:05:30,846 --> 00:05:32,256
像体验一个 360 度的视频


133
00:05:32,256 --> 00:05:35,066
在这个视频中


134
00:05:35,066 --> 00:05:36,496
体验者可以从同一个位置


135
00:05:36,496 --> 00:05:38,116
对虚拟内容进行观察


136
00:05:39,346 --> 00:05:41,226
也可以用于观察一个距离很远


137
00:05:41,226 --> 00:05:43,196
的 AR 物品上


138
00:05:44,336 --> 00:05:46,296
方向跟踪并不适用


139
00:05:46,396 --> 00:05:48,236
于真实世界中的视觉增强


140
00:05:48,236 --> 00:05:49,746
因为在真实世界中


141
00:05:49,746 --> 00:05:50,886
你希望能从不同的位置


142
00:05:50,886 --> 00:05:52,666
对事物进行观察


143
00:05:54,226 --> 00:05:56,416
让我们来看一下


144
00:05:56,556 --> 00:05:58,016
当方向跟踪在运行时


145
00:05:58,016 --> 00:05:59,486
究竟发生了些什么


146
00:06:00,036 --> 00:06:02,676
这很简单


147
00:06:03,106 --> 00:06:05,016
它只是使用了核心运动


148
00:06:05,016 --> 00:06:07,166
的旋转数据


149
00:06:07,326 --> 00:06:09,146
其将传感器融合应用到


150
00:06:09,146 --> 00:06:09,986
运动传感器数据中


151
00:06:11,426 --> 00:06:13,626
与相机图像相比


152
00:06:13,756 --> 00:06:15,696
运动数据的更新频率要高出许多


153
00:06:15,696 --> 00:06:17,936
一旦相机图像可用


154
00:06:18,206 --> 00:06:20,736
方向跟踪就会


155
00:06:20,736 --> 00:06:22,616
采用这一系列数据中


156
00:06:22,616 --> 00:06:23,636
最新的运动数据


157
00:06:23,776 --> 00:06:25,766
然后将两个结果


158
00:06:26,086 --> 00:06:27,206
返回给 ARFrame


159
00:06:27,526 --> 00:06:28,066
就这样


160
00:06:28,126 --> 00:06:28,956
很简单


161
00:06:29,686 --> 00:06:31,726
请注意 在方向跟踪中


162
00:06:31,726 --> 00:06:33,266
并不会处理到


163
00:06:33,266 --> 00:06:34,526
相机回馈信息


164
00:06:34,856 --> 00:06:35,956
也就是说这个过程


165
00:06:35,956 --> 00:06:37,036
中并不会有电脑版本产生


166
00:06:38,286 --> 00:06:40,376
如果你想要运转方向跟踪功能


167
00:06:40,986 --> 00:06:43,096
你只需要在你的


168
00:06:43,286 --> 00:06:45,876
ARSession 添加


169
00:06:45,876 --> 00:06:47,256
AROrientation TrackingConfiguration


170
00:06:48,266 --> 00:06:49,686
所产生的结果将会


171
00:06:49,686 --> 00:06:52,216
通过 ARFrame 返回为


172
00:06:52,996 --> 00:06:55,116
一个 ARCamera 对象


173
00:06:55,116 --> 00:06:57,926
通常一个 ARCamera 对象


174
00:06:57,926 --> 00:06:59,826
会包括不同的变换


175
00:06:59,826 --> 00:07:01,106
在方向跟踪中


176
00:07:01,106 --> 00:07:02,936
这些变形只包括


177
00:07:02,936 --> 00:07:05,006
你的现实相机反馈的


178
00:07:05,116 --> 00:07:06,016
旋转数据


179
00:07:07,096 --> 00:07:09,616
这些旋转数据


180
00:07:09,666 --> 00:07:11,596
也可以用欧拉角来代表


181
00:07:12,276 --> 00:07:14,306
你可以根据自己的喜好来选择


182
00:07:16,866 --> 00:07:18,686
接下来让我们


183
00:07:18,686 --> 00:07:20,326
讨论一些更高级的跟踪技术


184
00:07:21,156 --> 00:07:22,546
我们先从世界跟踪开始


185
00:07:23,076 --> 00:07:25,406
世界跟踪能够跟踪你的


186
00:07:25,676 --> 00:07:28,186
相机角度方向


187
00:07:28,186 --> 00:07:30,096
还能跟踪真实环境中


188
00:07:30,316 --> 00:07:32,056
的位置变化


189
00:07:32,326 --> 00:07:34,196
而且无需事先了解


190
00:07:34,196 --> 00:07:35,226
你的周围环境


191
00:07:36,266 --> 00:07:37,656
大家可以看到


192
00:07:37,656 --> 00:07:41,316
左边是现实生活中


193
00:07:41,316 --> 00:07:42,766
相机所看到的景象


194
00:07:42,766 --> 00:07:45,416
右边是跟踪相机


195
00:07:45,546 --> 00:07:47,466
在探索过程中的


196
00:07:47,466 --> 00:07:50,506
跟踪轨迹 这轨迹是用


197
00:07:50,506 --> 00:07:51,756
坐标系来表示


198
00:07:52,986 --> 00:07:54,406
接下来 我们将详细


199
00:07:54,506 --> 00:07:55,466
的说明当世界跟踪运作时


200
00:07:55,536 --> 00:07:56,376
到底发生了什么


201
00:07:56,896 --> 00:08:00,406
世界跟踪所使用


202
00:08:00,406 --> 00:08:03,236
运动传感器


203
00:08:03,236 --> 00:08:05,436
是你设备上的加速器


204
00:08:05,436 --> 00:08:08,376
和陀螺仪的运动数据


205
00:08:08,376 --> 00:08:10,796
由此高频计算它的方向


206
00:08:11,116 --> 00:08:12,436
和旋转变化


207
00:08:14,706 --> 00:08:16,926
它同时还为 Metal


208
00:08:17,116 --> 00:08:19,276
提供了正确比例数据


209
00:08:20,656 --> 00:08:23,056
从专业方面来说


210
00:08:23,106 --> 00:08:24,306
这一部分的跟踪系统


211
00:08:24,306 --> 00:08:25,976
也被称为惯性里程计


212
00:08:27,076 --> 00:08:29,046
尽管这一运动数据


213
00:08:29,106 --> 00:08:31,026
能为短间隔动作


214
00:08:31,026 --> 00:08:32,826
提供详细的运动信息


215
00:08:32,826 --> 00:08:34,226
但当出现


216
00:08:34,275 --> 00:08:36,566
突然的运动时


217
00:08:36,645 --> 00:08:38,635
就可能会出现较长时间的漂移


218
00:08:39,015 --> 00:08:40,456
因为这些数据


219
00:08:40,616 --> 00:08:42,126
并没有理想中准确


220
00:08:42,126 --> 00:08:43,296
而且有累积误差的可能性


221
00:08:44,496 --> 00:08:45,616
这也是为什么它不能


222
00:08:45,906 --> 00:08:47,446
单独用于跟踪


223
00:08:48,036 --> 00:08:51,456
为了补偿这一漂移


224
00:08:51,696 --> 00:08:53,416
世界跟踪在使用


225
00:08:53,516 --> 00:08:55,506
相机图像时


226
00:08:55,506 --> 00:08:59,606
应用了一个电脑版本


227
00:09:00,706 --> 00:09:02,426
这个技术提供了


228
00:09:02,506 --> 00:09:05,276
更高的准确性 但却


229
00:09:05,276 --> 00:09:06,546
以计算时间为代价


230
00:09:08,056 --> 00:09:10,176
同时 它对快速相机运动


231
00:09:10,226 --> 00:09:12,596
十分敏感


232
00:09:12,666 --> 00:09:14,396
这就会导致


233
00:09:14,396 --> 00:09:15,296
图像上的模糊


234
00:09:16,516 --> 00:09:18,626
这一系统中


235
00:09:18,626 --> 00:09:20,926
的版本也被称为


236
00:09:20,926 --> 00:09:21,666
视觉里程计


237
00:09:22,016 --> 00:09:24,276
通过对计算机视觉和


238
00:09:24,276 --> 00:09:26,576
动作这两个系统的融合


239
00:09:26,576 --> 00:09:30,016
ARKit 吸收了这两个系统的优势


240
00:09:30,646 --> 00:09:32,086
ARKit 选用了计算机视觉中


241
00:09:32,086 --> 00:09:34,116
长时间间隔的


242
00:09:34,116 --> 00:09:34,946
高准确性


243
00:09:35,566 --> 00:09:37,576
而从运动数据中 它吸收了


244
00:09:37,576 --> 00:09:39,586
短时间间隔的准确性


245
00:09:39,586 --> 00:09:41,356
和测量过程中的


246
00:09:41,356 --> 00:09:43,446
高更新频率以及


247
00:09:43,446 --> 00:09:44,276
以米为测量单位


248
00:09:44,856 --> 00:09:47,456
通过合并这两个系统


249
00:09:47,456 --> 00:09:49,446
在处理某些框架时


250
00:09:49,586 --> 00:09:51,036
世界跟踪可以跳过


251
00:09:51,246 --> 00:09:52,886
计算机视觉这一步骤


252
00:09:52,886 --> 00:09:54,736
但仍保持高效和


253
00:09:54,776 --> 00:09:55,966
高反馈的跟踪


254
00:09:56,956 --> 00:09:58,856
这样子就可以减少 CPU 占用


255
00:09:58,856 --> 00:10:00,356
你可以将多出的 CPU 空间用于


256
00:10:00,356 --> 00:10:00,956
你的 App 上


257
00:10:02,876 --> 00:10:04,376
在专业方面


258
00:10:04,376 --> 00:10:06,456
这一合并技术也被称为


259
00:10:06,506 --> 00:10:07,696
视觉惯性里程计


260
00:10:08,916 --> 00:10:11,456
让我们进一步了解这一部分


261
00:10:11,456 --> 00:10:13,306
中的视觉部分


262
00:10:14,116 --> 00:10:15,926
计算机视觉正在


263
00:10:15,926 --> 00:10:19,286
处理我导出的图像


264
00:10:19,286 --> 00:10:21,096
中的一个重点区域


265
00:10:21,356 --> 00:10:23,526
像这里的蓝色和橙色的点


266
00:10:24,406 --> 00:10:26,006
它们被提取出来


267
00:10:26,006 --> 00:10:27,496
以确保在同一环境


268
00:10:27,946 --> 00:10:30,246
的其他图像中


269
00:10:30,286 --> 00:10:31,516
也能提取出同样的点


270
00:10:33,016 --> 00:10:34,096
这些重点区域


271
00:10:34,096 --> 00:10:35,166
也被称为特征


272
00:10:36,496 --> 00:10:37,616
可以看到


273
00:10:37,616 --> 00:10:39,996
根据它们本身的相似性与外观


274
00:10:40,196 --> 00:10:43,686
可以在同一相机的不同图像中


275
00:10:43,686 --> 00:10:44,326
将它们匹配起来


276
00:10:45,176 --> 00:10:46,716
接下来发生的与你眼睛


277
00:10:46,716 --> 00:10:48,566
看到 3D 的效果


278
00:10:48,566 --> 00:10:49,286
的过程类似


279
00:10:50,176 --> 00:10:51,576
这两个点之间


280
00:10:51,576 --> 00:10:54,226
有细微的倾向一侧的距离


281
00:10:55,056 --> 00:10:56,976
它们之间的视觉差


282
00:10:56,976 --> 00:10:58,796
是十分重要的


283
00:10:58,796 --> 00:11:00,636
因为它们对环境


284
00:11:00,636 --> 00:11:02,236
的观察角度有细微的差别


285
00:11:02,606 --> 00:11:04,706
这个差别能让画面更立体


286
00:11:04,816 --> 00:11:06,056
更有深度感


287
00:11:07,106 --> 00:11:08,406
这就是 ARKit 在处理


288
00:11:08,406 --> 00:11:10,176
三角测量时


289
00:11:10,176 --> 00:11:12,076
对于同一相机


290
00:11:12,076 --> 00:11:14,156
不同角度的处理方式


291
00:11:14,736 --> 00:11:16,206
只要画面中有足够的视觉差


292
00:11:16,256 --> 00:11:18,046
ARKit 就能运行这一功能


293
00:11:18,896 --> 00:11:20,786
它能够计算这些


294
00:11:20,786 --> 00:11:23,306
匹配特征之间的深度数据


295
00:11:23,706 --> 00:11:26,826
换句话说 这些图像中的 2D 特征


296
00:11:26,826 --> 00:11:29,316
通过 3D 方式获得重组


297
00:11:30,806 --> 00:11:32,066
但是 这个重组


298
00:11:32,066 --> 00:11:34,476
成功的关键


299
00:11:35,706 --> 00:11:37,536
在于相机位置的


300
00:11:37,676 --> 00:11:39,776
改变以提供


301
00:11:39,776 --> 00:11:41,316
足够的视觉差


302
00:11:42,356 --> 00:11:44,626
比如说 往某一侧倾斜的运动


303
00:11:44,966 --> 00:11:47,746
单纯的旋转并不能为重组


304
00:11:47,746 --> 00:11:48,856
提供足够的信息


305
00:11:50,536 --> 00:11:52,606
这就是有关你所身处环境的


306
00:11:52,606 --> 00:11:53,646
第一份小地图


307
00:11:53,646 --> 00:11:55,826
在 ARKit 中 我们将它称为


308
00:11:55,906 --> 00:11:56,136
世界地图


309
00:11:57,396 --> 00:11:59,826
与此同时 你的镜头的


310
00:11:59,826 --> 00:12:01,626
相机位置以及


311
00:12:01,626 --> 00:12:04,226
方向都经过了计算


312
00:12:04,226 --> 00:12:06,766
在用字母 C 这里标志出来


313
00:12:07,476 --> 00:12:08,546
这代表着你的世界跟踪


314
00:12:08,546 --> 00:12:09,396
刚刚完成初始化


315
00:12:09,396 --> 00:12:12,496
这是跟踪系统的


316
00:12:12,496 --> 00:12:12,746
初始化阶段


317
00:12:12,746 --> 00:12:15,886
当世界地图


318
00:12:15,886 --> 00:12:17,326
经过初始重组时


319
00:12:17,326 --> 00:12:19,216
也定义了


320
00:12:19,406 --> 00:12:21,246
世界原点


321
00:12:21,986 --> 00:12:23,676
它被设置为


322
00:12:23,886 --> 00:12:27,326
第一个相机的三角框架帧的原点


323
00:12:28,046 --> 00:12:30,296
同时也被设置为与重力对齐


324
00:12:31,056 --> 00:12:34,286
我们在幻灯片中用 W 来代表


325
00:12:34,906 --> 00:12:35,786
因此 你现在可以在


326
00:12:35,786 --> 00:12:37,246
它的世界坐标系


327
00:12:37,246 --> 00:12:39,366
重建作为世界地图的


328
00:12:39,366 --> 00:12:40,896
真实环境


329
00:12:40,896 --> 00:12:42,046
的小型代表


330
00:12:42,836 --> 00:12:44,446
你所使用的相机


331
00:12:44,596 --> 00:12:46,646
也被用同样的


332
00:12:46,646 --> 00:12:48,646
世界坐标来跟踪


333
00:12:50,896 --> 00:12:53,186
现在 你可以往


334
00:12:53,186 --> 00:12:56,436
你的相机视角中加入视觉


335
00:12:56,436 --> 00:12:57,056
内容来增强它们


336
00:12:58,656 --> 00:13:01,016
为了将视觉内容


337
00:13:01,066 --> 00:13:03,416
正确的加入到一个 ARSession 中


338
00:13:03,416 --> 00:13:06,256
你需要运用到 ARkit 中的


339
00:13:06,636 --> 00:13:07,826
ARAnchors 这里我们用


340
00:13:07,826 --> 00:13:08,096
A 来表示


341
00:13:09,536 --> 00:13:12,326
ARAnchors 在这个世界地图


342
00:13:12,536 --> 00:13:14,076
也就是这个世界坐标体系


343
00:13:14,076 --> 00:13:15,816
中是一个参考点


344
00:13:16,486 --> 00:13:18,386
ARAnchor 是不可或缺的


345
00:13:18,386 --> 00:13:20,686
因为世界跟踪在跟踪


346
00:13:20,686 --> 00:13:22,206
时可能会更新


347
00:13:22,206 --> 00:13:23,636
也就是说


348
00:13:23,636 --> 00:13:25,336
所有分配给它的视觉内容都


349
00:13:25,336 --> 00:13:27,776
并会被更新并正确


350
00:13:27,886 --> 00:13:32,746
的增强到相机的景象中


351
00:13:32,746 --> 00:13:34,446
既然你已经应用了


352
00:13:34,446 --> 00:13:36,436
ARAnchors 你可以将视觉内容


353
00:13:36,466 --> 00:13:38,356
添加到锚点中


354
00:13:38,386 --> 00:13:40,496
这些内容稍后会


355
00:13:40,886 --> 00:13:44,116
以正确的增强方式添加到相机的镜头中


356
00:13:45,576 --> 00:13:48,676
从现在开始 这个基于你环境的


357
00:13:48,676 --> 00:13:51,026
3D 世界地图将会成为


358
00:13:51,026 --> 00:13:53,236
世界跟踪的参考系统


359
00:13:54,046 --> 00:13:56,456
它也是新图像的参考依据


360
00:13:57,076 --> 00:13:58,796
不同图像中的特征


361
00:13:58,796 --> 00:14:01,806
互相匹配且进行三角化


362
00:14:02,666 --> 00:14:04,216
与此同时 新的稳定的


363
00:14:04,216 --> 00:14:05,776
特征被提取出来


364
00:14:06,106 --> 00:14:08,246
经过匹配 三角化


365
00:14:08,246 --> 00:14:10,496
最终能帮助你扩展


366
00:14:10,556 --> 00:14:10,786
你的世界地图


367
00:14:11,286 --> 00:14:14,036
换句话说 ARKit 正在学习你的环境


368
00:14:15,926 --> 00:14:17,006
这个学习过程会产生


369
00:14:17,006 --> 00:14:18,686
对当前相机的


370
00:14:18,946 --> 00:14:20,936
位置和方向的


371
00:14:20,936 --> 00:14:21,956
跟踪计算的更新


372
00:14:23,086 --> 00:14:24,986
最后就能向


373
00:14:24,986 --> 00:14:26,796
当前相机视角输出


374
00:14:26,796 --> 00:14:27,426
正确的增强元素


375
00:14:27,946 --> 00:14:31,966
当你持续不断的探索世界时


376
00:14:31,966 --> 00:14:33,786
世界跟踪可以


377
00:14:33,786 --> 00:14:35,706
持续的跟踪你的相机


378
00:14:35,706 --> 00:14:37,826
并持续学习你


379
00:14:38,076 --> 00:14:39,426
所处的环境


380
00:14:40,506 --> 00:14:42,866
但是随着时间的推移


381
00:14:42,866 --> 00:14:45,506
增强效果可能会出现细微的漂移


382
00:14:45,506 --> 00:14:47,366
就像你可以在左边


383
00:14:47,366 --> 00:14:49,096
的图像中看到


384
00:14:49,096 --> 00:14:51,086
对增强效果的细微补偿


385
00:14:52,366 --> 00:14:54,736
这是因为就算很细微的补偿


386
00:14:55,216 --> 00:14:58,016
或者细微的错误


387
00:14:58,016 --> 00:14:59,786
在经过累积后都


388
00:14:59,786 --> 00:15:01,496
会变得明显


389
00:15:03,486 --> 00:15:05,286
现在这个设备回到了


390
00:15:05,426 --> 00:15:07,216
一个熟悉的视角


391
00:15:07,216 --> 00:15:09,036
它之前曾经探索过这个视角


392
00:15:09,086 --> 00:15:10,956
比如说是我们


393
00:15:10,956 --> 00:15:11,756
探索的出发点


394
00:15:11,756 --> 00:15:14,116
ARKit 会进行另外


395
00:15:14,116 --> 00:15:15,846
一种优化步骤


396
00:15:16,546 --> 00:15:18,086
这一步骤会让


397
00:15:18,086 --> 00:15:19,496
视觉惯性里程计系统


398
00:15:19,496 --> 00:15:21,986
这个 ARKit 支持的系统


399
00:15:21,986 --> 00:15:24,176
转换为一个


400
00:15:24,286 --> 00:15:26,206
视觉惯性 SLAM 系统


401
00:15:27,376 --> 00:15:28,826
现在让我们回到


402
00:15:28,916 --> 00:15:30,726
世界跟踪开始探索


403
00:15:30,726 --> 00:15:32,476
的第一张图像


404
00:15:33,956 --> 00:15:35,136
世界跟踪现在要做的事情是


405
00:15:35,136 --> 00:15:37,166
检查当前的跟踪信息


406
00:15:37,166 --> 00:15:39,246
与世界地图对当前


407
00:15:39,446 --> 00:15:41,076
景象的渲染是否


408
00:15:41,076 --> 00:15:43,666
与过去的吻合


409
00:15:43,926 --> 00:15:45,276
也就是一开始的图像


410
00:15:45,276 --> 00:15:48,586
接着 ARKit 就会进行


411
00:15:48,586 --> 00:15:51,886
优化步骤


412
00:15:52,066 --> 00:15:54,316
将当前的信息以及世界地图


413
00:15:54,316 --> 00:15:56,306
与你的真实


414
00:15:56,306 --> 00:15:57,666
物理环境匹配起来


415
00:15:58,816 --> 00:16:00,256
你们是否有注意到


416
00:16:00,256 --> 00:16:01,946
在这个过程中


417
00:16:01,946 --> 00:16:02,536
ARAnchor 也被更新了


418
00:16:03,006 --> 00:16:04,476
这也是为什么你在


419
00:16:04,476 --> 00:16:07,036
往你眼前的景象中加入


420
00:16:07,036 --> 00:16:09,436
视觉内容时要使用 ARAnchor


421
00:16:09,956 --> 00:16:14,396
在这个视频中 你可以看到


422
00:16:14,396 --> 00:16:16,956
这个修正步骤在真实相机反馈


423
00:16:16,956 --> 00:16:17,716
中重复了一次


424
00:16:18,116 --> 00:16:20,736
在左边 我们可以从相机角度看


425
00:16:20,736 --> 00:16:22,596
这个环境 还能看到


426
00:16:22,596 --> 00:16:25,026
这个图像中所跟踪的特征


427
00:16:25,406 --> 00:16:26,986
在右边则是


428
00:16:26,986 --> 00:16:28,356
从俯视角度观察该景象的画面


429
00:16:28,356 --> 00:16:30,596
这代表着 ARKit


430
00:16:30,596 --> 00:16:33,166
了解这个环境并


431
00:16:34,016 --> 00:16:36,386
正在展示这个环境的 3D 重建


432
00:16:36,996 --> 00:16:39,506
这些点的颜色


433
00:16:39,506 --> 00:16:41,346
是根据重建点的高度


434
00:16:41,346 --> 00:16:43,306
进行编译


435
00:16:43,306 --> 00:16:45,036
蓝色代表地面高度


436
00:16:45,096 --> 00:16:46,746
红色代表桌子和椅子


437
00:16:47,376 --> 00:16:51,216
一旦相机回到


438
00:16:51,216 --> 00:16:52,576
它曾经拍摄过的视角


439
00:16:52,576 --> 00:16:54,546
就好像这里的开始的视角


440
00:16:54,546 --> 00:16:56,636
ARKit 就会应用


441
00:16:56,636 --> 00:16:57,896
这一优化步骤


442
00:16:57,996 --> 00:16:59,436
你需要留意的是


443
00:16:59,436 --> 00:17:01,416
点云以及相机轨迹


444
00:17:02,826 --> 00:17:04,106
不知道你是否有留意到这一更新


445
00:17:04,556 --> 00:17:05,506
让我再展示一次


446
00:17:05,996 --> 00:17:10,866
这个更新将 ARKit


447
00:17:10,866 --> 00:17:12,175
的知识与你的现实世界


448
00:17:12,286 --> 00:17:15,016
以及相机运动


449
00:17:15,016 --> 00:17:17,536
匹配起来


450
00:17:17,536 --> 00:17:19,425
为接下来的相机帧数


451
00:17:19,425 --> 00:17:20,626
提供更好的增强效果


452
00:17:21,955 --> 00:17:23,306
顺便说一下


453
00:17:23,306 --> 00:17:24,935
有关世界跟踪的所有计算


454
00:17:25,435 --> 00:17:28,886
以及有关你所处环境的信息


455
00:17:29,346 --> 00:17:31,076
这些事情都只需要在你的设备


456
00:17:31,076 --> 00:17:31,996
上就可以完成


457
00:17:32,136 --> 00:17:33,496
这些数据也只存在于


458
00:17:33,496 --> 00:17:35,096
你的设备上


459
00:17:35,096 --> 00:17:37,836
如何才能将这一复杂的


460
00:17:37,836 --> 00:17:40,566
技术应用到你的 App 中呢


461
00:17:41,926 --> 00:17:45,606
很简单


462
00:17:45,716 --> 00:17:47,486
如果你想要运行世界跟踪功能


463
00:17:47,526 --> 00:17:49,766
你只需要在你的 ARSession 中


464
00:17:49,816 --> 00:17:51,896
添加 ARWorldTrackingConfiguration 类


465
00:17:52,976 --> 00:17:55,376
它将会以 ARCamera


466
00:17:55,376 --> 00:17:57,956
的对象 ARFrame 形式返回结果


467
00:18:00,216 --> 00:18:03,636
ARCamera 的对象


468
00:18:03,636 --> 00:18:05,626
包括了转换


469
00:18:05,626 --> 00:18:06,946
在世界跟踪中


470
00:18:07,496 --> 00:18:08,806
转换还包括旋转


471
00:18:08,806 --> 00:18:12,116
以及对跟踪相机的平移


472
00:18:13,286 --> 00:18:15,106
除此之外 ARCamera 还包括


473
00:18:15,106 --> 00:18:16,886
有关跟踪状态


474
00:18:16,886 --> 00:18:18,556
以及 trackingStateReason


475
00:18:18,556 --> 00:18:19,746
的信息


476
00:18:20,166 --> 00:18:22,366
这将会为当前


477
00:18:22,366 --> 00:18:24,206
的跟踪质量提供


478
00:18:24,306 --> 00:18:25,216
一些信息


479
00:18:26,736 --> 00:18:27,976
接下来 要讨论的是跟踪质量


480
00:18:28,516 --> 00:18:29,906
你是否曾经使用过一个


481
00:18:30,016 --> 00:18:32,376
跟踪十分差劲


482
00:18:32,376 --> 00:18:34,476
或者根本不能跟踪


483
00:18:34,476 --> 00:18:35,646
的 AR App 呢


484
00:18:36,276 --> 00:18:37,346
那时你的感觉是怎么样的


485
00:18:38,446 --> 00:18:39,846
是不是很崩溃


486
00:18:39,846 --> 00:18:40,796
可能你再也不会用


487
00:18:40,796 --> 00:18:41,166
这个 App 了


488
00:18:42,026 --> 00:18:43,566
那么 怎样才在你的 App 中


489
00:18:43,566 --> 00:18:45,326
实现更好的跟踪质量呢


490
00:18:46,726 --> 00:18:48,626
要实现更好的跟踪质量


491
00:18:48,626 --> 00:18:49,846
我们需要明白什么主要因素


492
00:18:49,846 --> 00:18:51,646
会影响到跟踪质量


493
00:18:52,156 --> 00:18:53,856
在这里 我想要强调其中三点


494
00:18:55,056 --> 00:18:56,756
首先 世界跟踪依赖于


495
00:18:56,756 --> 00:18:58,996
持续不断的


496
00:18:59,056 --> 00:19:01,136
相机图像以及传感器数据


497
00:19:01,556 --> 00:19:02,876
如果这些信息中断太久


498
00:19:02,876 --> 00:19:05,456
就可能会限制跟踪


499
00:19:06,926 --> 00:19:08,676
第二 世界跟踪在


500
00:19:08,676 --> 00:19:10,326
纹理明显 光线充足的环境下


501
00:19:10,326 --> 00:19:12,306
才最好的工作


502
00:19:12,646 --> 00:19:14,656
因为世界跟踪需要用到


503
00:19:14,656 --> 00:19:16,726
这些特征点来作为参考


504
00:19:16,726 --> 00:19:18,916
并最终将其位置三角化


505
00:19:18,916 --> 00:19:20,976
所以环境需要有


506
00:19:20,976 --> 00:19:23,136
足够的视觉复杂性这一点


507
00:19:23,136 --> 00:19:23,686
是十分重要的


508
00:19:24,706 --> 00:19:26,216
如果环境达不到这一要求


509
00:19:26,466 --> 00:19:28,046
比如说光线过于昏暗


510
00:19:28,046 --> 00:19:29,666
或者你正对着一面白墙


511
00:19:29,666 --> 00:19:31,606
那跟踪的表现


512
00:19:31,606 --> 00:19:32,656
就会很糟糕


513
00:19:33,166 --> 00:19:36,936
第三点是世界跟踪的


514
00:19:36,936 --> 00:19:38,646
最佳运作环境


515
00:19:38,646 --> 00:19:39,366
是静态环境


516
00:19:40,326 --> 00:19:42,026
如果你的取景框中


517
00:19:42,106 --> 00:19:45,166
大部分事物都是移动的


518
00:19:45,166 --> 00:19:47,036
那就会导致视觉数据与


519
00:19:47,036 --> 00:19:50,526
运动数据不吻合


520
00:19:50,526 --> 00:19:51,776
最终可能会导致漂移


521
00:19:52,846 --> 00:19:54,706
同时 设备不能处于


522
00:19:54,706 --> 00:19:55,936
一个移动平台上


523
00:19:55,936 --> 00:19:57,456
比如说公交车或者电梯


524
00:19:58,326 --> 00:19:59,726
假设设备被放置在电梯之中


525
00:19:59,726 --> 00:20:01,176
那运动传感器


526
00:20:01,176 --> 00:20:02,706
会检测到


527
00:20:02,706 --> 00:20:04,476
向上或向下的运动趋势


528
00:20:04,476 --> 00:20:06,766
但视觉上 它所处的环境


529
00:20:06,816 --> 00:20:07,566
并没有变化


530
00:20:08,066 --> 00:20:11,846
那你要如何获得


531
00:20:11,846 --> 00:20:13,966
用户在使用


532
00:20:14,036 --> 00:20:16,926
你的 App 时的体验的反馈呢


533
00:20:18,236 --> 00:20:20,586
ARKit 会监控它自己的跟踪表现


534
00:20:21,186 --> 00:20:22,776
我们在 ARKit 中应用了机器学习


535
00:20:23,076 --> 00:20:24,536
ARKit 在成千上万组


536
00:20:24,536 --> 00:20:26,546
的数据学习中获得提升


537
00:20:26,546 --> 00:20:28,706
这些数据中包括在不同


538
00:20:28,706 --> 00:20:30,266
情况下跟踪的表现


539
00:20:31,776 --> 00:20:33,276
为了训练出一个能够


540
00:20:33,276 --> 00:20:35,036
告诉你跟踪表现的分类器


541
00:20:35,036 --> 00:20:36,846
我们使用了一些注释


542
00:20:36,846 --> 00:20:39,486
比如视觉内容的数量


543
00:20:39,566 --> 00:20:41,136
在图像中跟踪到的可视特征


544
00:20:41,936 --> 00:20:44,346
以及设备当时的运动速率


545
00:20:45,496 --> 00:20:47,866
在运作时 跟踪的表现


546
00:20:47,866 --> 00:20:50,636
是由这些参数


547
00:20:50,636 --> 00:20:51,746
所决定的


548
00:20:52,616 --> 00:20:55,116
在这段视频中


549
00:20:55,116 --> 00:20:57,026
我们将镜头遮住


550
00:20:57,026 --> 00:20:58,406
但保持活动和


551
00:20:58,406 --> 00:21:00,846
对环境的探索


552
00:21:00,846 --> 00:21:03,926
可以看到左下角的健康预估


553
00:21:03,926 --> 00:21:06,296
指数在下降


554
00:21:07,796 --> 00:21:09,596
而当我们把遮住镜头


555
00:21:09,596 --> 00:21:11,336
的东西移走时


556
00:21:11,336 --> 00:21:12,496
这一数据就回归正常


557
00:21:14,036 --> 00:21:16,066
ARKit 通过给用户


558
00:21:16,066 --> 00:21:18,586
提供一个跟踪状态将


559
00:21:18,586 --> 00:21:19,596
数据进行简化


560
00:21:20,466 --> 00:21:22,186
跟踪状态由三个


561
00:21:22,316 --> 00:21:23,426
不同的值表示


562
00:21:23,426 --> 00:21:26,776
正常也就是


563
00:21:26,856 --> 00:21:29,676
健康状态 大部分情况下


564
00:21:29,676 --> 00:21:30,546
都是这种状态


565
00:21:30,546 --> 00:21:31,956
没错 在大部分情况下都会是这种状态


566
00:21:32,506 --> 00:21:34,096
另一种状态是限制状态


567
00:21:34,236 --> 00:21:35,396
这种状态会出现在跟踪


568
00:21:35,396 --> 00:21:36,426
表现糟糕时


569
00:21:37,486 --> 00:21:40,116
如果出现了这种状态


570
00:21:40,116 --> 00:21:42,056
那 ARKit 还会告诉你限制


571
00:21:42,056 --> 00:21:43,786
的原因 比如说没有足够的


572
00:21:43,946 --> 00:21:45,406
特征或者


573
00:21:45,406 --> 00:21:47,766
移动太快或者


574
00:21:47,806 --> 00:21:49,966
当前正处于初始化阶段


575
00:21:50,716 --> 00:21:53,496
还有另一种状态便是不可用状态


576
00:21:53,496 --> 00:21:55,256
这意味着当前还未


577
00:21:55,256 --> 00:21:56,086
完成初始化


578
00:21:57,116 --> 00:21:58,826
这样子 不管跟踪状态何时改变


579
00:21:58,916 --> 00:22:01,146
你都能够知道


580
00:22:01,416 --> 00:22:03,636
相机在跟踪过程中确实


581
00:22:03,636 --> 00:22:04,076
会发生变化


582
00:22:05,216 --> 00:22:06,116
这个功能让你能够


583
00:22:06,116 --> 00:22:08,666
在限制状态出现时


584
00:22:08,896 --> 00:22:10,346
及时地通知


585
00:22:10,346 --> 00:22:11,096
你的用户


586
00:22:12,116 --> 00:22:13,226
你还需要为你的


587
00:22:13,426 --> 00:22:15,426
用户提供有关


588
00:22:15,776 --> 00:22:17,936
改善他们跟踪情况


589
00:22:17,936 --> 00:22:19,626
的高实用性且可操作的建议


590
00:22:20,136 --> 00:22:22,566
因为改进跟踪环境的主动权


591
00:22:22,566 --> 00:22:23,396
掌握在用户手上


592
00:22:23,776 --> 00:22:25,706
这些建议可以是


593
00:22:25,706 --> 00:22:27,996
我们之前讨论过的 往某侧移动以


594
00:22:28,096 --> 00:22:30,656
运行设备进行初始化


595
00:22:31,206 --> 00:22:32,236
或者确保有足够的


596
00:22:32,236 --> 00:22:34,316
光线来保证


597
00:22:34,416 --> 00:22:35,646
足够的复杂性


598
00:22:36,236 --> 00:22:39,646
接下来 我将会总结一下


599
00:22:39,646 --> 00:22:40,446
世界跟踪


600
00:22:42,136 --> 00:22:46,156
世界跟踪为你的相机


601
00:22:46,156 --> 00:22:48,046
提供方向和位置的 6


602
00:22:48,276 --> 00:22:51,556
自由度跟踪


603
00:22:51,556 --> 00:22:53,356
这个跟踪基于你所处的环境


604
00:22:53,356 --> 00:22:55,286
但它本身对你所处


605
00:22:55,286 --> 00:22:56,916
对环境并没有


606
00:22:56,916 --> 00:22:59,066
事先的了解


607
00:22:59,606 --> 00:23:01,866
这个跟踪让在真实


608
00:23:01,866 --> 00:23:02,836
世界中的增强内容


609
00:23:02,836 --> 00:23:05,166
可以从任何角度进行察看


610
00:23:06,636 --> 00:23:08,676
世界跟踪也创造了一个


611
00:23:08,676 --> 00:23:11,196
世界地图


612
00:23:11,196 --> 00:23:13,206
这个世界地图成为


613
00:23:13,206 --> 00:23:15,416
新的图像定位的参考系统


614
00:23:17,266 --> 00:23:18,386
为了提供更好的用户体验


615
00:23:18,386 --> 00:23:20,466
跟踪质量需要被监控


616
00:23:20,466 --> 00:23:22,696
并能为用户提供


617
00:23:22,786 --> 00:23:25,196
反馈或者指引


618
00:23:25,736 --> 00:23:28,676
世界跟踪只在你的


619
00:23:28,676 --> 00:23:29,606
设备上运行


620
00:23:30,056 --> 00:23:31,516
所有的结果都只保存


621
00:23:31,516 --> 00:23:31,926
于你的设备中


622
00:23:33,446 --> 00:23:34,716
如果你还没有尝试过这个


623
00:23:35,576 --> 00:23:37,206
不妨在我们的开发者


624
00:23:37,206 --> 00:23:37,836
模板中试一下


625
00:23:37,936 --> 00:23:39,116
比如说你可以创造你的


626
00:23:39,116 --> 00:23:41,546
AR 初体验


627
00:23:41,546 --> 00:23:43,726
你可以做出一些探索


628
00:23:43,726 --> 00:23:44,866
花上 15 分钟时间了解不同情况


629
00:23:44,866 --> 00:23:46,536
下的跟踪质量 比如不同灯光效果


630
00:23:46,616 --> 00:23:48,036
或者运动频率


631
00:23:48,536 --> 00:23:50,476
一直记住 在用户遇到


632
00:23:50,476 --> 00:23:53,596
限制跟踪状态时


633
00:23:53,826 --> 00:23:56,696
要及时给他提供指导


634
00:23:56,696 --> 00:23:58,646
以保证他能有一个


635
00:23:58,886 --> 00:24:00,036
好的跟踪体验


636
00:24:01,456 --> 00:24:04,586
世界跟踪与相机相关


637
00:24:04,586 --> 00:24:06,566
它与你的相机位置


638
00:24:06,566 --> 00:24:09,186
以及你所处的环境相关


639
00:24:10,156 --> 00:24:13,026
接下来 我们要讨论


640
00:24:13,026 --> 00:24:14,536
视觉内容是如何


641
00:24:14,836 --> 00:24:16,776
与现实环境互动的


642
00:24:17,136 --> 00:24:19,156
这一效果时通过平面检测


643
00:24:19,156 --> 00:24:19,726
来实现


644
00:24:22,916 --> 00:24:24,876
接下来这段视频


645
00:24:24,876 --> 00:24:26,576
没错这段视频也来自于 Ikea App


646
00:24:26,576 --> 00:24:28,146
是一个很棒的平面检测例子


647
00:24:28,466 --> 00:24:30,626
将虚拟物品放置到


648
00:24:30,626 --> 00:24:32,566
你的真实环境中


649
00:24:32,566 --> 00:24:34,426
并与它互动


650
00:24:35,296 --> 00:24:37,786
首先 我们要留意的是


651
00:24:37,786 --> 00:24:39,256
在 Ikea App 中 设计者是如何


652
00:24:39,256 --> 00:24:41,156
指引用户进行运动的


653
00:24:42,296 --> 00:24:44,326
接着 一旦检测到水平面后


654
00:24:44,376 --> 00:24:46,796
一个虚拟桌子出现


655
00:24:46,796 --> 00:24:49,916
并等待着你去放置


656
00:24:51,256 --> 00:24:52,856
你为它选好位置 并按照你的


657
00:24:52,856 --> 00:24:54,646
想法旋转后


658
00:24:54,646 --> 00:24:55,886
你可以将它固定到环境中


659
00:24:56,126 --> 00:24:56,976
你是否有留意到


660
00:24:56,976 --> 00:24:59,206
当你将这个桌子固定住时


661
00:24:59,256 --> 00:25:02,196
平面与桌子之间的互动


662
00:25:02,196 --> 00:25:04,916
就是地面有轻微的抖动


663
00:25:05,276 --> 00:25:06,986
当我们知道地面在哪时


664
00:25:06,986 --> 00:25:08,416
我们就可以实现这个效果


665
00:25:09,556 --> 00:25:10,996
让我们进一步的讨论


666
00:25:10,996 --> 00:25:12,696
这个过程中究竟发生了什么


667
00:25:14,016 --> 00:25:16,086
平面检测所运用的


668
00:25:16,166 --> 00:25:18,726
是我刚刚所提到的


669
00:25:18,726 --> 00:25:20,656
世界地图


670
00:25:20,656 --> 00:25:22,726
在这里我们用


671
00:25:22,726 --> 00:25:24,716
黄色的点来代表它


672
00:25:25,176 --> 00:25:28,446
平面检测利用这些点


673
00:25:28,446 --> 00:25:30,966
来检测水平或者垂直的平面


674
00:25:30,966 --> 00:25:32,936
比如说地面


675
00:25:32,936 --> 00:25:34,586
长椅以及小墙面


676
00:25:35,386 --> 00:25:36,946
它通过积累多个


677
00:25:36,946 --> 00:25:38,746
ARFrame 的信息


678
00:25:38,746 --> 00:25:39,266
来实现这个效果


679
00:25:40,096 --> 00:25:42,496
随着用户对场景的


680
00:25:42,596 --> 00:25:44,316
进一步探索 它能获得


681
00:25:44,316 --> 00:25:46,026
更多有关真实平面的信息


682
00:25:46,896 --> 00:25:48,086
这些信息让平面检测


683
00:25:48,086 --> 00:25:52,056
可以提供和扩张平面


684
00:25:52,056 --> 00:25:52,916
就像一个凸壳


685
00:25:53,386 --> 00:25:58,006
如果在同一物理表面上


686
00:25:58,006 --> 00:26:00,676
检测到多于一个的平面


687
00:26:00,676 --> 00:26:02,876
就像我们现在看的这一部分


688
00:26:02,876 --> 00:26:04,636
绿色和紫色的平面


689
00:26:05,226 --> 00:26:06,726
一旦他们重叠到一起


690
00:26:06,726 --> 00:26:07,926
就会被合并


691
00:26:08,596 --> 00:26:11,356
如果水平和垂直的平面


692
00:26:11,356 --> 00:26:13,026
出现相交的情况


693
00:26:13,026 --> 00:26:15,666
它们相交的地方会被剪掉


694
00:26:15,666 --> 00:26:18,156
这也是 ARKit 2 中的新功能


695
00:26:19,876 --> 00:26:21,776
平面检测被设定为


696
00:26:21,776 --> 00:26:24,686
有很少的误差


697
00:26:24,766 --> 00:26:27,166
因为它重新使用了世界跟踪


698
00:26:27,166 --> 00:26:28,296
中的 3D 点


699
00:26:29,086 --> 00:26:31,166
然后它将平面放置到


700
00:26:31,166 --> 00:26:33,386
这些点云中


701
00:26:33,696 --> 00:26:36,346
并不断的集合越来越多的点


702
00:26:36,346 --> 00:26:38,326
将已重叠的平面


703
00:26:38,606 --> 00:26:40,286
合并起来


704
00:26:40,936 --> 00:26:42,516
所以 检测第一个平面


705
00:26:42,516 --> 00:26:44,806
需要耗费一定的时间


706
00:26:46,096 --> 00:26:47,276
这对你有什么影响呢


707
00:26:48,856 --> 00:26:50,726
当你的 App 刚启动时


708
00:26:50,726 --> 00:26:53,036
可能并不会立刻有可以


709
00:26:53,036 --> 00:26:55,846
放置物品或与物品互动的平面


710
00:26:57,266 --> 00:26:58,786
如果你的体验一定需要


711
00:26:58,876 --> 00:27:01,496
检测到平面的话


712
00:27:01,786 --> 00:27:04,006
你需要指引你的用户


713
00:27:04,426 --> 00:27:06,006
移动相机以获得足够


714
00:27:06,106 --> 00:27:08,956
的平移来提供


715
00:27:08,956 --> 00:27:11,616
足够的视觉差以保证密集的重建


716
00:27:11,616 --> 00:27:13,396
以及在场景中


717
00:27:13,396 --> 00:27:15,086
有足够的视觉复杂性


718
00:27:15,976 --> 00:27:18,096
对了 单有旋转数据是


719
00:27:18,096 --> 00:27:20,416
不足以重建的


720
00:27:20,946 --> 00:27:23,856
怎么样才能启用


721
00:27:23,856 --> 00:27:24,796
平面检测呢


722
00:27:25,786 --> 00:27:26,966
这个也很简单


723
00:27:27,116 --> 00:27:28,676
因为平面检测使用的是


724
00:27:28,676 --> 00:27:30,286
世界跟踪的 3D 地图


725
00:27:30,286 --> 00:27:32,486
它可以通过使用


726
00:27:32,646 --> 00:27:33,316
ARWorldTrackingConfiguration


727
00:27:33,316 --> 00:27:35,126
这一参数进行配置


728
00:27:35,646 --> 00:27:38,346
planeDetection 的特征


729
00:27:38,346 --> 00:27:40,706
可以设置为


730
00:27:40,706 --> 00:27:41,986
水平 垂直


731
00:27:41,986 --> 00:27:43,626
或者像这种情况下


732
00:27:43,706 --> 00:27:43,926
两者皆可


733
00:27:45,116 --> 00:27:47,446
接下来你便可以


734
00:27:47,446 --> 00:27:49,046
用这个配置


735
00:27:49,106 --> 00:27:50,226
来运行你的 ARSession


736
00:27:50,476 --> 00:27:52,146
ARKit 就开始对


737
00:27:52,146 --> 00:27:52,856
平面的检测了


738
00:27:53,526 --> 00:27:56,086
那这些检测到的平面结果


739
00:27:56,086 --> 00:27:58,576
是如何返回给你的呢


740
00:28:01,496 --> 00:28:03,216
检测的平面会以


741
00:28:03,216 --> 00:28:05,006
ARPlaneAnchor 的形式返回给你


742
00:28:05,936 --> 00:28:07,996
ARPlaneAnchor 是 ARAnchor


743
00:28:07,996 --> 00:28:08,946
的一个子类


744
00:28:10,106 --> 00:28:14,476
每个 ARAnchor 都会提供一个变换


745
00:28:14,476 --> 00:28:16,326
这个变换包含锚点在你


746
00:28:16,606 --> 00:28:17,566
的世界地图中的位置信息


747
00:28:18,126 --> 00:28:20,216
一个平面锚点


748
00:28:20,216 --> 00:28:25,886
也有着有关平面表面的几何信息


749
00:28:26,816 --> 00:28:27,856
这些信息有两种


750
00:28:27,856 --> 00:28:28,976
可选的代表方式


751
00:28:29,316 --> 00:28:31,256
一种是像一个有着中心点


752
00:28:31,256 --> 00:28:35,096
和拓展的密封箱子


753
00:28:35,096 --> 00:28:37,306
另一种则是 3D 网格


754
00:28:37,356 --> 00:28:39,336
这些网格描述着


755
00:28:39,386 --> 00:28:41,306
所检测到的平面的凸壳以及其几何特征


756
00:28:42,636 --> 00:28:44,646
当平面出现增加 更新


757
00:28:44,736 --> 00:28:48,266
或者移除的情况时


758
00:28:48,266 --> 00:28:51,266
需要有提示


759
00:28:51,266 --> 00:28:53,676
来通知我们


760
00:28:54,656 --> 00:28:57,086
这能让你及时


761
00:28:57,086 --> 00:28:59,566
利用这些平面并对更新


762
00:28:59,686 --> 00:29:01,486
做出反应


763
00:29:01,666 --> 00:29:03,366
你可以对平面做些什么呢


764
00:29:04,866 --> 00:29:05,956
我们刚刚在 Ikea App 中


765
00:29:05,956 --> 00:29:07,796
看到的就是很好的例子


766
00:29:08,026 --> 00:29:09,496
比如说你可以通过冲击测试


767
00:29:09,496 --> 00:29:10,916
来放置虚拟物品


768
00:29:12,046 --> 00:29:13,606
你也可以与一些


769
00:29:13,606 --> 00:29:15,186
虚拟物品进行真实互动


770
00:29:15,286 --> 00:29:17,846
就好像我们刚刚也看到了


771
00:29:17,846 --> 00:29:18,706
抖动是可以实现的


772
00:29:19,816 --> 00:29:21,966
你也可以在检测到的平面


773
00:29:21,966 --> 00:29:23,226
上添加一个遮挡平面


774
00:29:23,226 --> 00:29:25,346
所有虚拟物品


775
00:29:25,496 --> 00:29:27,286
就会被隐藏到


776
00:29:27,336 --> 00:29:30,916
这个遮挡屏幕之下或者之后


777
00:29:32,616 --> 00:29:34,436
所以 让我来总结一下我们现在


778
00:29:34,436 --> 00:29:35,386
所学习了的东西


779
00:29:36,536 --> 00:29:37,756
我们了解了方向跟踪


780
00:29:37,816 --> 00:29:41,176
世界跟踪


781
00:29:41,286 --> 00:29:44,386
以及平面检测


782
00:29:45,306 --> 00:29:47,666
接下里 Michele 将会


783
00:29:47,666 --> 00:29:48,986
进一步的介绍我们的新的


784
00:29:48,986 --> 00:29:50,316
跟踪技术 这些技术


785
00:29:50,386 --> 00:29:52,976
将会应用在 ARKit 2 中


786
00:29:53,046 --> 00:29:54,246
让我们欢迎 Michele


787
00:29:55,176 --> 00:29:57,500
[ 掌声 ]


788
00:29:58,396 --> 00:29:59,186
>> 谢谢 Marion


789
00:30:00,506 --> 00:30:01,766
大家好 我是 Michele


790
00:30:01,766 --> 00:30:02,766
很高兴能负责为大家


791
00:30:02,766 --> 00:30:03,636
讲解此次演示


792
00:30:03,636 --> 00:30:03,886
余下的话题


793
00:30:05,136 --> 00:30:07,886
接下来要讨论的是保存和


794
00:30:07,886 --> 00:30:08,326
加载地图


795
00:30:08,936 --> 00:30:10,486
这个功能可以


796
00:30:10,486 --> 00:30:11,816
将一个阶段中所需要的


797
00:30:11,816 --> 00:30:12,906
所有数据存储起来


798
00:30:13,336 --> 00:30:14,256
这样子 在稍后的另一个阶段中


799
00:30:14,256 --> 00:30:16,216
这些数据就可以


800
00:30:16,216 --> 00:30:18,116
被重新运用并用于


801
00:30:18,336 --> 00:30:19,706
创造与某个特定地区


802
00:30:19,706 --> 00:30:21,486
体验一致的 AR 体验


803
00:30:22,406 --> 00:30:23,566
或者它也可以存储在


804
00:30:23,566 --> 00:30:25,416
另一个设备上来创造


805
00:30:25,606 --> 00:30:27,906
多用户的 AR 体验


806
00:30:28,646 --> 00:30:30,000
举个例子


807
00:30:37,076 --> 00:30:38,876
这里有个男人


808
00:30:38,876 --> 00:30:40,906
让我们叫他 Andre


809
00:30:40,906 --> 00:30:41,956
他手里拿着设备


810
00:30:41,956 --> 00:30:43,506
绕着桌子走


811
00:30:43,506 --> 00:30:44,416
正在体验 AR 


812
00:30:45,366 --> 00:30:47,596
你可以看到他现在


813
00:30:47,986 --> 00:30:48,856
通过手上的设备往桌上


814
00:30:48,856 --> 00:30:50,766
添加了一个虚拟花瓶


815
00:30:50,766 --> 00:30:52,506
让这一切看起来更有趣了


816
00:30:54,556 --> 00:30:56,746
几分钟后


817
00:30:56,986 --> 00:30:58,126
他的朋友也来了


818
00:30:58,366 --> 00:31:00,576
现在他们都看着这个场景


819
00:31:00,656 --> 00:31:01,926
Andre 的设备在左边


820
00:31:02,446 --> 00:31:04,196
而他朋友的设备


821
00:31:04,196 --> 00:31:05,126
在右边


822
00:31:06,546 --> 00:31:07,446
你可以看到他们


823
00:31:07,706 --> 00:31:08,836
正在看同一个空间


824
00:31:08,926 --> 00:31:09,886
他们能看到彼此


825
00:31:10,266 --> 00:31:11,806
但最重要的是


826
00:31:11,806 --> 00:31:13,386
他们可以看到同样的虚拟内容


827
00:31:14,286 --> 00:31:15,446
他们正在体验的是一个


828
00:31:15,446 --> 00:31:19,246
共享的 AR 体验


829
00:31:19,246 --> 00:31:21,456
上面这个例子


830
00:31:21,456 --> 00:31:23,916
可以被分为三个阶段


831
00:31:24,266 --> 00:31:25,816
第一个阶段 Andre 围着


832
00:31:25,816 --> 00:31:27,616
桌子走动并获得了世界地图


833
00:31:28,886 --> 00:31:30,246
第二个阶段 世界地图在设备之间


834
00:31:30,536 --> 00:31:31,506
进行了分享


835
00:31:32,276 --> 00:31:34,346
第三个阶段 他的朋友的设备


836
00:31:34,496 --> 00:31:36,056
重新定位了世界地图


837
00:31:37,496 --> 00:31:38,986
这意味着这个对象


838
00:31:38,986 --> 00:31:40,536
在新设备上也能识别


839
00:31:40,536 --> 00:31:42,936
这个场景与另一个设备上的是相同的


840
00:31:43,586 --> 00:31:45,406
然后根据地图来


841
00:31:45,406 --> 00:31:46,556
计算设备的准确位置


842
00:31:46,556 --> 00:31:48,356
之后就开始跟踪


843
00:31:48,356 --> 00:31:50,106
就好像新加入的设备


844
00:31:50,106 --> 00:31:51,746
自己获取了这个世界地图一样


845
00:31:52,376 --> 00:31:54,956
接下来 我们将会对


846
00:31:54,956 --> 00:31:56,196
这三个阶段进行更细节的了解


847
00:31:56,866 --> 00:31:59,246
但首先 让我们复习一下


848
00:31:59,246 --> 00:32:00,426
什么是世界地图


849
00:32:01,156 --> 00:32:02,866
世界地图包括


850
00:32:02,866 --> 00:32:04,966
系统定位所需要的


851
00:32:04,966 --> 00:32:06,476
所有跟踪数据


852
00:32:06,986 --> 00:32:08,546
包括那些特征点


853
00:32:08,546 --> 00:32:09,866
就像 Marion 刚刚已经


854
00:32:09,866 --> 00:32:10,506
详细解释的一样


855
00:32:10,876 --> 00:32:12,596
也包括了这个点的


856
00:32:12,596 --> 00:32:13,000
周围外观


857
00:32:17,046 --> 00:32:18,496
世界地图还包括了


858
00:32:18,496 --> 00:32:19,826
被添加到环节中的


859
00:32:19,826 --> 00:32:21,806
所有锚点 不管这些锚点是由用户


860
00:32:21,916 --> 00:32:23,636
添加的 比如说像平面


861
00:32:24,896 --> 00:32:26,126
我的意思是像由系统


862
00:32:26,126 --> 00:32:26,456
添加的平面


863
00:32:26,456 --> 00:32:28,126
或者由用户添加的锚点


864
00:32:28,446 --> 00:32:29,646
就像我们刚刚在示范中看到的花瓶


865
00:32:30,746 --> 00:32:33,786
这个数据是可串性化


866
00:32:33,786 --> 00:32:35,536
可获取的 你可以利用这些数据


867
00:32:35,536 --> 00:32:37,346
来创造一些可持续的


868
00:32:37,826 --> 00:32:39,966
或者多用户的 AR 体验


869
00:32:40,326 --> 00:32:41,486
现在让我们来看看第一个阶段


870
00:32:41,486 --> 00:32:43,526
也就是获得


871
00:32:43,526 --> 00:32:44,006
世界地图的阶段


872
00:32:44,756 --> 00:32:47,576
我们可以回放第一段视频


873
00:32:48,056 --> 00:32:49,396
Andre 绕着桌子走到时候


874
00:32:49,396 --> 00:32:51,026
在左边 你可以


875
00:32:51,026 --> 00:32:52,596
看到他的设备 就在这里


876
00:32:53,256 --> 00:32:57,216
在右边 你可以以鸟瞰角度


877
00:32:57,216 --> 00:32:59,126
看到通过跟踪系统获得


878
00:32:59,126 --> 00:33:00,426
的世界地图


879
00:33:00,776 --> 00:33:02,516
你可以围绕着


880
00:33:03,096 --> 00:33:04,566
它的桌子和椅子


881
00:33:05,136 --> 00:33:06,956
在这个获取过程中


882
00:33:06,956 --> 00:33:09,276
有几点需要我们注意


883
00:33:10,076 --> 00:33:11,986
首先 Marion 在跟踪部分


884
00:33:11,986 --> 00:33:13,946
所说到的所有技术 都在这里得到应用


885
00:33:14,456 --> 00:33:15,606
场景需要有足够的


886
00:33:15,606 --> 00:33:17,356
视觉复杂性


887
00:33:17,446 --> 00:33:18,626
地图才能获得足够的深度特征点


888
00:33:19,546 --> 00:33:21,276
场景需要是静止的 当然 


889
00:33:22,086 --> 00:33:22,856
如果有细微的变化


890
00:33:22,856 --> 00:33:24,276
也是没有关系的


891
00:33:24,276 --> 00:33:25,716
大家也可以看到桌布被风吹动


892
00:33:26,026 --> 00:33:27,216
但场景的主要部分


893
00:33:27,566 --> 00:33:27,956
需要是静止的


894
00:33:29,036 --> 00:33:30,606
另外 在我们


895
00:33:30,606 --> 00:33:31,936
获取世界地图进行分享时


896
00:33:31,936 --> 00:33:33,856
我们需要从多个


897
00:33:33,856 --> 00:33:35,316
不同的角度来


898
00:33:35,316 --> 00:33:36,636
探索这个环境


899
00:33:37,516 --> 00:33:38,746
特别是我们想要


900
00:33:38,746 --> 00:33:41,046
覆盖所有我们想要


901
00:33:41,106 --> 00:33:42,786
被定位的方向


902
00:33:45,366 --> 00:33:46,926
为了让这个过程变得更简单


903
00:33:47,356 --> 00:33:50,296
我们增加了世界地图状态提示


904
00:33:50,296 --> 00:33:51,736
让你能了解有关


905
00:33:51,736 --> 00:33:52,566
世界地图的信息


906
00:33:53,446 --> 00:33:54,816
如果你们有参加过


907
00:33:54,816 --> 00:33:56,066
What's New in ARKit 这一演讲


908
00:33:56,636 --> 00:33:57,506
那么 Arsalan 会极大的扩展这一点


909
00:33:57,546 --> 00:33:58,936
以便快速回顾一下


910
00:33:59,576 --> 00:34:00,766
当你开始会话时


911
00:34:00,766 --> 00:34:02,776
世界地图的状态 会从限制状态开始


912
00:34:02,776 --> 00:34:03,936
随着设备了解了


913
00:34:04,056 --> 00:34:06,746
更多场景的信息


914
00:34:06,746 --> 00:34:07,616
就会变成初始化的状态


915
00:34:08,036 --> 00:34:09,176
最后当系统


916
00:34:09,176 --> 00:34:10,856
确定你保持在同一位置时


917
00:34:10,856 --> 00:34:12,156
我们终于可以开始


918
00:34:12,156 --> 00:34:12,766
绘制地图了


919
00:34:13,726 --> 00:34:15,206
这是在制图阶段


920
00:34:15,206 --> 00:34:16,976
你想要保存的地图


921
00:34:17,626 --> 00:34:20,315
所以 这是好的信息


922
00:34:20,315 --> 00:34:22,056
但是这主要是


923
00:34:22,056 --> 00:34:24,746
用户方面的适用操作


924
00:34:24,966 --> 00:34:26,076
所以 这对开发者而言


925
00:34:26,156 --> 00:34:26,926
代表着什么呢


926
00:34:27,505 --> 00:34:29,085
意味着你需要给用户提供指引


927
00:34:30,275 --> 00:34:32,255
我们可以告知用户制图时


928
00:34:32,326 --> 00:34:33,985
的不同状态


929
00:34:33,985 --> 00:34:35,835
甚至可以设置不允许保存


930
00:34:35,835 --> 00:34:37,886
或者分享世界地图


931
00:34:37,926 --> 00:34:39,476
直到制图状态显示绘制状态


932
00:34:39,996 --> 00:34:44,255
我们也可以监控在


933
00:34:44,326 --> 00:34:45,576
获取环节中的


934
00:34:45,576 --> 00:34:48,696
跟踪质量


935
00:34:48,926 --> 00:34:50,045
如果跟踪状态出现一段时间的


936
00:34:50,045 --> 00:34:51,356
限制状态 比如说持续几秒


937
00:34:51,356 --> 00:34:52,146
就可以反馈给用户


938
00:34:53,056 --> 00:34:54,356
或许甚至可以提供


939
00:34:54,356 --> 00:34:56,036
重新开始获取环节的选项


940
00:34:56,476 --> 00:34:58,496
在设备的接受方面


941
00:34:58,496 --> 00:35:00,726
我们也可以指引用户


942
00:35:00,726 --> 00:35:02,346
以获得更好的定位流程


943
00:35:03,116 --> 00:35:04,856
我们又回到了获取设备这里


944
00:35:04,856 --> 00:35:06,776
当我们在地图阶段时


945
00:35:06,776 --> 00:35:08,196
我们可以对场景进行拍照


946
00:35:08,196 --> 00:35:10,296
然后将它与


947
00:35:10,396 --> 00:35:11,946
世界地图一起发出


948
00:35:11,986 --> 00:35:13,596
在接收端


949
00:35:13,596 --> 00:35:16,026
我们可以让用户找到这个视觉


950
00:35:16,026 --> 00:35:17,766
来开始你的分享体验


951
00:35:18,386 --> 00:35:20,946
这就是获得


952
00:35:20,946 --> 00:35:21,336
世界地图的流程


953
00:35:21,506 --> 00:35:22,876
现在 让我们了解一下如何


954
00:35:22,876 --> 00:35:24,666
才能分享世界地图


955
00:35:24,966 --> 00:35:26,366
首先 你可以简单地通过


956
00:35:26,366 --> 00:35:27,386
在 ARSession 中运行


957
00:35:27,496 --> 00:35:29,926
getCurrentWorldMap 方式来


958
00:35:29,926 --> 00:35:30,536
获得世界地图


959
00:35:30,896 --> 00:35:33,286
这是获得世界地图的一种方式


960
00:35:34,376 --> 00:35:37,476
世界地图是一个可串行化的类


961
00:35:37,566 --> 00:35:38,856
所以我们可以通过


962
00:35:38,956 --> 00:35:40,576
NSKeyedArchiver 公式来


963
00:35:40,576 --> 00:35:42,106
将它串行为一串二进制的数据


964
00:35:42,106 --> 00:35:44,446
你可以选择将这串数据


965
00:35:44,446 --> 00:35:46,276
保存在硬盘中


966
00:35:46,276 --> 00:35:48,946
供单机持续性的应用


967
00:35:49,706 --> 00:35:52,636
你也可以选择 将它在设备之间分享


968
00:35:52,906 --> 00:35:54,626
为了实现设备间分享


969
00:35:54,626 --> 00:35:56,176
你可以使用 MultiPeerConnectivity 框架


970
00:35:56,936 --> 00:35:58,196
这个框架有着很棒的功能


971
00:35:58,196 --> 00:36:00,186
像自动装置 发现附近设备等


972
00:36:00,186 --> 00:36:01,856
这使设备之间的数据


973
00:36:01,856 --> 00:36:04,876
可以高效沟通


974
00:36:05,626 --> 00:36:06,996
我们在 ARKit 中也有如何运用


975
00:36:06,996 --> 00:36:09,396
这个技巧的例子


976
00:36:09,396 --> 00:36:11,036
被称为 创造一个多用户的


977
00:36:11,036 --> 00:36:12,126
AR 体验


978
00:36:12,126 --> 00:36:13,516
你可以在我们的开发者网站上找到这个例子


979
00:36:14,076 --> 00:36:16,836
让我们看一下


980
00:36:16,836 --> 00:36:18,086
接收端设备一旦收到世界地图之后


981
00:36:18,086 --> 00:36:20,096
需要如何设置


982
00:36:20,096 --> 00:36:21,046
世界跟踪参数


983
00:36:21,046 --> 00:36:22,126
来使用这个地图


984
00:36:22,426 --> 00:36:22,986
非常简单


985
00:36:22,986 --> 00:36:24,916
你只需要赋予世界地图


986
00:36:24,916 --> 00:36:27,366
原始数据地图的特征


987
00:36:28,266 --> 00:36:29,986
当你运行这个会话时


988
00:36:29,986 --> 00:36:31,426
系统会尝试去寻找


989
00:36:31,526 --> 00:36:32,376
之前的世界地图


990
00:36:33,636 --> 00:36:35,336
但这会耗费一定时间


991
00:36:35,336 --> 00:36:36,366
因为用户可能并没有


992
00:36:36,366 --> 00:36:37,546
从以前的位置拍摄


993
00:36:37,546 --> 00:36:37,906
这一的场景


994
00:36:38,756 --> 00:36:39,656
那我们怎样才能知道


995
00:36:39,656 --> 00:36:40,606
定位正在运行呢


996
00:36:41,686 --> 00:36:44,666
这个信息可以在跟踪阶段获得


997
00:36:44,666 --> 00:36:45,896
只要你用原始世界地图


998
00:36:45,896 --> 00:36:47,596
开始一个会话


999
00:36:47,596 --> 00:36:49,256
那跟踪阶段就会被


1000
00:36:49,256 --> 00:36:51,366
限制为重新定位


1001
00:36:51,906 --> 00:36:54,226
但在这里你还是可以获得


1002
00:36:54,226 --> 00:36:55,816
跟踪数据


1003
00:36:56,176 --> 00:36:58,726
但原始世界将会是


1004
00:36:58,726 --> 00:37:00,826
第一个相机 就像在一个新的会话中一样


1005
00:37:01,336 --> 00:37:04,156
只要用户将设备


1006
00:37:04,156 --> 00:37:05,606
对着同样的场景时


1007
00:37:05,606 --> 00:37:06,616
系统就会开始定位


1008
00:37:07,076 --> 00:37:08,136
跟踪阶段会重归正常


1009
00:37:08,136 --> 00:37:09,866
同时原始世界


1010
00:37:09,866 --> 00:37:12,036
将会与记录的世界地图一致


1011
00:37:13,046 --> 00:37:15,496
在这里 你之前的所有


1012
00:37:15,496 --> 00:37:16,856
锚点在你的会话中


1013
00:37:16,856 --> 00:37:17,916
都是可用的 所以你可以将以前


1014
00:37:17,976 --> 00:37:19,226
的虚拟内容放回来


1015
00:37:21,816 --> 00:37:23,506
在这里需要留意的是


1016
00:37:23,506 --> 00:37:24,986
这些幕后操作


1017
00:37:25,206 --> 00:37:26,806
是我们


1018
00:37:26,806 --> 00:37:28,206
正在匹配那些特征点


1019
00:37:28,616 --> 00:37:29,996
你获得世界地图的场景


1020
00:37:29,996 --> 00:37:32,306
与你想要重新定位的


1021
00:37:32,306 --> 00:37:33,326
场景之间需要有


1022
00:37:33,536 --> 00:37:35,146
足够的视觉相似性


1023
00:37:36,116 --> 00:37:37,406
所以 如果你想在晚上的时候


1024
00:37:37,406 --> 00:37:38,696
回到这个桌子旁进行 AR 体验


1025
00:37:38,696 --> 00:37:41,946
那效果很可能会不好


1026
00:37:42,206 --> 00:37:44,116
这就是你如何通过


1027
00:37:44,506 --> 00:37:46,516
保存或者加载地图


1028
00:37:46,516 --> 00:37:47,686
来实现多用户体验


1029
00:37:47,686 --> 00:37:49,526
或者持续体验的方式


1030
00:37:50,536 --> 00:37:54,846
接下来要讲的是图像跟踪


1031
00:37:54,846 --> 00:37:56,306
AR 就是


1032
00:37:56,306 --> 00:37:59,186
在物理世界上添加


1033
00:37:59,186 --> 00:38:00,306
一些虚拟内容


1034
00:38:00,536 --> 00:38:01,686
在物理世界中


1035
00:38:01,686 --> 00:38:02,866
到处都可以发现图像


1036
00:38:02,866 --> 00:38:05,226
想想


1037
00:38:05,226 --> 00:38:07,086
杂志封面


1038
00:38:07,426 --> 00:38:08,336
广告


1039
00:38:08,816 --> 00:38:10,136
图像跟踪是一个工具


1040
00:38:10,136 --> 00:38:11,506
它让你能识别这些真实图像


1041
00:38:11,506 --> 00:38:13,976
并能在他们的周围


1042
00:38:14,216 --> 00:38:15,876
建立 AR 体验


1043
00:38:17,876 --> 00:38:18,746
让我们来看一个例子


1044
00:38:20,006 --> 00:38:21,896
大家可以看到这里


1045
00:38:21,896 --> 00:38:23,356
有两张图像被同时跟踪


1046
00:38:24,426 --> 00:38:26,766
在左边的图像中


1047
00:38:27,206 --> 00:38:29,376
一只美丽的大象被


1048
00:38:29,376 --> 00:38:30,716
放在真实图像中的大象之上


1049
00:38:31,606 --> 00:38:32,896
而在右边 一个真实的图像


1050
00:38:32,896 --> 00:38:35,116
被转化为一个虚拟的屏幕


1051
00:38:36,186 --> 00:38:37,626
还有一点需要留意的


1052
00:38:37,626 --> 00:38:39,016
图像可以在环境中随意移动


1053
00:38:39,016 --> 00:38:40,966
因为相机正在以每秒 60 帧的


1054
00:38:40,966 --> 00:38:42,146
速度进行跟踪


1055
00:38:43,306 --> 00:38:45,146
接下来让我们


1056
00:38:45,146 --> 00:38:46,866
讨论在这景象


1057
00:38:47,156 --> 00:38:47,476
背后发生了些什么


1058
00:38:47,476 --> 00:38:48,996
假设说你有一张


1059
00:38:48,996 --> 00:38:50,526
和这个大象一样的图像


1060
00:38:50,896 --> 00:38:52,186
你想要在一个类似这样的


1061
00:38:52,236 --> 00:38:52,876
景象中找到它


1062
00:38:54,266 --> 00:38:55,556
我们将会用灰度来实现这个效果


1063
00:38:55,556 --> 00:38:56,956
第一种方式


1064
00:38:56,956 --> 00:38:58,426
和我们在跟踪时用到的


1065
00:38:58,426 --> 00:38:58,836
有点类似


1066
00:38:58,936 --> 00:38:59,956
我们会跟踪


1067
00:39:00,046 --> 00:39:01,526
参考图像与


1068
00:39:01,526 --> 00:39:03,566
当前环境中的特征点


1069
00:39:04,676 --> 00:39:05,996
然后 我们会试图


1070
00:39:05,996 --> 00:39:07,356
在当前的场景中匹配


1071
00:39:07,356 --> 00:39:09,346
那些在参考图像中特征点


1072
00:39:10,436 --> 00:39:11,696
通过运用一些投影几何


1073
00:39:11,696 --> 00:39:12,866
以及线性代数


1074
00:39:13,186 --> 00:39:14,366
这就可以根据


1075
00:39:14,456 --> 00:39:16,126
当前的场景


1076
00:39:16,126 --> 00:39:17,286
对图像的位置方向


1077
00:39:17,286 --> 00:39:19,156
进行初步的估算


1078
00:39:20,516 --> 00:39:21,386
但我们并不满足于此


1079
00:39:22,566 --> 00:39:23,486
为了让你能获得一个


1080
00:39:23,486 --> 00:39:25,836
真正准确的姿势并做到


1081
00:39:25,836 --> 00:39:27,846
每秒 60 帧的跟踪


1082
00:39:27,846 --> 00:39:29,276
我们做了一个高密度跟踪阶段


1083
00:39:29,986 --> 00:39:31,316
在初步的预估之后


1084
00:39:31,976 --> 00:39:33,136
所以我们将现有场景的像素


1085
00:39:33,136 --> 00:39:36,486
回溯到一个


1086
00:39:36,566 --> 00:39:38,266
长方形的形状


1087
00:39:38,266 --> 00:39:40,916
也就是你在右边 右上角所能看到的


1088
00:39:41,306 --> 00:39:42,776
所以这是将


1089
00:39:43,536 --> 00:39:45,296
现有图像的像素回溯成


1090
00:39:45,296 --> 00:39:46,986
一个长方形后重组的图像


1091
00:39:47,916 --> 00:39:48,766
我们将重组后的图像


1092
00:39:48,766 --> 00:39:50,456
与我们所拥有的


1093
00:39:50,506 --> 00:39:51,726
参考图像进行对比


1094
00:39:51,726 --> 00:39:54,056
来创造你们所看到的


1095
00:39:54,056 --> 00:39:55,436
下面这个误差图像


1096
00:39:56,636 --> 00:39:59,836
接下来 我们会优化图像的位置方向


1097
00:39:59,906 --> 00:40:00,976
这样能使误差最小化


1098
00:40:03,366 --> 00:40:04,786
这也意味着


1099
00:40:04,786 --> 00:40:06,796
对你来说这个结果将会


1100
00:40:06,796 --> 00:40:07,266
十分准确


1101
00:40:08,146 --> 00:40:09,706
谢谢


1102
00:40:10,526 --> 00:40:12,076
而且还能做到每秒


1103
00:40:12,076 --> 00:40:12,826
跟踪 60 帧


1104
00:40:15,296 --> 00:40:16,906
让我们看看我们在


1105
00:40:16,906 --> 00:40:18,126
ARKit 中时如何实现这些


1106
00:40:18,916 --> 00:40:21,856
照常 ARKit 的 API


1107
00:40:21,856 --> 00:40:22,506
十分简单


1108
00:40:22,566 --> 00:40:24,566
我们只需要做三个简单的步骤


1109
00:40:24,606 --> 00:40:26,686
首先 我们需要收集


1110
00:40:26,686 --> 00:40:27,596
所有参考图像


1111
00:40:28,486 --> 00:40:31,346
接着 我们要设置 AR 会话的参数


1112
00:40:31,606 --> 00:40:32,646
我们有两种选择


1113
00:40:33,166 --> 00:40:34,306
一个是世界跟踪配置


1114
00:40:34,306 --> 00:40:37,426
它也提供设备位置


1115
00:40:37,426 --> 00:40:39,296
这个是我们目前所讨论的方式


1116
00:40:39,856 --> 00:40:42,576
在 iOS 12 中 将会有一个新的配置


1117
00:40:42,576 --> 00:40:44,476
这是一个独立的跟踪配置


1118
00:40:44,936 --> 00:40:47,906
一旦你开始了会话


1119
00:40:47,906 --> 00:40:49,896
你就会开始通过 ARImageAnchor


1120
00:40:49,896 --> 00:40:52,326
的形式接收结果


1121
00:40:53,296 --> 00:40:54,306
下面 我们将会


1122
00:40:54,306 --> 00:40:55,626
进一步研究这三个步骤


1123
00:40:56,036 --> 00:40:57,886
让我们从参考图像开始


1124
00:40:58,426 --> 00:41:01,286
往你的 App 中加入


1125
00:41:01,286 --> 00:41:02,836
参考图像的最便捷方式


1126
00:41:02,896 --> 00:41:05,106
就是通过资产目录


1127
00:41:06,146 --> 00:41:08,066
你可以创造一个 AR 资源组


1128
00:41:08,066 --> 00:41:10,386
然后将你的图像拖拽到组里


1129
00:41:11,566 --> 00:41:13,026
接下来 你需要设置


1130
00:41:13,026 --> 00:41:14,446
图像的物理尺寸


1131
00:41:14,556 --> 00:41:16,196
你可以通过右上角


1132
00:41:16,196 --> 00:41:17,436
的特征窗口进行设置


1133
00:41:17,946 --> 00:41:20,656
设置物理尺寸是硬性要求


1134
00:41:21,096 --> 00:41:23,126
这里有几个原因


1135
00:41:24,466 --> 00:41:26,126
第一 设置物理尺寸能让


1136
00:41:26,126 --> 00:41:27,816
图像以物理尺寸展示


1137
00:41:28,386 --> 00:41:29,776
也就是说 你的内容


1138
00:41:29,816 --> 00:41:31,076
将会是物理大小


1139
00:41:31,186 --> 00:41:32,666
在 ARKit 中 所有事物都是


1140
00:41:32,666 --> 00:41:33,986
以米为测量单位 所以你的虚拟


1141
00:41:33,986 --> 00:41:36,226
物品将会以米为长度


1142
00:41:37,056 --> 00:41:38,626
第二 设置正确的


1143
00:41:38,626 --> 00:41:40,266
图像物理尺寸是十分重要的


1144
00:41:40,266 --> 00:41:41,586
因为我们


1145
00:41:41,946 --> 00:41:43,166
有可能会将图像跟踪


1146
00:41:43,216 --> 00:41:44,476
与世界跟踪合并到一起


1147
00:41:44,936 --> 00:41:46,206
这会立刻给予图像


1148
00:41:46,526 --> 00:41:48,616
与世界之间


1149
00:41:48,616 --> 00:41:51,196
一致的姿态


1150
00:41:51,196 --> 00:41:52,826
让我们看看有关参考图像的


1151
00:41:53,256 --> 00:41:54,166
更多例子


1152
00:41:54,816 --> 00:41:57,806
这里有两幅很漂亮的图像


1153
00:41:58,476 --> 00:41:59,776
这些图像十分适合


1154
00:41:59,776 --> 00:42:01,066
图像跟踪


1155
00:42:01,156 --> 00:42:03,426
它们有着高饱和度


1156
00:42:03,426 --> 00:42:04,956
高对比度


1157
00:42:04,956 --> 00:42:06,376
合理分配的柱形图


1158
00:42:06,376 --> 00:42:08,416
并且没有重复的结构


1159
00:42:08,536 --> 00:42:10,486
这里也有一些


1160
00:42:10,486 --> 00:42:12,936
和系统适配度不高的图像


1161
00:42:13,436 --> 00:42:14,726
右边这个图像就是不合适类型


1162
00:42:15,456 --> 00:42:16,176
的一个示范


1163
00:42:17,116 --> 00:42:19,606
如果我们仔细观察上面


1164
00:42:19,696 --> 00:42:21,596
这两个例子 我们会发现


1165
00:42:21,596 --> 00:42:23,466
好的图像都有着


1166
00:42:23,466 --> 00:42:25,036
许多关键点


1167
00:42:25,536 --> 00:42:26,306
图像中的


1168
00:42:26,306 --> 00:42:27,716
柱形图在整个


1169
00:42:27,716 --> 00:42:28,686
范围内平均分布


1170
00:42:29,316 --> 00:42:30,126
而右边的图像


1171
00:42:30,126 --> 00:42:33,096
则只有很少的关键点


1172
00:42:33,096 --> 00:42:34,286
它们的柱形图也


1173
00:42:34,286 --> 00:42:36,536
倾向白色的部分


1174
00:42:37,236 --> 00:42:40,536
在 Xcode 中


1175
00:42:40,536 --> 00:42:42,066
你可以直观的了解到一张图像


1176
00:42:42,066 --> 00:42:42,916
是否适合用于跟踪


1177
00:42:44,076 --> 00:42:46,066
你只需要将图像拖放到


1178
00:42:46,066 --> 00:42:48,396
Xcode 中 Xcode 就会自动


1179
00:42:48,576 --> 00:42:50,316
对图像进行分析 然后界面会以


1180
00:42:50,316 --> 00:42:52,206
警告的方式给你及早的反馈


1181
00:42:52,276 --> 00:42:54,516
这个反馈甚至还要早于你 运行你的 App


1182
00:42:55,646 --> 00:42:56,666
比如说 如果你点击


1183
00:42:56,666 --> 00:42:59,086
底部的这张图像


1184
00:42:59,086 --> 00:43:01,836
这张图像可能是杂志的某一页


1185
00:43:01,836 --> 00:43:04,396
我们可以看到 Xcode 反馈说


1186
00:43:04,556 --> 00:43:06,436
这张图像的柱形图 分布不均匀


1187
00:43:06,676 --> 00:43:07,626
你可以看到图像中


1188
00:43:07,626 --> 00:43:09,326
有大量的白色


1189
00:43:09,476 --> 00:43:11,716
这个图像也包含了许多重复结构


1190
00:43:11,716 --> 00:43:15,636
这些重复结构主要是文字


1191
00:43:15,776 --> 00:43:18,076
另一个例子就是 如果你有两张


1192
00:43:18,076 --> 00:43:20,126
极其类似且


1193
00:43:20,126 --> 00:43:22,176
十分容易在检测时混淆的图像


1194
00:43:22,506 --> 00:43:24,366
Xcode 也会


1195
00:43:24,366 --> 00:43:25,166
做出相应的警告


1196
00:43:25,906 --> 00:43:26,986
这两张有关


1197
00:43:26,986 --> 00:43:28,906
同一齿状山脉的图像


1198
00:43:28,906 --> 00:43:29,756
就是一个很好的例子


1199
00:43:30,156 --> 00:43:32,446
如果这个警告出现时


1200
00:43:32,446 --> 00:43:33,506
我们并不能做什么来改正


1201
00:43:33,936 --> 00:43:34,956
例如 让我们回到这张


1202
00:43:34,956 --> 00:43:38,236
有着重复结构


1203
00:43:38,236 --> 00:43:40,746
且分布不平均


1204
00:43:40,746 --> 00:43:41,746
的柱形图图像


1205
00:43:42,516 --> 00:43:44,186
你可以尝试去定位


1206
00:43:44,186 --> 00:43:45,126
这张图像中区别足够


1207
00:43:45,126 --> 00:43:46,596
明显的地方 在这里


1208
00:43:46,596 --> 00:43:48,736
就是页面中真正的


1209
00:43:48,736 --> 00:43:49,536
图像部分


1210
00:43:50,106 --> 00:43:51,276
接下来 你就可以对图像


1211
00:43:51,276 --> 00:43:52,756
进行裁剪 将裁剪后的图像


1212
00:43:52,756 --> 00:43:53,556
作为参考图像


1213
00:43:53,876 --> 00:43:55,126
这样子 Xcode 就不会发出


1214
00:43:55,476 --> 00:43:56,756
任何警告


1215
00:43:56,756 --> 00:43:58,296
你也可以获得更好的


1216
00:43:58,946 --> 00:43:59,716
跟踪质量


1217
00:44:00,026 --> 00:44:03,106
我们还可以


1218
00:44:03,436 --> 00:44:06,376
使用多个 AR 资源群组


1219
00:44:07,496 --> 00:44:09,176
这个功能可以同时检测


1220
00:44:09,396 --> 00:44:10,146
多张图像


1221
00:44:10,416 --> 00:44:11,726
为了保持体验的高效性


1222
00:44:11,726 --> 00:44:13,816
与反应性


1223
00:44:14,236 --> 00:44:15,296
你可以设置每个组最多


1224
00:44:15,296 --> 00:44:16,736
只能处理 25 张图像


1225
00:44:17,936 --> 00:44:19,016
但群组的数量


1226
00:44:19,246 --> 00:44:19,826
是没有限制的


1227
00:44:20,036 --> 00:44:21,896
接着 你就可以通过编程


1228
00:44:21,896 --> 00:44:23,156
的方式切换不同的群组


1229
00:44:23,376 --> 00:44:26,076
举个例子 如果你想要创造


1230
00:44:26,076 --> 00:44:27,076
一个博物馆中的 AR 体验


1231
00:44:27,076 --> 00:44:28,846
这个博物馆可能有着


1232
00:44:28,846 --> 00:44:30,646
成千上百的图像


1233
00:44:31,816 --> 00:44:33,396
通常 这些图像


1234
00:44:33,396 --> 00:44:34,586
分布在博物馆的


1235
00:44:34,586 --> 00:44:35,336
不同房间中


1236
00:44:35,646 --> 00:44:37,646
那你将要做的是


1237
00:44:38,246 --> 00:44:39,756
将在同一个房间中的图像


1238
00:44:39,756 --> 00:44:41,476
划分到一个群组中


1239
00:44:41,686 --> 00:44:43,036
在另外一个房间的图像则属于


1240
00:44:43,036 --> 00:44:43,596
另一个群组


1241
00:44:44,246 --> 00:44:45,806
随后 你可以用中心定点


1242
00:44:45,806 --> 00:44:48,686
来切换不同的房间


1243
00:44:49,186 --> 00:44:52,116
在这种情况下


1244
00:44:52,446 --> 00:44:53,906
只要在不同的群组中就可以


1245
00:44:53,946 --> 00:44:55,126
存在类似的图像


1246
00:44:55,896 --> 00:44:57,716
好的 以上就是有关


1247
00:44:57,746 --> 00:44:58,266
参考图像的信息


1248
00:44:58,266 --> 00:45:01,916
现在 让我们来看看我们的两个配置吧


1249
00:45:03,006 --> 00:45:05,286
ARImageTrackingConfiguration


1250
00:45:05,416 --> 00:45:06,876
是一个全新的 独立的


1251
00:45:06,876 --> 00:45:07,836
图像跟踪配置


1252
00:45:07,866 --> 00:45:09,646
这意味着它不会运转世界跟踪


1253
00:45:10,636 --> 00:45:11,846
也就是说并没有


1254
00:45:12,136 --> 00:45:12,916
它并没有世界原点


1255
00:45:13,216 --> 00:45:14,676
所以 每一张反馈给你的图像


1256
00:45:14,676 --> 00:45:16,526
都是与当前相机的视角有关


1257
00:45:18,246 --> 00:45:19,346
你可以将图像跟踪


1258
00:45:19,406 --> 00:45:21,646
与世界跟踪的配置结合起来


1259
00:45:22,466 --> 00:45:24,536
在这种情况下


1260
00:45:24,536 --> 00:45:25,636
你可以获得所有与


1261
00:45:25,636 --> 00:45:27,506
场景理解有关的东西


1262
00:45:27,506 --> 00:45:29,356
像平面检测 灯光预估


1263
00:45:29,356 --> 00:45:30,026
以及其他信息


1264
00:45:31,066 --> 00:45:32,796
那这两种配置中


1265
00:45:32,796 --> 00:45:34,376
哪一种更适合我们使用呢


1266
00:45:35,066 --> 00:45:35,536
让我们看看


1267
00:45:35,626 --> 00:45:37,106
ARImageTrackingConfigurations


1268
00:45:37,516 --> 00:45:39,916
是专门为围绕


1269
00:45:39,916 --> 00:45:41,226
图像发生的 AR 体验


1270
00:45:41,226 --> 00:45:42,906
所量身定制的


1271
00:45:43,196 --> 00:45:44,826
在屏幕的左侧 我们可以看到相应的例子


1272
00:45:46,606 --> 00:45:48,516
图像可以是


1273
00:45:48,516 --> 00:45:49,666
课本的某一页


1274
00:45:50,506 --> 00:45:51,576
为了让这次体验更


1275
00:45:51,576 --> 00:45:53,976
吸引人 我们


1276
00:45:54,286 --> 00:45:54,746
动态地将图表重叠


1277
00:45:54,746 --> 00:45:55,846
在这个例子里就是


1278
00:45:55,846 --> 00:45:56,796
如何画一个等边三角形


1279
00:45:57,866 --> 00:45:58,746
你可以看到


1280
00:45:58,746 --> 00:46:00,966
这个体验真的是为图像量身定制的


1281
00:46:01,116 --> 00:46:03,826
让我们来看另一个例子


1282
00:46:04,546 --> 00:46:05,516
图像跟踪用于


1283
00:46:05,606 --> 00:46:07,266
触发一些超出


1284
00:46:07,266 --> 00:46:09,496
图像范围的内容


1285
00:46:09,496 --> 00:46:12,086
在这种情况下 你需要使用


1286
00:46:12,086 --> 00:46:13,726
ARWorldTrackingConfiguration


1287
00:46:13,806 --> 00:46:14,906
因为你需要设备的位置信息


1288
00:46:14,906 --> 00:46:16,506
来保持对图像外的


1289
00:46:16,506 --> 00:46:19,856
内容的跟踪


1290
00:46:20,076 --> 00:46:21,146
因为图像跟踪并不使用


1291
00:46:21,246 --> 00:46:23,576
运动数据


1292
00:46:23,576 --> 00:46:24,676
所以它适用于像


1293
00:46:24,676 --> 00:46:26,506
公交车或者电梯这类


1294
00:46:27,076 --> 00:46:28,146
运动数据与视觉数据


1295
00:46:28,146 --> 00:46:29,896
不匹配的场景下


1296
00:46:30,916 --> 00:46:33,036
下面要讲的是


1297
00:46:33,036 --> 00:46:33,756
如何在代码中实现这些功能


1298
00:46:35,236 --> 00:46:36,916
你可以轻易地分辨出


1299
00:46:36,946 --> 00:46:37,716
这里的三个步骤


1300
00:46:37,936 --> 00:46:40,476
第一个是收集所有图像


1301
00:46:40,876 --> 00:46:41,796
这里有个很方便的功能


1302
00:46:41,796 --> 00:46:43,396
ARReferenceImage 这个功能


1303
00:46:43,566 --> 00:46:45,436
可以收集在


1304
00:46:45,436 --> 00:46:47,436
特定群组中的


1305
00:46:47,436 --> 00:46:48,366
所有图像


1306
00:46:48,556 --> 00:46:50,226
这个群组被命名为 Room1


1307
00:46:51,896 --> 00:46:53,256
我们可以简单的赋予


1308
00:46:53,256 --> 00:46:55,626
ARImageTrackingConfigurations


1309
00:46:55,626 --> 00:46:56,306
中这些图像


1310
00:46:56,306 --> 00:46:58,156
trackingImages 的特征


1311
00:46:58,586 --> 00:46:59,516
然后运行这一会话


1312
00:47:00,866 --> 00:47:02,766
你就能开始接收


1313
00:47:02,766 --> 00:47:04,306
反馈 比如说在这个会话中


1314
00:47:04,306 --> 00:47:06,346
didUpdate 这个锚点指定了方式


1315
00:47:06,466 --> 00:47:08,266
你可以检查这个


1316
00:47:08,266 --> 00:47:10,096
锚点是否是属于


1317
00:47:10,146 --> 00:47:11,156
ARImageAnchor


1318
00:47:12,606 --> 00:47:14,186
在这个锚点中


1319
00:47:14,246 --> 00:47:15,386
你可以找到图像的


1320
00:47:15,386 --> 00:47:17,336
位置与方向以及


1321
00:47:17,336 --> 00:47:18,656
参考图像本身


1322
00:47:18,716 --> 00:47:19,856
你可能会问


1323
00:47:19,856 --> 00:47:21,836
那要怎样才能找到以


1324
00:47:21,836 --> 00:47:23,156
你式命名的图像名称


1325
00:47:23,156 --> 00:47:24,126
然后确认哪些图像被


1326
00:47:24,126 --> 00:47:24,746
检测到了呢


1327
00:47:25,986 --> 00:47:26,966
这里有一个叫 Boolean 的特性


1328
00:47:26,966 --> 00:47:28,936
它能告诉你这个


1329
00:47:28,936 --> 00:47:30,086
图像当前是否


1330
00:47:30,086 --> 00:47:33,666
正在被跟踪


1331
00:47:33,866 --> 00:47:35,306
除了我们现在


1332
00:47:35,306 --> 00:47:36,496
所了解到的使用例子外


1333
00:47:36,496 --> 00:47:38,576
当你围绕着


1334
00:47:38,576 --> 00:47:40,546
图像进行建设


1335
00:47:40,546 --> 00:47:43,196
图像检测以及跟踪还有


1336
00:47:43,976 --> 00:47:44,126.
其他的功能


1337
00:47:45,226 --> 00:47:47,496
就好像 如果两个设备都对着


1338
00:47:47,496 --> 00:47:48,866
同一张真实图像


1339
00:47:48,866 --> 00:47:51,446
你可以在两个设备中


1340
00:47:51,446 --> 00:47:52,416
同时检测到这个图像


1341
00:47:52,846 --> 00:47:54,586
这可以为你提供一个


1342
00:47:54,586 --> 00:47:56,066
分享坐标体系


1343
00:47:56,066 --> 00:47:57,626
这个体系可以为分享体验


1344
00:47:58,106 --> 00:47:59,386
提供另一种选择


1345
00:48:01,476 --> 00:48:03,846
另一个例子是 如果你刚好知道


1346
00:48:03,846 --> 00:48:05,226
某张图像在世界上的


1347
00:48:05,346 --> 00:48:06,916
真实位置


1348
00:48:08,296 --> 00:48:09,826
比如说 你知道这幅公园地图


1349
00:48:09,826 --> 00:48:12,076
在现实生活中的位置


1350
00:48:12,456 --> 00:48:14,446
你可以通过图像跟踪来


1351
00:48:14,446 --> 00:48:15,956
获得拍摄图像


1352
00:48:15,956 --> 00:48:17,576
设备的位置


1353
00:48:17,576 --> 00:48:19,636
还能获得这个设备


1354
00:48:19,636 --> 00:48:20,656
在这个世界上的位置


1355
00:48:20,656 --> 00:48:21,946
你可以利用这些数据


1356
00:48:21,946 --> 00:48:23,956
来找到真实世界


1357
00:48:24,196 --> 00:48:27,086
中真正的方向


1358
00:48:27,646 --> 00:48:31,376
有关图像跟踪的讨论


1359
00:48:31,436 --> 00:48:31,836
就到此为止


1360
00:48:31,836 --> 00:48:34,006
接下来我们要讨论的是


1361
00:48:34,066 --> 00:48:35,266
物体检测


1362
00:48:38,016 --> 00:48:39,546
通过图像跟踪 我们了解了


1363
00:48:39,546 --> 00:48:41,716
我们是如何检测图像的


1364
00:48:41,716 --> 00:48:43,676
而图像在真实生活中


1365
00:48:43,676 --> 00:48:44,406
是一个平面物体


1366
00:48:45,376 --> 00:48:46,786
物品检测将这个概念


1367
00:48:46,786 --> 00:48:48,256
扩展到三维世界


1368
00:48:48,346 --> 00:48:50,586
让检测常用物品成为可能


1369
00:48:51,046 --> 00:48:53,626
但是这个物品


1370
00:48:53,816 --> 00:48:55,826
在场景中需要是静止的


1371
00:48:55,826 --> 00:48:57,086
它与图像不同


1372
00:48:57,086 --> 00:48:57,676
检测图像时是可以移动的


1373
00:48:58,866 --> 00:49:00,046
我们可以看看这个例子


1374
00:49:00,046 --> 00:49:02,566
这是娜芙蒂蒂胸像


1375
00:49:02,716 --> 00:49:04,266
这是一个可以在


1376
00:49:04,266 --> 00:49:05,326
博物馆中展览的雕塑


1377
00:49:05,636 --> 00:49:07,576
现在 你可以用 ARKit 来检测它


1378
00:49:08,216 --> 00:49:10,376
然后可以在真实物品的


1379
00:49:10,376 --> 00:49:14,476
上方显示一些信息


1380
00:49:15,466 --> 00:49:17,536
ARKit 中的物品检测


1381
00:49:17,736 --> 00:49:18,936
指的是有关物品的


1382
00:49:18,936 --> 00:49:21,116
真实实例的检测


1383
00:49:21,646 --> 00:49:22,506
所以我们正在讨论的


1384
00:49:22,506 --> 00:49:23,986
不是检测所有的雕像


1385
00:49:24,346 --> 00:49:26,406
而是检测这一特定的


1386
00:49:26,406 --> 00:49:27,456
娜芙蒂蒂雕像


1387
00:49:28,836 --> 00:49:29,986
我们如何在 ARKit


1388
00:49:29,986 --> 00:49:31,006
中重现这些物体呢


1389
00:49:31,626 --> 00:49:33,736
首先你需要扫描这个物体


1390
00:49:33,806 --> 00:49:35,086
事实上 只需要两步就可以


1391
00:49:35,086 --> 00:49:35,196
完成这个检测


1392
00:49:35,316 --> 00:49:36,936
首先 你对这个对象进行扫描


1393
00:49:36,936 --> 00:49:38,336
然后你就可以检测它


1394
00:49:39,096 --> 00:49:40,546
让我们讨论一下扫描这一部分


1395
00:49:40,546 --> 00:49:42,446
对开发者来说


1396
00:49:42,446 --> 00:49:44,166
扫描这一部分主要是你们的责任


1397
00:49:44,886 --> 00:49:46,016
你们需要负责


1398
00:49:46,016 --> 00:49:47,296
创造可用于


1399
00:49:47,406 --> 00:49:49,176
检测的物体重现


1400
00:49:51,276 --> 00:49:53,716
从内在来看 这个物体


1401
00:49:53,716 --> 00:49:55,616
的重现方式与


1402
00:49:55,616 --> 00:49:56,196
世界地图类似


1403
00:49:56,776 --> 00:49:58,576
在左边 你可以看到


1404
00:49:58,916 --> 00:50:00,456
娜芙蒂蒂胸像上的


1405
00:50:00,456 --> 00:50:01,886
3D 特征点


1406
00:50:03,056 --> 00:50:04,876
你可以使用扫描与检测 3D 物体


1407
00:50:05,006 --> 00:50:06,516
的开发者样本来


1408
00:50:06,516 --> 00:50:08,496
扫描物体


1409
00:50:08,496 --> 00:50:10,416
这个样本可在网站上获得


1410
00:50:11,706 --> 00:50:13,136
需要注意的是


1411
00:50:13,136 --> 00:50:14,766
你在运行时的


1412
00:50:14,766 --> 00:50:17,306
检测质量很大程度


1413
00:50:17,306 --> 00:50:18,986
上受扫描质量的影响


1414
00:50:19,776 --> 00:50:21,826
所以 不妨花上一些时间来


1415
00:50:21,826 --> 00:50:23,266
研究在扫描过程中


1416
00:50:23,336 --> 00:50:27,386
如何才能获得最好的质量


1417
00:50:27,546 --> 00:50:29,066
一旦你建立并运行了


1418
00:50:29,066 --> 00:50:30,666
这个开发者样本


1419
00:50:30,786 --> 00:50:32,406
你会在你的设备上 看到与这个类似的东西


1420
00:50:33,286 --> 00:50:35,976
第一步是要找到你


1421
00:50:35,976 --> 00:50:37,926
物体周围的空间


1422
00:50:39,036 --> 00:50:40,066
App 会试图


1423
00:50:40,066 --> 00:50:41,556
自动估算这个


1424
00:50:41,586 --> 00:50:42,946
密封盒子的大小


1425
00:50:42,946 --> 00:50:43,946
探索不同的特征点


1426
00:50:44,896 --> 00:50:46,206
但你可以随时通过


1427
00:50:46,256 --> 00:50:48,856
拖拽这个盒子的边框来


1428
00:50:49,066 --> 00:50:50,726
调整它 使它变大或缩小


1429
00:50:52,876 --> 00:50:55,766
有一点需要特别留意


1430
00:50:55,766 --> 00:50:57,266
当你围着这个物体扫描时


1431
00:50:57,266 --> 00:50:58,796
不能漏掉这个物体


1432
00:50:58,796 --> 00:51:01,166
的任何特征点


1433
00:51:01,856 --> 00:51:03,606
你也可以从上方用两指手势


1434
00:51:03,606 --> 00:51:05,146
对盒子进行旋转操作


1435
00:51:05,856 --> 00:51:08,406
你需要确保这个盒子


1436
00:51:08,606 --> 00:51:09,666
围绕着物体


1437
00:51:09,696 --> 00:51:11,286
而且包括了这一物体的


1438
00:51:11,916 --> 00:51:11,986
所有特征点


1439
00:51:13,016 --> 00:51:15,246
接下来就是真正的扫描部分


1440
00:51:16,376 --> 00:51:19,356
在这个阶段


1441
00:51:19,356 --> 00:51:21,426
我们希望能真正的从各个


1442
00:51:21,736 --> 00:51:23,486
角度来观察这个对象


1443
00:51:23,486 --> 00:51:24,846
任何你觉得你的用户可能会想要


1444
00:51:24,846 --> 00:51:25,806
检测的角度都需要被考虑到


1445
00:51:27,056 --> 00:51:28,386
为了让你能更容易的


1446
00:51:28,386 --> 00:51:30,096
统计物体哪部分


1447
00:51:30,096 --> 00:51:31,656
已经被获取了


1448
00:51:31,656 --> 00:51:33,376
就像对这个美丽的


1449
00:51:33,376 --> 00:51:34,356
地板的重现


1450
00:51:34,726 --> 00:51:36,046
你可以看到在顶部


1451
00:51:36,456 --> 00:51:37,676
有一个百分比


1452
00:51:37,676 --> 00:51:38,786
它能告诉你有多少块地板


1453
00:51:38,786 --> 00:51:39,366
已经被获取了


1454
00:51:40,406 --> 00:51:41,786
在这个阶段中


1455
00:51:41,786 --> 00:51:43,716
你需要在物体有着


1456
00:51:43,716 --> 00:51:45,126
许多特征点的 或者


1457
00:51:45,126 --> 00:51:46,546
有明显区别性的区域花上足够的时间


1458
00:51:46,726 --> 00:51:47,726
这是十分重要的


1459
00:51:47,726 --> 00:51:49,446
你需要走近对象去


1460
00:51:49,446 --> 00:51:50,346
捕获所有细节


1461
00:51:50,686 --> 00:51:52,046
你还需要从所有


1462
00:51:52,046 --> 00:51:56,246
角度对对象进行扫描


1463
00:51:56,436 --> 00:51:59,456
就好像你在这里看到的


1464
00:51:59,686 --> 00:52:01,186
如果你对你已经获取的


1465
00:52:01,186 --> 00:52:02,996
信息感到满意了


1466
00:52:02,996 --> 00:52:04,476
你可以进入下一步


1467
00:52:04,476 --> 00:52:06,656
你可以通过简单的拖拽


1468
00:52:06,656 --> 00:52:09,166
在颜色系统中


1469
00:52:09,166 --> 00:52:10,026
调整原点


1470
00:52:10,536 --> 00:52:12,496
这个系统将会


1471
00:52:12,496 --> 00:52:13,886
是锚点在检测阶段时


1472
00:52:13,886 --> 00:52:16,506
反馈给你的系统


1473
00:52:16,506 --> 00:52:17,766
所以你要确保你将它


1474
00:52:17,766 --> 00:52:19,316
放置在一个适合你的


1475
00:52:19,316 --> 00:52:20,606
虚拟内容的位置上


1476
00:52:20,676 --> 00:52:25,526
现在 你已经有了


1477
00:52:25,676 --> 00:52:27,286
有关物体的完整重现


1478
00:52:27,286 --> 00:52:30,686
这个重现可以用来检测


1479
00:52:30,686 --> 00:52:33,106
现在 App 会转换到


1480
00:52:33,106 --> 00:52:34,976
检测模式


1481
00:52:36,106 --> 00:52:37,386
我们强力建议你使用这一模式


1482
00:52:37,386 --> 00:52:39,966
以尽早获得有关


1483
00:52:39,966 --> 00:52:41,046
检测质量的反馈


1484
00:52:41,866 --> 00:52:44,776
你可能会想要


1485
00:52:44,776 --> 00:52:46,126
从不同的角度来观察这个物体


1486
00:52:46,126 --> 00:52:47,796
以确认这个物体


1487
00:52:47,796 --> 00:52:49,776
已经从所有不同角度


1488
00:52:50,336 --> 00:52:51,696
检测过了


1489
00:52:51,696 --> 00:52:53,816
你可以将你的设备移开


1490
00:52:53,816 --> 00:52:55,086
然后从另一个角度拿回来


1491
00:52:55,836 --> 00:52:58,386
你需要确保扫描质量好到可以


1492
00:52:58,386 --> 00:53:00,006
用于检测物体


1493
00:53:00,526 --> 00:53:03,106
你也可以移动这些物体


1494
00:53:03,276 --> 00:53:05,866
这样 光照条件就会不同


1495
00:53:06,836 --> 00:53:08,216
你需要确保在这些情况下


1496
00:53:08,216 --> 00:53:09,196
物体还能被检测到


1497
00:53:09,196 --> 00:53:10,316
对像玩具这样的物体而言


1498
00:53:10,366 --> 00:53:12,536
这是十分重要的


1499
00:53:12,536 --> 00:53:13,646
因为你并不知道


1500
00:53:13,646 --> 00:53:15,546
它们的真实位置


1501
00:53:17,096 --> 00:53:18,776
我们也建议你将物体


1502
00:53:18,776 --> 00:53:21,556
放在一个完全不同的环境中


1503
00:53:22,016 --> 00:53:24,576
但还能确保它能被检测到


1504
00:53:25,666 --> 00:53:27,756
如果这个物体不能被检测到


1505
00:53:27,756 --> 00:53:28,806
那你可能需要回到


1506
00:53:28,806 --> 00:53:31,406
扫描环节重新扫描


1507
00:53:31,406 --> 00:53:32,506
并确保你所处的环境光线充足


1508
00:53:33,786 --> 00:53:35,956
光线充足的环境


1509
00:53:35,956 --> 00:53:37,916
在扫描时是至关重要的


1510
00:53:38,506 --> 00:53:39,776
如果你使用的是 Verilux 灯泡


1511
00:53:39,776 --> 00:53:41,916
那 500 lux 是最佳的亮度


1512
00:53:43,046 --> 00:53:45,006
如果这个亮度也不够


1513
00:53:45,166 --> 00:53:46,556
你可能需要保存不同的


1514
00:53:46,556 --> 00:53:50,446
扫描版本


1515
00:53:50,576 --> 00:53:51,826
当你对检测质量


1516
00:53:51,826 --> 00:53:53,266
感到满意时


1517
00:53:53,266 --> 00:53:55,136
你只需要将这个模型


1518
00:53:55,136 --> 00:53:57,846
存入你的 Mac 里


1519
00:53:57,846 --> 00:53:59,656
并将它添加到 AR 资源群组中


1520
00:53:59,746 --> 00:54:03,016
就像你对图像所做的一样


1521
00:54:03,016 --> 00:54:04,526
有一些物体


1522
00:54:04,526 --> 00:54:07,366
十分适合这个系统


1523
00:54:07,586 --> 00:54:08,706
这些物体就像


1524
00:54:08,706 --> 00:54:08,986
屏幕左侧展示的一样


1525
00:54:09,586 --> 00:54:10,956
首先 它们都是刚性物体


1526
00:54:10,956 --> 00:54:13,206
纹理丰富


1527
00:54:13,206 --> 00:54:14,696
区分明显


1528
00:54:15,436 --> 00:54:16,416
但也有着一些


1529
00:54:16,416 --> 00:54:18,506
物体不适用于这一系统


1530
00:54:19,066 --> 00:54:22,136
可以参考屏幕右侧的物体


1531
00:54:22,686 --> 00:54:24,756
像金属的


1532
00:54:24,826 --> 00:54:26,606
透明的 或者金属的


1533
00:54:26,606 --> 00:54:27,796
反光的物体不适合用于


1534
00:54:27,856 --> 00:54:28,096
这个系统


1535
00:54:29,206 --> 00:54:31,496
透明的物体


1536
00:54:31,576 --> 00:54:32,796
比如说玻璃材质的物体


1537
00:54:32,796 --> 00:54:34,376
也不适用于这个系统


1538
00:54:34,376 --> 00:54:35,236
因为这些物体的外观


1539
00:54:35,236 --> 00:54:37,766
受它们所处的场景影响


1540
00:54:38,816 --> 00:54:38,936
太大


1541
00:54:39,756 --> 00:54:41,506
这就是如何扫描物体


1542
00:54:41,706 --> 00:54:43,126
让我再强调一次


1543
00:54:43,126 --> 00:54:44,026
确保你的环境光线充足


1544
00:54:44,996 --> 00:54:46,596
接下来要了解的是


1545
00:54:46,646 --> 00:54:48,056
我们如何在 ARKit 中检测到这一操作


1546
00:54:50,046 --> 00:54:51,956
如果你觉得这个很眼熟


1547
00:54:51,956 --> 00:54:53,606
那时因为这个 API


1548
00:54:53,606 --> 00:54:55,006
与其中之一的图像十分相似


1549
00:54:55,586 --> 00:54:56,716
我们会有很方便的特征


1550
00:54:56,816 --> 00:54:58,586
来将所有对象集中到


1551
00:54:58,586 --> 00:54:58,896
一个群组中


1552
00:54:59,506 --> 00:55:00,656
在这里 指的是


1553
00:55:00,656 --> 00:55:01,916
ARReferenceObjects 这一类


1554
00:55:02,806 --> 00:55:05,136
当你配置你的


1555
00:55:05,136 --> 00:55:07,056
ARWorldTracking 参数时


1556
00:55:07,056 --> 00:55:08,516
你只需要将这个对象


1557
00:55:08,516 --> 00:55:10,876
传输到 the detectionObjects 的特征下


1558
00:55:13,206 --> 00:55:15,566
一旦你开始运行这个会话


1559
00:55:15,566 --> 00:55:17,236
你会收到结果反馈


1560
00:55:18,306 --> 00:55:19,356
在这里 你会需要


1561
00:55:19,356 --> 00:55:21,656
检查 ARObjectAnchor


1562
00:55:22,386 --> 00:55:23,656
它能为你提供现实生活中


1563
00:55:23,786 --> 00:55:25,436
物体的位置


1564
00:55:25,436 --> 00:55:27,866
以及方向


1565
00:55:28,716 --> 00:55:30,346
物体的名字也会


1566
00:55:30,346 --> 00:55:35,246
在资产目录中得到展示


1567
00:55:35,246 --> 00:55:36,516
你可能已经注意到


1568
00:55:36,706 --> 00:55:38,316
物体检测与


1569
00:55:38,316 --> 00:55:40,506
世界匹配重新定位之间


1570
00:55:40,506 --> 00:55:42,726
有着某些类似之处


1571
00:55:43,426 --> 00:55:44,806
但它们也有着一些不同


1572
00:55:44,856 --> 00:55:46,316
在物体检测时


1573
00:55:46,316 --> 00:55:48,406
我们是根据


1574
00:55:48,746 --> 00:55:50,946
现实世界 赋予物体位置


1575
00:55:51,506 --> 00:55:52,426
而在世界地图的重新定位时


1576
00:55:52,426 --> 00:55:53,966
是相机本身


1577
00:55:53,966 --> 00:55:56,066
调整了之前的


1578
00:55:56,066 --> 00:55:56,816
世界地图


1579
00:55:58,286 --> 00:56:01,336
而且 你还可以检测多个物体


1580
00:56:01,966 --> 00:56:03,626
物体检测十分适合


1581
00:56:03,736 --> 00:56:05,306
那些在放置在桌面上


1582
00:56:05,496 --> 00:56:06,376
家具大小的物体


1583
00:56:07,146 --> 00:56:08,446
而世界地图


1584
00:56:08,446 --> 00:56:10,266
获取的则是整个场景


1585
00:56:10,776 --> 00:56:14,576
以上便是有关物品检测的信息


1586
00:56:14,686 --> 00:56:16,266
让我们来总结一下我们


1587
00:56:17,796 --> 00:56:19,336
今天所讲过的东西


1588
00:56:19,516 --> 00:56:21,876
方向跟踪只能跟踪


1589
00:56:21,876 --> 00:56:23,696
设备的旋转


1590
00:56:23,696 --> 00:56:25,756
可以用于探索


1591
00:56:25,756 --> 00:56:26,416
静止的环境


1592
00:56:27,946 --> 00:56:29,266
世界跟踪是


1593
00:56:29,266 --> 00:56:30,776
全特征的位置和


1594
00:56:30,776 --> 00:56:32,246
方向跟踪


1595
00:56:32,246 --> 00:56:33,926
它能根据世界原点为你


1596
00:56:33,926 --> 00:56:36,286
提供设备的位置


1597
00:56:37,016 --> 00:56:38,776
并启用有关理解场景的所有


1598
00:56:38,776 --> 00:56:40,486
相关的功能


1599
00:56:41,166 --> 00:56:43,366
比如平面检测


1600
00:56:44,476 --> 00:56:46,256
平面检测让你可以与真实的


1601
00:56:46,256 --> 00:56:50,226
水平或垂直的平面互动


1602
00:56:50,226 --> 00:56:51,636
你可以在这些平面上放置虚拟物体


1603
00:56:51,996 --> 00:56:55,306
我们也了解了你可以通过


1604
00:56:55,656 --> 00:56:57,036
框架中的保存和上传


1605
00:56:57,036 --> 00:56:58,926
地图特征的功能创造


1606
00:56:58,926 --> 00:57:00,566
一个持续的或者多用户的体验


1607
00:57:01,516 --> 00:57:03,036
还有你如何才能用图像跟踪来


1608
00:57:03,036 --> 00:57:04,816
检测物理图像并以


1609
00:57:04,816 --> 00:57:06,096
每秒 60 帧的速度来跟踪它们


1610
00:57:06,096 --> 00:57:08,596
以及你是如何通过物体跟踪


1611
00:57:08,596 --> 00:57:11,176
来检测更多常用物体


1612
00:57:12,576 --> 00:57:15,216
我希望通过这个演讲


1613
00:57:15,216 --> 00:57:17,496
你们现在对 ARkit 中的所有


1614
00:57:17,496 --> 00:57:19,126
不同的跟踪技术以及


1615
00:57:19,586 --> 00:57:20,816
它们是如何运作的


1616
00:57:20,816 --> 00:57:22,016
有更清晰的理解


1617
00:57:23,006 --> 00:57:24,666
以及更了解你如何才能


1618
00:57:24,806 --> 00:57:25,976
获得最佳的跟踪质量


1619
00:57:26,436 --> 00:57:28,106
我们非常期待


1620
00:57:28,106 --> 00:57:29,226
你即将利用


1621
00:57:29,226 --> 00:57:29,656
ARKit 所创造出来的作品


1622
00:57:30,976 --> 00:57:32,416
更多的信息可以在


1623
00:57:32,486 --> 00:57:33,636
开发者网站上的


1624
00:57:33,636 --> 00:57:34,496
演讲链接中获得


1625
00:57:34,496 --> 00:57:36,226
明天早上 9 点


1626
00:57:36,536 --> 00:57:37,586
将会有 ARKit 实验室的活动


1627
00:57:38,356 --> 00:57:39,806
我和 Marion 都会


1628
00:57:39,806 --> 00:57:41,786
在现场回答有关


1629
00:57:41,786 --> 00:57:42,666
ARKit 的任何问题


1630
00:57:43,756 --> 00:57:44,976
最后 十分感谢大家


1631
00:57:44,976 --> 00:57:46,976
希望大家能享受这次知识冲击


1632
00:57:47,516 --> 00:57:53,506
[ 掌声 ]

