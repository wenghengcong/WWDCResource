1
00:00:19,700 --> 00:00:20,567
大家好


2
00:00:20,767 --> 00:00:24,367
我是Henry Mason
是Siri语音识别工程师


3
00:00:25,433 --> 00:00:28,733
今天我们非常激动地
发布一项全新的API


4
00:00:28,933 --> 00:00:32,567
它将让语音识别
也能为你的app解决问题


5
00:00:35,100 --> 00:00:37,767
先快速回顾一下什么是语音识别


6
00:00:38,300 --> 00:00:40,467
语音识别是自动的过程


7
00:00:40,533 --> 00:00:43,500
将人类语音的音频转换成文本


8
00:00:44,133 --> 00:00:46,267
它取决于语音的语言


9
00:00:46,467 --> 00:00:49,533
比如 英语会和汉语的识别不同


10
00:00:50,233 --> 00:00:52,333
在iOS 大多数人会想到Siri


11
00:00:52,433 --> 00:00:55,367
但语音识别对许多其他任务也有用


12
00:00:56,367 --> 00:00:58,767
由于Siri与iPhone 4S
一起发布


13
00:00:59,200 --> 00:01:01,600
iOS也带有keyboard听写


14
00:01:02,700 --> 00:01:06,267
在iOS keyboard空格键旁
那个小小的麦克风按键


15
00:01:06,333 --> 00:01:10,067
触发对任何UI kit
文本输入的语音识别


16
00:01:11,033 --> 00:01:13,733
每天有成千上万个应用使用这个功能


17
00:01:14,200 --> 00:01:17,300
事实上 大约三分之一的请求
来自第三方应用


18
00:01:18,100 --> 00:01:20,067
它使用起来极其方便


19
00:01:20,133 --> 00:01:23,400
它处理录音和录音中断


20
00:01:23,667 --> 00:01:25,567
它显示用户界面


21
00:01:25,633 --> 00:01:27,433
它不需要你再写任何代码


22
00:01:27,500 --> 00:01:29,700
就能支持任何文本输入


23
00:01:30,600 --> 00:01:32,700
而且它从iOS 5开始就可供使用


24
00:01:32,767 --> 00:01:35,900
iOS keyboard听写
从2011年起便可供使用


25
00:01:35,967 --> 00:01:38,333
但它的简化带来很多限制


26
00:01:40,033 --> 00:01:43,567
你的用户界面
通常并不需要keyboard


27
00:01:44,433 --> 00:01:47,133
当录音开始时你不能控制


28
00:01:47,733 --> 00:01:49,867
不能控制使用哪一种语言


29
00:01:49,933 --> 00:01:52,667
只是刚好使用系统的
keyboard语言


30
00:01:53,167 --> 00:01:56,433
甚至没有办法知道
听写键是否可用


31
00:01:58,100 --> 00:02:01,633
默认录音可能对你的使用案例不合理


32
00:02:02,267 --> 00:02:04,733
你可能想要更多信息 而不只是文本


33
00:02:07,100 --> 00:02:09,133
那么现在在iOS 10


34
00:02:09,199 --> 00:02:11,567
我们引入一种新的语音框架


35
00:02:11,667 --> 00:02:14,233
语音识别API更加强大


36
00:02:14,300 --> 00:02:18,533
它使用相同基本技术和Siri及
Dictation中所使用的一样


37
00:02:19,500 --> 00:02:21,600
它提供快速而准确的结果


38
00:02:21,833 --> 00:02:24,167
显而易见地定制给用户


39
00:02:24,600 --> 00:02:26,800
而无需你收集任何用户数据


40
00:02:29,033 --> 00:02:31,633
该框架也提供了识别的更多信息


41
00:02:33,300 --> 00:02:35,300
而不只是文本


42
00:02:36,333 --> 00:02:38,900
例如 我们也提供另外的解读


43
00:02:38,967 --> 00:02:42,867
关于你的用户可能说了什么
置信水平 以及定时信息


44
00:02:44,433 --> 00:02:47,800
用于API的音频可来自预录文件


45
00:02:47,900 --> 00:02:49,600
或现场来源 比如麦克风


46
00:02:49,667 --> 00:02:52,167
语音识别API的可用性
深远而广泛 经过许可


47
00:02:52,233 --> 00:02:57,133
iOS 10支持超过50种语言和
方言 从阿拉伯语到越南语


48
00:02:59,000 --> 00:03:01,433
任何运行iOS 10的设备都支持


49
00:03:03,233 --> 00:03:06,200
语音识别API
通常能胜任


50
00:03:06,267 --> 00:03:09,367
在需要互联网连接的大型服务器上


51
00:03:10,367 --> 00:03:15,800
不过 某些新的设备确实
时刻都支持语音识别


52
00:03:16,400 --> 00:03:18,833
我们提供可用性API以确定


53
00:03:18,900 --> 00:03:21,133
某个既定语言当前是否可用


54
00:03:21,400 --> 00:03:25,233
使用这个 而不是去寻找互联网连接


55
00:03:28,267 --> 00:03:29,700
由于语音识别需要


56
00:03:29,767 --> 00:03:32,100
传送用户的音频经过互联网


57
00:03:32,633 --> 00:03:34,667
用户必须明确提供许可给你的应用


58
00:03:34,733 --> 00:03:37,767
在可以使用语音识别之前


59
00:03:39,367 --> 00:03:40,767
语音识别
解释、授权、请求


60
00:03:40,833 --> 00:03:44,200
有四个主要步骤
在你的应用中采用语音识别


61
00:03:46,733 --> 00:03:50,267
首先在应用的Info.plist中
提供使用描述


62
00:03:51,533 --> 00:03:54,967
例如 你的相机应用Phromage


63
00:03:55,033 --> 00:03:59,400
可能用了语音识别的使用描述...


64
00:04:00,333 --> 00:04:04,167
这能让你只说cheese就能拍照


65
00:04:05,967 --> 00:04:10,367
其次 请求授权
利用请求授权级别方法


66
00:04:11,467 --> 00:04:14,300
你先前提供的解释会被呈现给用户


67
00:04:14,367 --> 00:04:16,533
在一个熟悉的对话中


68
00:04:17,200 --> 00:04:18,800
然后用户将能够决定


69
00:04:18,867 --> 00:04:21,666
他们是否想要让你的应用语音识别


70
00:04:23,200 --> 00:04:25,500
接下来 创建语音识别请求


71
00:04:27,133 --> 00:04:29,500
如果你已经有录好的音频文件


72
00:04:29,567 --> 00:04:33,433
使用SFSpeechURL
RecognitionRequest级别


73
00:04:34,000 --> 00:04:35,100
否则 你要使用


74
00:04:35,167 --> 00:04:38,800
SFSpeechAudioBuffer
RecognitionRequest


75
00:04:40,767 --> 00:04:42,667
最后 提交识别请求


76
00:04:42,733 --> 00:04:45,433
给SFSpeech Recognizer
开始识别


77
00:04:46,333 --> 00:04:49,267
你可以选择保留返回的识别任务


78
00:04:49,333 --> 00:04:52,867
这有助于监控识别过程


79
00:04:56,700 --> 00:04:58,567 line:1
我们来看看这个在代码中长什么样


80
00:04:59,300 --> 00:05:01,467 line:1
假定我们已更新info.plist


81
00:05:01,533 --> 00:05:04,733 line:1
通过准确的描述
关于如何使用它


82
00:05:05,467 --> 00:05:07,500 line:1
下一步是请求授权


83
00:05:08,667 --> 00:05:10,467 line:1
也许最好等到


84
00:05:10,533 --> 00:05:12,733 line:1
用户调用你的应用的功能后再这样做


85
00:05:12,800 --> 00:05:14,667 line:1
这个功能要依靠语音识别


86
00:05:17,267 --> 00:05:20,633 line:1
请求授权级别方法
借助完成处理程序


87
00:05:20,700 --> 00:05:23,433 line:1
它不保证某个执行语境


88
00:05:24,533 --> 00:05:27,100 line:1
应用通常要发送到主队列


89
00:05:27,167 --> 00:05:31,000 line:1
如果它们要做点什么
比如开启或关闭用户界面按钮


90
00:05:33,967 --> 00:05:37,600 line:1
如果你的授权处理程序
已给出authorized状态


91
00:05:38,200 --> 00:05:40,100 line:1
你应该准备开始识别


92
00:05:41,567 --> 00:05:44,300
否则 识别就无法对你的应用可用


93
00:05:45,500 --> 00:05:48,500
重要的是采用合适的方法
禁用必要的功能


94
00:05:48,567 --> 00:05:50,267
当用户作出这个决定时


95
00:05:50,800 --> 00:05:54,767
或当设备受限 无法使用语音识别时


96
00:05:55,500 --> 00:05:59,300
授权可稍后修改
在设备的隐私设置里


97
00:06:01,633 --> 00:06:04,900 line:1
我们来看看如何识别
一个预录的音频文件


98
00:06:05,867 --> 00:06:08,133 line:1
假设我们已有一个文件url


99
00:06:09,800 --> 00:06:14,867 line:1
识别需要语音识别程序
它只识别一种语言


100
00:06:15,467 --> 00:06:19,833 line:1
默认的SFSpeechRecognizer
启动程序可能会失败


101
00:06:20,533 --> 00:06:23,933 line:1
于是我返回0
如果区域不支持的话


102
00:06:24,833 --> 00:06:27,867 line:1
默认的启动程序使用设备的当前区域


103
00:06:29,800 --> 00:06:32,400 line:1
在这个功能中
我们只要返回1 在这个情况下


104
00:06:34,700 --> 00:06:38,633 line:1
虽然这个语音识别可能受支持
但它也许不可用


105
00:06:38,700 --> 00:06:41,133 line:1
可能由于没有互联网连接


106
00:06:41,900 --> 00:06:45,833 line:1
使用isAvailable属性
在你的识别程序中 以便监控它


107
00:06:48,833 --> 00:06:52,400 line:1
现在我们创建一个识别请求
用录好的文件的url


108
00:06:52,467 --> 00:06:57,300 line:1
然后将它给予识别程序的识别任务方法


109
00:07:01,967 --> 00:07:03,800
这个方法完成处理程序


110
00:07:03,867 --> 00:07:06,467
借助两种可选的参数
result和error


111
00:07:07,600 --> 00:07:09,233
如果result是0


112
00:07:09,300 --> 00:07:11,700
那意味着出于某种原因 识别失败


113
00:07:12,000 --> 00:07:14,267
检查error的参数 寻求解释


114
00:07:15,667 --> 00:07:18,900
否则 我们可以读出
我们已经识别的语音


115
00:07:19,233 --> 00:07:20,367
通过查看结果


116
00:07:21,633 --> 00:07:25,167
注意 完成处理程序
可能会被唤起不止一次


117
00:07:25,233 --> 00:07:27,267
当语音被逐步识别


118
00:07:28,200 --> 00:07:30,400
你可以确定识别已完成


119
00:07:30,467 --> 00:07:33,900
通过检查结果的isFinal属性


120
00:07:34,667 --> 00:07:37,600
这里我们只打印出最终识别的文本


121
00:07:43,833 --> 00:07:47,867 line:1
识别来自设备麦克风的
现场音频也很相似


122
00:07:47,933 --> 00:07:49,400 line:1
但需要一些改动


123
00:07:50,500 --> 00:07:53,367 line:1
我们要做出音频缓冲识别请求


124
00:07:53,867 --> 00:07:56,933 line:1
这能让我们提供内存音频缓冲的序列


125
00:07:57,000 --> 00:07:58,567 line:1
而不是硬盘上的文件


126
00:07:59,633 --> 00:08:03,300 line:1
我们使用AVAudioEngine
来获取音频缓冲流


127
00:08:04,933 --> 00:08:06,700 line:1
然后将其附加到请求


128
00:08:07,467 --> 00:08:09,733 line:1
注意 完全可以附加音频缓冲


129
00:08:09,800 --> 00:08:13,533 line:1
到识别请求
在开始识别之前和之后


130
00:08:17,333 --> 00:08:18,233
一个不同之处在于


131
00:08:18,300 --> 00:08:22,767
我们不再忽略识别任务方法的返回值


132
00:08:23,400 --> 00:08:25,733
反而 我们要将它保存在
一个变量的属性中


133
00:08:26,300 --> 00:08:27,533
等会儿我们就知道为什么


134
00:08:28,833 --> 00:08:30,100
当我们完成录音后


135
00:08:31,133 --> 00:08:34,232
我们需要通知请求
没有更多音频了


136
00:08:34,299 --> 00:08:35,967
以便它能完成识别


137
00:08:36,933 --> 00:08:39,200
使用endAudio方法来实现


138
00:08:40,267 --> 00:08:44,100
但要是用户取消录音
或者录音被中断呢？


139
00:08:44,733 --> 00:08:47,333
在这种情况下 我们真的不关心结果


140
00:08:47,400 --> 00:08:51,000
而且我们应该释放
仍在被语音识别使用的任何资源


141
00:08:52,700 --> 00:08:55,067
只要取消我们开始的识别任务...


142
00:08:55,133 --> 00:08:57,000
我们开始识别时保存的


143
00:08:57,367 --> 00:09:00,100
这对于预录音频的识别也能做到


144
00:09:00,967 --> 00:09:02,100
最佳做法


145
00:09:02,167 --> 00:09:04,633
简单说说一些最佳做法


146
00:09:05,833 --> 00:09:07,367
资源
负责任


147
00:09:07,433 --> 00:09:10,667
我们开放语音识别
给所有应用免费使用


148
00:09:11,233 --> 00:09:13,100
但我们的确有设置一些合理的限制


149
00:09:13,167 --> 00:09:15,600
以便这项服务一直对每个人可用


150
00:09:17,100 --> 00:09:20,033
不同的设备可能受限于


151
00:09:20,100 --> 00:09:21,700
每天可以识别的量


152
00:09:22,967 --> 00:09:26,900
应用也会在全球范围内被节流
根据每天的请求


153
00:09:28,600 --> 00:09:33,133
正如API支持的其他服务
例如CLGO Coder


154
00:09:33,567 --> 00:09:36,733
要有所准备以处理
网络和速率受限的故障


155
00:09:38,233 --> 00:09:40,667
如果你发现你经常达到节流的限制


156
00:09:40,733 --> 00:09:41,767
请告诉我们


157
00:09:44,633 --> 00:09:46,833
同样重要的是 要注意语音识别


158
00:09:46,900 --> 00:09:50,467
会极大地耗费电池和网络流量


159
00:09:52,333 --> 00:09:56,900 line:1
对于iOS 10我们开始限制
音频长度为大约一分钟


160
00:09:56,967 --> 00:09:59,133 line:1
类似于keyboard听写的时长


161
00:10:01,100 --> 00:10:02,767
隐私和可用性
透明度


162
00:10:02,833 --> 00:10:06,500
简单说说关于透明度
以及尊重用户的隐私


163
00:10:07,600 --> 00:10:09,267
如果你在录用户的语音


164
00:10:09,333 --> 00:10:12,833
最好在你的用户界面中说得非常明确


165
00:10:13,533 --> 00:10:17,100
播放录制的声音和/或
显示可见的录制指示


166
00:10:17,167 --> 00:10:20,167
可让用户清楚知道他们正在被录音


167
00:10:22,133 --> 00:10:24,733
有些语音不适合识别


168
00:10:25,467 --> 00:10:29,133
密码、健康数据、财务信息
以及其他敏感语音


169
00:10:29,200 --> 00:10:31,333
不应给予语音识别


170
00:10:33,733 --> 00:10:37,100
显示识别的语音
像Siri和Dictation做的


171
00:10:37,167 --> 00:10:40,333
也能帮助用户理解你的应用在做什么


172
00:10:40,900 --> 00:10:42,467
它对用户很有帮助


173
00:10:42,533 --> 00:10:45,300
以便他们可以在识别出错时及时看到


174
00:10:46,500 --> 00:10:47,767
总结


175
00:10:47,833 --> 00:10:49,300
那么 开发者们


176
00:10:49,367 --> 00:10:50,867
你们的应用现在可以免费获得


177
00:10:50,933 --> 00:10:54,233
高性能的语音识别
可识别几十种语言


178
00:10:54,933 --> 00:10:57,567
但重要的是要得体地处理
当它不可用时的情况


179
00:10:57,633 --> 00:10:59,833
或者用户不想让你的应用使用它


180
00:11:01,067 --> 00:11:03,233
透明度是最好的政策


181
00:11:03,400 --> 00:11:06,500
让用户清楚知道
什么时候语音识别正在被使用


182
00:11:07,900 --> 00:11:12,300
我们很兴奋地期待
你们会为语音识别带来什么新用途


183
00:11:13,200 --> 00:11:14,033
更多信息


184
00:11:14,100 --> 00:11:18,400
欲了解更多信息及一些样本代码
请查看本讲的网页


185
00:11:18,900 --> 00:11:21,700
你可能会对部分关于
SiriKit的会话感兴趣


186
00:11:21,867 --> 00:11:25,633
周三有一场
周四有一场更高级别的


187
00:11:26,633 --> 00:11:29,233
谢谢参与
祝你们在 WWDC 大有收获

